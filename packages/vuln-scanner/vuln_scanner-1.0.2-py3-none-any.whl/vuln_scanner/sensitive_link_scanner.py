import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import threading
from queue import Queue
from rich.console import Console
import re

console = Console()

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# âœ… Ø¯Ø§Ù„Ø© Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
def fix_url(url):
    parsed = urlparse(url)
    fixed_path = re.sub(r'/{2,}', '/', parsed.path)
    return f"{parsed.scheme}://{parsed.netloc}{fixed_path}"

# ğŸ” ÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©
def check_sensitive_files(url):
    sensitive_paths = [
        "/robots.txt", "/sitemap.xml", "/.well-known/security.txt",
        "/admin/", "/login/", "/dashboard/", "/config/", "/database/",
        "/backup/", "/uploads/", "/includes/", "/private/", "/hidden/",
        "/.git/", "/.env", "/.htaccess", "/server-status",
        "/phpinfo.php", "/test.php", "/info.php",
        "/dev/", "/staging/", "/temp/", "/logs/",
        "/data/", "/db/", "/core/", "/old/",
        "/backup.zip", "/backup.tar.gz", "/db.sql", "/config.php.bak"
    ]
    
    found_files = []
    for path in sensitive_paths:
        full_url = urljoin(url, path)
        fixed_url = fix_url(full_url)
        try:
            response = requests.get(fixed_url, headers=HEADERS, timeout=5)
            if response.status_code == 200:
                console.print(f"[green][+] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰: {fixed_url}[/green]")
                found_files.append(fixed_url)
        except requests.exceptions.RequestException:
            pass
    return found_files

# ğŸ” Bruteforce Ù„Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
def brute_force_dirs(url, wordlist):
    try:
        with open(wordlist, 'r', encoding='utf-8') as file:
            directories = [line.strip() for line in file if line.strip()]
    except FileNotFoundError:
        console.print("[red]âŒ Ù…Ù„Ù Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.[/red]")
        return []

    q = Queue()
    for directory in directories:
        q.put(directory)

    found_dirs = []

    def check_directory():
        while not q.empty():
            directory = q.get()
            full_url = urljoin(url, directory)
            fixed_url = fix_url(full_url)
            try:
                response = requests.get(fixed_url, headers=HEADERS, timeout=5)
                if response.status_code == 200:
                    console.print(f"[green][+] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰: {fixed_url}[/green]")
                    found_dirs.append(fixed_url)
            except:
                pass
            q.task_done()

    threads = []
    for _ in range(30):
        thread = threading.Thread(target=check_directory)
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    return found_dirs

# ğŸ•·ï¸ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„ØµÙØ­Ø©
def extract_links(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        links = set()

        for tag in soup.find_all(["a", "link", "script", "img"]):
            href = tag.get("href") or tag.get("src")
            if href:
                full_url = urljoin(url, href)
                fixed_url = fix_url(full_url)
                if urlparse(fixed_url).netloc == urlparse(url).netloc:
                    links.add(fixed_url)
        return links
    except requests.exceptions.RequestException:
        return set()

# ğŸ” Ø¯Ø§Ù„Ø© Ø§Ù„ÙØ­Øµ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def hidden_link_finder(url, wordlist):
    console.print(f"[yellow]ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø­Ø³Ø§Ø³Ø© ÙÙŠ {url}...[/yellow]")

    all_links = set()

    # ğŸ“‚ ÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©
    console.print("[cyan]ğŸ“‚ ÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©...[/cyan]")
    sensitive_files = check_sensitive_files(url)
    all_links.update(sensitive_files)

    # ğŸ•·ï¸ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„ØµÙØ­Ø©
    console.print("[cyan]ğŸ•·ï¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·...[/cyan]")
    page_links = extract_links(url)
    all_links.update(page_links)

    # ğŸ› ï¸ Bruteforce
    console.print("[cyan]ğŸ› ï¸ ØªØ´ØºÙŠÙ„ Bruteforce...[/cyan]")
    brute_dirs = brute_force_dirs(url, wordlist)
    all_links.update(brute_dirs)

    # âœ… Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    if all_links:
        console.print(f"\n[bold green]âœ… Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©:[/bold green]")
        for link in all_links:
            console.print(f"[green]{link}[/green]")

        with open("sensitive_links_results.txt", "w", encoding="utf-8") as file:
            file.write("\n".join(all_links))
        console.print("[cyan]ğŸ“„ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ [bold]sensitive_links_results.txt[/bold][/cyan]")
    else:
        console.print("[red]âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø±ÙˆØ§Ø¨Ø· Ø­Ø³Ø§Ø³Ø©.[/red]")
