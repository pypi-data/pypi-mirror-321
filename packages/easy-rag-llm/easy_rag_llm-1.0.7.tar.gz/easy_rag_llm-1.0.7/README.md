# easy_rag_llm

## CAUTION
- easy-rag-llm==1.0.* version is testing version. These versions are usually invalid.

## ğŸ‡°ğŸ‡· ì†Œê°œ
- easy_rag_llmëŠ” OpenAI ë° DeepSeek ëª¨ë¸ì„ ì§€ì›í•˜ëŠ” ê°„ë‹¨í•œ RAG(ì •ë³´ ê²€ìƒ‰ ë° ìƒì„±) ê¸°ë°˜ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê°„ë‹¨í•˜ê²Œ RAG LLMì„ ì„œë¹„ìŠ¤ì— í†µí•©ì‹œí‚¬ ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.
- (2025.01.15 ê¸°ì¤€/ v1.0.0) í•™ìŠµê°€ëŠ¥í•œ ìë£Œ í¬ë§·ì€ PDFì…ë‹ˆë‹¤.

## ğŸ‡ºğŸ‡¸ Introduction
- easy_rag_llm is a lightweight RAG-based service that supports both OpenAI and DeepSeek models.
It is designed to seamlessly integrate RAG-based LLM functionalities into your service.
- As of 2025-01-15 (v1.0.0), the supported resource format for training is PDF.

## Usage
#### Install
```bash
pip install easy_rag_llm
```

#### How to integrate to your service?
```python
from easy_rag import RagService

rs = RagService(
    embedding_model="text-embedding-3-small", #Fixed to OpenAI model
    response_model="deepseek-chat",  # Options: "openai" or "deepseek-chat"
    open_api_key="your_openai_api_key_here",
    deepseek_api_key="your_deepseek_api_key_here",
    deepseek_base_url="https://api.deepseek.com",
)

resource = rs.rsc("./rscFiles")  # Learn from all files under ./rscFiles

query = "What is the summary of the first document?"
response = rs.generate_response(resource, query)

print(response)
```

### ğŸ‡°ğŸ‡· ë©”ëª¨.
pdf ì œëª©ì„ ëª…í™•í•˜ê²Œ ì ì–´ì£¼ì„¸ìš”. ë©”íƒ€ë°ì´í„°ì—ëŠ” pdfì œëª©ì´ ì¶”ì¶œë˜ì–´ ë“¤ì–´ê°€ë©°, ë‹µë³€ ê·¼ê±°ë¥¼ ì¶œë ¥í• ë•Œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

### ğŸ‡ºğŸ‡¸ Memo.
- Ensure that your PDFs have clear titles. Extracted titles from the PDF metadata are used during training and for generating evidence-based responses.

### Author Information
- ê³½ë³‘í˜ (https://github.com/Aiden-Kwak)
