from mlagents.trainers.torch_entities.networks import SimpleActor, NetworkBody, Critic
from mlagents.trainers.settings import NetworkSettings
from mlagents_envs.base_env import ActionSpec, ObservationSpec
from mlagents.trainers.torch_entities.agent_action import AgentAction
from mlagents.trainers.torch_entities.decoders import ValueHeads
from mlagents.trainers.buffer import AgentBuffer

from mlagents.torch_utils import torch, nn

from typing import cast, List, Dict, Tuple, Optional, Union, Any
import importlib.util
import attr

from .action_model_aggregated import AggregatedActionModel

@attr.s(auto_attribs=True)
class CustomPPONetworkSettings(NetworkSettings):
    path_to_model: str = ""
    path_to_mapping: str = ""

class ImportedNetworkBody(NetworkBody):
    def __init__(
        self,
        observation_specs: List[ObservationSpec],
        model_spec,
        model_module,
        mapping_spec,
        mapping_module
    ):
        nn.Module.__init__(self)
        self.observation_specs = observation_specs

        self.load_body(model_spec, model_module, mapping_spec, mapping_module)

    def load_body(self, model_spec, model_module, mapping_spec, mapping_module):
        model_spec.loader.exec_module(model_module)
        mapping_spec.loader.exec_module(mapping_module)

        # Get body 
        exec("self.body = model_module.Model(self.observation_specs, mapping_module)")

    def update_normalization(self, buffer: AgentBuffer) -> None:
        pass

    def copy_normalization(self, other_network: "NetworkBody") -> None:
        pass

    @property
    def memory_size(self) -> int:
        # preserves one number that is how many memories to be available
        return 1

    def forward(
        self,
        inputs: List[torch.Tensor],
        actions: Optional[torch.Tensor] = None,
        memories: Optional[torch.Tensor] = None,
        sequence_length: int = 1,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        
        memories_mask = memories
        if isinstance(memories_mask, list) or memories_mask.nelement() <= 0:
            memories_mask = torch.tensor(0)

        encoding = self.body(inputs, memories_mask)
        memories_mask.add_(1)

        return encoding, memories_mask

class WrapperActor(SimpleActor):
    """
    Wrapper actor receives a path to the model generated by the graph and uses it instead of ml-agents autogenerated models
    """
    
    MODEL_EXPORT_VERSION = 3  # Corresponds to ModelApiVersion.MLAgents2_0

    def __init__(
        self,
        observation_specs: List[ObservationSpec],
        network_settings: NetworkSettings,
        action_spec: ActionSpec,
        conditional_sigma: bool = False,
        tanh_squash: bool = False,
    ):
        super().__init__(observation_specs, network_settings, action_spec)
        self.action_spec = action_spec
        self.version_number = torch.nn.Parameter(
            torch.Tensor([self.MODEL_EXPORT_VERSION]), requires_grad=False
        )
        self.is_continuous_int_deprecated = torch.nn.Parameter(
            torch.Tensor([int(self.action_spec.is_continuous())]), requires_grad=False
        )
        self.continuous_act_size_vector = torch.nn.Parameter(
            torch.Tensor([int(self.action_spec.continuous_size)]), requires_grad=False
        )
        self.discrete_act_size_vector = torch.nn.Parameter(
            torch.Tensor([self.action_spec.discrete_branches]), requires_grad=False
        )
        self.act_size_vector_deprecated = torch.nn.Parameter(
            torch.Tensor(
                [
                    self.action_spec.continuous_size
                    + sum(self.action_spec.discrete_branches)
                ]
            ),
            requires_grad=False,
        )

        custom_settings = cast(CustomPPONetworkSettings, network_settings)

        path_to_model = custom_settings.path_to_model
        model_spec = importlib.util.spec_from_file_location("model", path_to_model)
        model_module = importlib.util.module_from_spec(model_spec)

        path_to_mapping = custom_settings.path_to_mapping
        mapping_spec = importlib.util.spec_from_file_location("mapping", path_to_mapping)
        mapping_module = importlib.util.module_from_spec(mapping_spec)

        self.network_body = ImportedNetworkBody(observation_specs, model_spec, model_module, mapping_spec, mapping_module)

        if network_settings.memory is not None:
            self.encoding_size = network_settings.memory.memory_size // 2
        else:
            self.encoding_size = network_settings.hidden_units
        self.memory_size_vector = torch.nn.Parameter(
            torch.Tensor([int(self.network_body.memory_size)]), requires_grad=False
        )

        exec("self.action_models = model_module.get_action_models(mapping_module.literals_mapping)")
        self.action_model = AggregatedActionModel(self.action_models, self.action_spec)

    @property
    def memory_size(self) -> int:
        return self.network_body.memory_size

    def update_normalization(self, buffer: AgentBuffer) -> None:
        self.network_body.update_normalization(buffer)

    def get_action_and_stats(
        self,
        inputs: List[torch.Tensor],
        masks: Optional[torch.Tensor] = None,
        memories: Optional[torch.Tensor] = None,
        sequence_length: int = 1,
    ) -> Tuple[AgentAction, Dict[str, Any], torch.Tensor]:

        encoding, memories = self.network_body(
            inputs, memories=memories, sequence_length=sequence_length
        )
        action, log_probs, entropies = self.action_model(encoding, masks)
        run_out = {}
        # This is the clipped action which is not saved to the buffer
        # but is exclusively sent to the environment.
        run_out["env_action"] = action.to_action_tuple(
            clip=self.action_model.clip_action
        )
        run_out["log_probs"] = log_probs
        run_out["entropy"] = entropies

        return action, run_out, memories

    def get_stats(
        self,
        inputs: List[torch.Tensor],
        actions: AgentAction,
        masks: Optional[torch.Tensor] = None,
        memories: Optional[torch.Tensor] = None,
        sequence_length: int = 1,
    ) -> Dict[str, Any]:
        encoding, actor_mem_outs = self.network_body(
            inputs, memories=memories, sequence_length=sequence_length
        )

        log_probs, entropies = self.action_model.evaluate(encoding, masks, actions)
        run_out = {}
        run_out["log_probs"] = log_probs
        run_out["entropy"] = entropies
        
        return run_out

    def forward(
        self,
        inputs: List[torch.Tensor],
        masks: Optional[torch.Tensor] = None,
        memories: Optional[torch.Tensor] = None,
    ) -> Tuple[Union[int, torch.Tensor], ...]:
        """
        Note: This forward() method is required for exporting to ONNX. Don't modify the inputs and outputs.

        At this moment, torch.onnx.export() doesn't accept None as tensor to be exported,
        so the size of return tuple varies with action spec.
        """

        encoding, memories_out = self.network_body(
            inputs, memories=memories, sequence_length=1
        )

        (
            cont_action_out,
            disc_action_out,
            action_out_deprecated,
            deterministic_cont_action_out,
            deterministic_disc_action_out,
        ) = self.action_model.get_action_out(encoding, masks)
        export_out = [self.version_number, self.memory_size_vector]
        if self.action_spec.continuous_size > 0:
            export_out += [
                cont_action_out,
                self.continuous_act_size_vector,
                deterministic_cont_action_out,
            ]
        if self.action_spec.discrete_size > 0:
            export_out += [
                disc_action_out,
                self.discrete_act_size_vector,
                deterministic_disc_action_out,
            ]
        if self.network_body.memory_size > 0:
            export_out += [memories_out]
        return tuple(export_out)
    
class WrapperSharedActorCritic(WrapperActor, Critic):
    def __init__(
        self,
        observation_specs: List[ObservationSpec],
        network_settings: NetworkSettings,
        action_spec: ActionSpec,
        stream_names: List[str],
        conditional_sigma: bool = False,
        tanh_squash: bool = False,
    ):
        self.use_lstm = network_settings.memory is not None
        super().__init__(
            observation_specs,
            network_settings,
            action_spec,
            conditional_sigma,
            tanh_squash,
        )
        self.stream_names = stream_names
        self.encoding_size = self.action_model.encoding_size
        self.value_heads = ValueHeads(stream_names, self.encoding_size)

    def critic_pass(
        self,
        inputs: List[torch.Tensor],
        memories: Optional[torch.Tensor] = None,
        sequence_length: int = 1,
    ) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:
        encoding, memories_out = self.network_body(
            inputs, memories=memories, sequence_length=sequence_length
        )

        return self.value_heads(encoding), memories_out