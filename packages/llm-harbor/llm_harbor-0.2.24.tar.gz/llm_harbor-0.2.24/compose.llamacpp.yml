services:
  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:full-cuda
    container_name: ${HARBOR_CONTAINER_PREFIX}.llamacpp
    volumes:
      - ${HARBOR_HF_CACHE}:/app/models
      - ${HARBOR_LLAMACPP_CACHE}:/root/.cache/llama.cpp
    ports:
      - ${HARBOR_LLAMACPP_HOST_PORT}:8080
    command: >
      --server
      ${HARBOR_LLAMACPP_MODEL_SPECIFIER}
      ${HARBOR_LLAMACPP_EXTRA_ARGS}
      --port 8080
      --host 0.0.0.0
    networks:
      - harbor-network