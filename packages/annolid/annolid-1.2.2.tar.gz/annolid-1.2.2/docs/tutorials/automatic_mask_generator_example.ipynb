{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa21d44",
      "metadata": {
        "id": "5fa21d44"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "References:\n",
        "1. https://github.com/facebookresearch/segment-anything\n",
        "2. https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\n",
        "3. https://github.com/openai/CLIP\n",
        "4.https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb"
      ],
      "metadata": {
        "id": "DSIuprsbkYal"
      },
      "id": "DSIuprsbkYal"
    },
    {
      "cell_type": "markdown",
      "id": "b7c0041e",
      "metadata": {
        "id": "b7c0041e"
      },
      "source": [
        "# Automatically generating object masks with SAM and Classify masks with CLIP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289bb0b4",
      "metadata": {
        "id": "289bb0b4"
      },
      "source": [
        "Since SAM can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image. This method was used to generate the dataset SA-1B.\n",
        "\n",
        "The class `SamAutomaticMaskGenerator` implements this capability. It works by sampling single-point input prompts in a grid over the image, from each of which SAM can predict multiple masks. Then, masks are filtered for quality and deduplicated using non-maximal suppression. Additional options allow for further improvement of mask quality and quantity, such as running prediction on multiple crops of the image or postprocessing masks to remove small disconnected regions and holes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072e25b8",
      "metadata": {
        "id": "072e25b8"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(\n",
        "\"\"\"\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/healthonrails/annolid/blob/main/docs/tutorials/automatic_mask_generator_example.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\"\"\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0b71431",
      "metadata": {
        "id": "c0b71431"
      },
      "source": [
        "## Environment Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47e5a78f",
      "metadata": {
        "id": "47e5a78f"
      },
      "source": [
        "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fe300fb",
      "metadata": {
        "id": "4fe300fb"
      },
      "outputs": [],
      "source": [
        "using_colab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0685a2f5",
      "metadata": {
        "id": "0685a2f5"
      },
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "    !mkdir images\n",
        "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n",
        "\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "ftXzhv7EO9Ru"
      },
      "id": "ftXzhv7EO9Ru",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install decord"
      ],
      "metadata": {
        "id": "oBGS7xwo_uAH"
      },
      "id": "oBGS7xwo_uAH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install MobileSAM"
      ],
      "metadata": {
        "id": "Q6nHAbU6gdsb"
      },
      "id": "Q6nHAbU6gdsb"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ChaoningZhang/MobileSAM.git\n"
      ],
      "metadata": {
        "id": "2MVpnr7mf7_l"
      },
      "id": "2MVpnr7mf7_l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MobileSAM/\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "iiw9id0LhRKV"
      },
      "id": "iiw9id0LhRKV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "tCBTK5DUhuWS"
      },
      "id": "tCBTK5DUhuWS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fd2bc687",
      "metadata": {
        "id": "fd2bc687"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "560725a2",
      "metadata": {
        "id": "560725a2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from pkg_resources import packaging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mobile_encoder.setup_mobile_sam import setup_model\n",
        "checkpoint = torch.load('./weights/mobile_sam.pt')\n",
        "mobile_sam = setup_model()\n",
        "mobile_sam.load_state_dict(checkpoint,strict=True)"
      ],
      "metadata": {
        "id": "XhATW6C9h7zF"
      },
      "id": "XhATW6C9h7zF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74b6e5f0",
      "metadata": {
        "id": "74b6e5f0"
      },
      "outputs": [],
      "source": [
        "def show_anns(anns):\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    ax = plt.gca()\n",
        "    ax.set_autoscale_on(False)\n",
        "\n",
        "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "    img[:,:,3] = 0\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
        "        img[m] = color_mask\n",
        "    ax.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c41445",
      "metadata": {
        "id": "27c41445"
      },
      "source": [
        "## Example image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad354922",
      "metadata": {
        "id": "ad354922"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread('/content/R2202_02-10-2023_000100275.png')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ac8c67",
      "metadata": {
        "id": "e0ac8c67"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8c2824a",
      "metadata": {
        "id": "b8c2824a"
      },
      "source": [
        "## Automatic mask generation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "ZiYfyuaqiekB"
      },
      "id": "ZiYfyuaqiekB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobile_sam.to(device=device)\n",
        "mobile_sam.eval()\n",
        "predictor = SamPredictor(mobile_sam)"
      ],
      "metadata": {
        "id": "dn6c8Xi6ihIU"
      },
      "id": "dn6c8Xi6ihIU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_generator = SamAutomaticMaskGenerator(mobile_sam)\n"
      ],
      "metadata": {
        "id": "LtP0Dyx3iUuZ"
      },
      "id": "LtP0Dyx3iUuZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d9ef74c5",
      "metadata": {
        "id": "d9ef74c5"
      },
      "source": [
        "To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1848a108",
      "metadata": {
        "id": "1848a108"
      },
      "outputs": [],
      "source": [
        "\n",
        "# sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "# model_type = \"vit_h\"\n",
        "# sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "# sam.to(device=device)\n",
        "\n",
        "# mask_generator = SamAutomaticMaskGenerator(sam)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b1ea21",
      "metadata": {
        "id": "d6b1ea21"
      },
      "source": [
        "To generate masks, just run `generate` on an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "391771c1",
      "metadata": {
        "id": "391771c1"
      },
      "outputs": [],
      "source": [
        "masks = mask_generator.generate(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e36a1a39",
      "metadata": {
        "id": "e36a1a39"
      },
      "source": [
        "Mask generation returns a list over masks, where each mask is a dictionary containing various data about the mask. These keys are:\n",
        "* `segmentation` : the mask\n",
        "* `area` : the area of the mask in pixels\n",
        "* `bbox` : the boundary box of the mask in XYWH format\n",
        "* `predicted_iou` : the model's own prediction for the quality of the mask\n",
        "* `point_coords` : the sampled input point that generated this mask\n",
        "* `stability_score` : an additional measure of mask quality\n",
        "* `crop_box` : the crop of the image used to generate this mask in XYWH format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fae8d66",
      "metadata": {
        "id": "4fae8d66"
      },
      "outputs": [],
      "source": [
        "print(len(masks))\n",
        "print(masks[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53009a1f",
      "metadata": {
        "id": "53009a1f"
      },
      "source": [
        "Show all the masks overlayed on the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ac29c5",
      "metadata": {
        "scrolled": false,
        "id": "77ac29c5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image)\n",
        "show_anns(masks)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b3d6b2",
      "metadata": {
        "id": "00b3d6b2"
      },
      "source": [
        "## Automatic mask generation options"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "183de84e",
      "metadata": {
        "id": "183de84e"
      },
      "source": [
        "There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68364513",
      "metadata": {
        "id": "68364513"
      },
      "outputs": [],
      "source": [
        "mask_generator_2 = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=32,\n",
        "    pred_iou_thresh=0.86,\n",
        "    stability_score_thresh=0.92,\n",
        "    crop_n_layers=1,\n",
        "    crop_n_points_downscale_factor=2,\n",
        "    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bebcdaf1",
      "metadata": {
        "id": "bebcdaf1"
      },
      "outputs": [],
      "source": [
        "masks2 = mask_generator_2.generate(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8473f3c",
      "metadata": {
        "id": "b8473f3c"
      },
      "outputs": [],
      "source": [
        "len(masks2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb702ae3",
      "metadata": {
        "id": "fb702ae3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image)\n",
        "show_anns(masks2)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c937160",
      "metadata": {
        "id": "8c937160"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def crop_image_with_masks(image,\n",
        "                          masks,\n",
        "                          max_area=8000,\n",
        "                          min_area=500,\n",
        "                          width_height_ratio=0.9):\n",
        "    \"\"\"\n",
        "    Crop the image based on provided masks and apply the masks to each cropped region.\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): The input image.\n",
        "        masks (list): A list of dictionaries containing mask data.\n",
        "        max_area (int): Max area of the mask\n",
        "        min_area (int): Min area of the mask\n",
        "        width_height_ratio(float): Min width / height\n",
        "\n",
        "    Returns:\n",
        "        list: A list of cropped images with applied masks.\n",
        "    \"\"\"\n",
        "    cropped_images = []\n",
        "\n",
        "    for mask_data in masks:\n",
        "        # Extract mask and bounding box data\n",
        "        bbox = mask_data['bbox']\n",
        "        seg = mask_data['segmentation']\n",
        "        x, y, w, h = bbox\n",
        "\n",
        "        # Crop the image based on the bounding box\n",
        "        cropped_image = image[y:y+h, x:x+w]\n",
        "\n",
        "        # Create an 8-bit mask from the segmentation data\n",
        "        mask = np.asarray(seg[y:y+h, x:x+w], dtype=np.uint8) * 255\n",
        "        # Apply the mask to the cropped image\n",
        "        cropped_image = cv2.bitwise_and(cropped_image, cropped_image, mask=mask)\n",
        "        cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
        "        if mask_data['area'] >= min_area and mask_data['area'] <= max_area and w/h >= width_height_ratio:\n",
        "            cropped_images.append(cropped_image)\n",
        "\n",
        "    return cropped_images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cropped_images = crop_image_with_masks(image,masks)"
      ],
      "metadata": {
        "id": "PzauTuMuJsNC"
      },
      "id": "PzauTuMuJsNC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cimg in cropped_images:\n",
        "    cv2_imshow(cimg)"
      ],
      "metadata": {
        "id": "0KwoxT9MJ5v-"
      },
      "id": "0KwoxT9MJ5v-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify masks with openai/CLIP"
      ],
      "metadata": {
        "id": "ZFo7Y94-kuwW"
      },
      "id": "ZFo7Y94-kuwW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the model\n",
        "\n",
        "`clip.available_models()` will list the names of available CLIP models."
      ],
      "metadata": {
        "id": "Gr1Z2G7sj38_"
      },
      "id": "Gr1Z2G7sj38_"
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ],
      "metadata": {
        "id": "uy--NUyyQnEB"
      },
      "id": "uy--NUyyQnEB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "sRQkK66tQqBf"
      },
      "id": "sRQkK66tQqBf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up input images and texts"
      ],
      "metadata": {
        "id": "ob3e9Qykk8xC"
      },
      "id": "ob3e9Qykk8xC"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# images to use and their textual descriptions\n",
        "descriptions = {\n",
        "    \"rat\": \"a small mammal with fur, long tail, and a white stripe on its back\",\n",
        "    \"hand\": \"a human hand with fingers and palm\",\n",
        "    \"arm\": \"a human arm extending from the shoulder to the hand\",\n",
        "    \"cup\": \"a petri dish used for holding odor treated white or yellow sand\",\n",
        "    \"book\": \"a bound collection of paper sheets used for writing or reading\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "fA58S5t3Q5ZN"
      },
      "id": "fA58S5t3Q5ZN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-Shot Image Classification\n",
        "\n",
        "You can classify images using the cosine similarity (times 100) as the logits to the softmax operation.\n"
      ],
      "metadata": {
        "id": "AT534INDlOHn"
      },
      "id": "AT534INDlOHn"
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = list(descriptions.keys())\n",
        "class_names"
      ],
      "metadata": {
        "id": "QSjatEIRUfS1"
      },
      "id": "QSjatEIRUfS1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_descriptions = [f\"This is a photo of a {label}, {descriptions[label]}\" for label in class_names]\n",
        "text_tokens = clip.tokenize(text_descriptions).cuda()"
      ],
      "metadata": {
        "id": "BLNrnxtmR8EZ"
      },
      "id": "BLNrnxtmR8EZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "xsOlPLJgR9b3"
      },
      "id": "xsOlPLJgR9b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_probs_and_labels(image, masks, model, text_features):\n",
        "    \"\"\"\n",
        "    Computes the top probabilities and labels for matching text and images.\n",
        "\n",
        "    Args:\n",
        "        image (ndarray): The input image.\n",
        "        masks (ndarray): The masks for cropping the image.\n",
        "        model: The CLIP model used for encoding image features.\n",
        "        text_features: The text features used for comparison.\n",
        "\n",
        "    Returns:\n",
        "        top_probs (Tensor): Top probabilities of text matching for the cropped images.\n",
        "        top_labels (Tensor): Top labels corresponding to the top probabilities.\n",
        "    \"\"\"\n",
        "    # Crop images using masks\n",
        "    cropped_images = crop_image_with_masks(image, masks)\n",
        "\n",
        "    # Preprocess cropped images\n",
        "    images = [preprocess(Image.fromarray(cimg)) for cimg in cropped_images]\n",
        "\n",
        "    # Convert images to tensor and move to GPU\n",
        "    image_input = torch.tensor(np.stack(images)).cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode image features\n",
        "        image_features = model.encode_image(image_input).float()\n",
        "\n",
        "        # Calculate text probabilities\n",
        "        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "        # Get top probabilities and labels\n",
        "        top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\n",
        "\n",
        "    return top_probs, top_labels\n"
      ],
      "metadata": {
        "id": "Y3SvSytLCrLI"
      },
      "id": "Y3SvSytLCrLI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mask_features(image, mask, model):\n",
        "    \"\"\"\n",
        "    Computes the features of the mask portion of an image.\n",
        "\n",
        "    Args:\n",
        "        image (ndarray): The input image.\n",
        "        mask (ndarray): The mask for cropping the image.\n",
        "        model: The CLIP model used for encoding image features.\n",
        "\n",
        "    Returns:\n",
        "        mask_features (Tensor): The features of the mask portion of the image.\n",
        "    \"\"\"\n",
        "    # Apply the mask to the image\n",
        "    masked_image = image.copy()\n",
        "    masked_image[~mask] = 0\n",
        "\n",
        "    # Preprocess the masked image\n",
        "    masked_image = preprocess(Image.fromarray(masked_image))\n",
        "\n",
        "    # Convert the image to tensor and move to GPU\n",
        "    #image_input = torch.tensor(masked_image).unsqueeze(0).cuda()\n",
        "    image_input = masked_image.unsqueeze(0).cuda().float()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode image features\n",
        "        image_features = model.encode_image(image_input).float()\n",
        "\n",
        "    return image_features.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "nhYMcIb3uwZY"
      },
      "id": "nhYMcIb3uwZY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_probs, top_labels = get_top_probs_and_labels(image, masks, model, text_features)"
      ],
      "metadata": {
        "id": "3IvKOr6HC1P4"
      },
      "id": "3IvKOr6HC1P4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = int(np.ceil(len(cropped_images)/ 2))"
      ],
      "metadata": {
        "id": "0ZZLY96mVjvP"
      },
      "id": "0ZZLY96mVjvP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 16))\n",
        "\n",
        "for i, _image in enumerate(cropped_images):\n",
        "    _image = cv2.cvtColor(_image,cv2.COLOR_BGR2RGB)\n",
        "    plt.subplot(num_rows, 4, 2 * i + 1)\n",
        "    plt.imshow(_image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(num_rows, 4, 2 * i + 2)\n",
        "    y = np.arange(top_probs.shape[-1])\n",
        "    plt.grid()\n",
        "    plt.barh(y, top_probs[i])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.gca().set_axisbelow(True)\n",
        "    plt.yticks(y, [class_names[index] for index in top_labels[i].numpy()])\n",
        "    plt.xlabel(\"probability\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LL6nyaVVSCNU"
      },
      "id": "LL6nyaVVSCNU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import euclidean, cosine\n",
        "\n",
        "def generate_mask_id(mask_features, existing_masks, threshold=6.0, distance_metric=\"euclidean\"):\n",
        "    \"\"\"\n",
        "    Generates an ID for the mask based on its features and compares it with existing masks.\n",
        "\n",
        "    Args:\n",
        "        mask_features (ndarray): The features of the mask.\n",
        "        existing_masks (list): List of existing masks and their features.\n",
        "        threshold (float): Similarity threshold for considering a match (default: 0.9).\n",
        "        distance_metric (str): Distance metric to be used (default: \"euclidean\").\n",
        "                               Options: \"euclidean\", \"cosine\".\n",
        "\n",
        "    Returns:\n",
        "        mask_id (int): The generated ID for the mask.\n",
        "    \"\"\"\n",
        "    mask_id = -1  # Initialize the mask ID\n",
        "\n",
        "    if distance_metric == \"euclidean\":\n",
        "        distance_function = euclidean\n",
        "    elif distance_metric == \"cosine\":\n",
        "        distance_function = cosine\n",
        "    else:\n",
        "        raise ValueError(\"Invalid distance metric. Choose either 'euclidean' or 'cosine'.\")\n",
        "\n",
        "    for idx, (existing_id, existing_features) in enumerate(existing_masks):\n",
        "        similarity = distance_function(mask_features.flatten(), existing_features.flatten())\n",
        "\n",
        "        if similarity < threshold:\n",
        "            mask_id = existing_id\n",
        "            break\n",
        "\n",
        "    if mask_id == -1:\n",
        "        mask_id = len(existing_masks) + 1  # Assign a new ID if no match is found\n",
        "        existing_masks.append((mask_id, mask_features.flatten()))\n",
        "\n",
        "    return mask_id\n"
      ],
      "metadata": {
        "id": "pVufxTIGwHrg"
      },
      "id": "pVufxTIGwHrg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pycocotools.mask as mask_util\n",
        "def convert_to_annolid_format(frame_number,\n",
        "                              masks,\n",
        "                              frame=None,\n",
        "                              model=None,\n",
        "                              min_mask_area=float('-inf'),\n",
        "                              max_mask_area=float('inf'),\n",
        "                              existing_masks=None\n",
        "                              ):\n",
        "    \"\"\"Converts predicted SAM masks information to annolid format.\n",
        "\n",
        "    Args:\n",
        "        frame_number (int): The frame number associated with the masks.\n",
        "        masks (list): List of dictionaries representing the predicted masks.\n",
        "            Each dictionary should contain the following keys:\n",
        "                -segmentation : the mask\n",
        "                -area : the area of the mask in pixels\n",
        "                -bbox : the boundary box of the mask in XYWH format\n",
        "                -predicted_iou : the model's own prediction for the quality of the mask\n",
        "                -point_coords : the sampled input point that generated this mask\n",
        "                -stability_score : an additional measure of mask quality\n",
        "                -crop_box : the crop of the image used to generate this mask in XYWH format\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries representing the masks in annolid format.\n",
        "            Each dictionary contains the following keys:\n",
        "                - \"frame_number\": The frame number associated with the masks.\n",
        "                - \"x1\", \"y1\", \"x2\", \"y2\": The coordinates of the bounding box in XYXY format.\n",
        "                - \"instance_name\": The name of the instance/object.\n",
        "                - \"class_score\": The predicted IoU (Intersection over Union) for the mask.\n",
        "                - \"segmentation\": The segmentation mask.\n",
        "                - \"tracking_id\": The tracking ID associated with the mask.\n",
        "\n",
        "    \"\"\"\n",
        "    pred_rows = []\n",
        "    for mask in masks:\n",
        "        mask_area = mask.get(\"area\",0)\n",
        "        if min_mask_area <= mask_area <= max_mask_area:\n",
        "            x1 = mask.get(\"bbox\")[0]\n",
        "            y1 = mask.get(\"bbox\")[1]\n",
        "            x2 = mask.get(\"bbox\")[0] + mask.get(\"bbox\")[2]\n",
        "            y2 = mask.get(\"bbox\")[1] + mask.get(\"bbox\")[3]\n",
        "            score = mask.get(\"predicted_iou\", '')\n",
        "            segmentation = mask.get(\"segmentation\", '')\n",
        "            mask_features = get_mask_features(frame,segmentation,model)\n",
        "            mask_id = generate_mask_id(mask_features,existing_masks)\n",
        "            instance_name = mask.get(\"instance_name\", f'instance_{mask_id}')\n",
        "            segmentation = mask_util.encode(segmentation)\n",
        "            tracking_id = mask.get(\"tracking_id\", \"\")\n",
        "\n",
        "            pred_rows.append({\n",
        "                \"frame_number\": frame_number,\n",
        "                \"x1\": x1,\n",
        "                \"y1\": y1,\n",
        "                \"x2\": x2,\n",
        "                \"y2\": y2,\n",
        "                \"instance_name\": instance_name,\n",
        "                \"class_score\": score,\n",
        "                \"segmentation\": segmentation,\n",
        "                \"tracking_id\": tracking_id\n",
        "            })\n",
        "\n",
        "    return pred_rows"
      ],
      "metadata": {
        "id": "2gTJzSoHH1eh"
      },
      "id": "2gTJzSoHH1eh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_rows = convert_to_annolid_format(100275,masks,image,model,existing_masks=[])"
      ],
      "metadata": {
        "id": "1VQHcWtUICLG"
      },
      "id": "1VQHcWtUICLG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(predict_rows)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uMdL-T1VI9RW"
      },
      "id": "uMdL-T1VI9RW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"rats_v1_coco_dataset_R2202_02-10-2023_mask_rcnn_tracking_results_with_segmentation.csv\")"
      ],
      "metadata": {
        "id": "dCmhKBpOJwVM"
      },
      "id": "dCmhKBpOJwVM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example video"
      ],
      "metadata": {
        "id": "c5_Z-QVlZhfm"
      },
      "id": "c5_Z-QVlZhfm"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/sleap-data/datasets/eleni_mice/clips/20200111_USVpairs_court1_M1_F1_top-01112020145828-0000%400-2560.mp4"
      ],
      "metadata": {
        "id": "eKQwq-V054Xq"
      },
      "id": "eKQwq-V054Xq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_video_file = \"/content/MobileSAM/20200111_USVpairs_court1_M1_F1_top-01112020145828-0000@0-2560.mp4\"\n",
        "video_file = '/content/usvpairs_court1.mp4'"
      ],
      "metadata": {
        "id": "1LNsqjphZgmV"
      },
      "id": "1LNsqjphZgmV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cut 2s of the video starting from second 30"
      ],
      "metadata": {
        "id": "JdEnEz6T4Miy"
      },
      "id": "JdEnEz6T4Miy"
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i {original_video_file} -ss 00:00:30 -t 00:00:02 -c:v copy -c:a copy {video_file}"
      ],
      "metadata": {
        "id": "71tGxPvw3Y5X"
      },
      "id": "71tGxPvw3Y5X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import decord as de\n",
        "import pandas as pd\n",
        "\n",
        "def process_video_and_save_tracking_results(video_file, mask_generator):\n",
        "    \"\"\"\n",
        "    Process a video file, generate tracking results with segmentation masks,\n",
        "    and save the results to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        video_file (str): Path to the video file.\n",
        "        mask_generator: An instance of the mask generator class.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    video_reader = de.VideoReader(video_file)\n",
        "    tracking_results = []\n",
        "    existing_masks = []\n",
        "\n",
        "    for key_index in video_reader.get_key_indices():\n",
        "        frame = video_reader[key_index].asnumpy()\n",
        "        masks = mask_generator.generate(frame)\n",
        "        tracking_results += convert_to_annolid_format(key_index, masks,frame,model,existing_masks=existing_masks)\n",
        "        print(key_index)\n",
        "\n",
        "    dataframe = pd.DataFrame(tracking_results)\n",
        "    output_file = f\"{video_file.split('.')[0]}_mask_tracking_results_with_segmentation.csv\"\n",
        "    dataframe.to_csv(output_file)\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "5Ub-Zbz20lYW"
      },
      "id": "5Ub-Zbz20lYW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tracking_results_file = process_video_and_save_tracking_results(video_file, mask_generator)"
      ],
      "metadata": {
        "id": "TxXOm0l33A92"
      },
      "id": "TxXOm0l33A92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.files import download"
      ],
      "metadata": {
        "id": "Wi8mdTLv6Jyq"
      },
      "id": "Wi8mdTLv6Jyq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download(tracking_results_file)"
      ],
      "metadata": {
        "id": "W8T_qKD56QkV"
      },
      "id": "W8T_qKD56QkV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download(video_file)"
      ],
      "metadata": {
        "id": "D2DCBv519Ev0"
      },
      "id": "D2DCBv519Ev0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}