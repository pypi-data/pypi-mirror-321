{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "PointRend in Detectron2 Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/healthonrails/annolid/blob/main/docs/tutorials/PointRend_in_Detectron2_Tutorial.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ],
      "metadata": {
        "id": "AZtQQyOgelW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"[PointRend](https://arxiv.org/abs/1912.08193) in Detectron2\" Tutorial\n",
        "\n",
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "Welcome to the [PointRend project](https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend) in detectron2! In this tutorial, we will go through some basics usage of PointRend, including the following:\n",
        "* Run inference on images or videos, with an existing PointRend model\n",
        "* Look into PointRend internal representation.\n",
        "\n",
        "You can make a copy of this tutorial or use \"File -> Open in playground mode\" to play with it yourself.\n"
      ],
      "metadata": {
        "id": "QHnVupBBn9eR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install detectron2"
      ],
      "metadata": {
        "id": "vM54r6jlKTII"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# install dependencies: \n",
        "!pip install pyyaml==5.1\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab"
      ],
      "outputs": [],
      "metadata": {
        "id": "9_FzH13EjseR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# install detectron2: (Colab has CUDA 10.1 + torch 1.8)\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "import torch\n",
        "assert torch.__version__.startswith(\"1.8\")   # need to manually install torch 1.8 if Colab changes its default version\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "# exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
      ],
      "outputs": [],
      "metadata": {
        "id": "iUmK767kWQlq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# clone the repo in order to access pre-defined configs\n",
        "!git clone --branch v0.3 https://github.com/facebookresearch/detectron2.git detectron2_repo"
      ],
      "outputs": [],
      "metadata": {
        "id": "M1QQpW-Msqnf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2.data import MetadataCatalog\n",
        "coco_metadata = MetadataCatalog.get(\"coco_2017_val\")\n",
        "\n",
        "# import PointRend project\n",
        "from detectron2.projects import point_rend"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload a labeled COCO format dataset as follows"
      ],
      "metadata": {
        "id": "LdIK_wzhrydb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "dataset =  list(uploaded.keys())[0]  #\"/content/dataset.zip\""
      ],
      "outputs": [],
      "metadata": {
        "id": "-w-hdUHDrxDb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# please uncomment and change the dataset location in case your dataset is\n",
        "# already upload to colab\n",
        "#dataset = \"/content/novelctrlk6_8_coco_dataset\""
      ],
      "outputs": [],
      "metadata": {
        "id": "Rz6vwjopulPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!unzip $dataset -d /content/"
      ],
      "outputs": [],
      "metadata": {
        "id": "pnO3VMUzO5-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a pre-trained PointRend model"
      ],
      "metadata": {
        "id": "Vk4gID50K03a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first download an image from the COCO dataset:"
      ],
      "metadata": {
        "id": "JgKyUL4pngvE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "im = cv2.imread(\"/content/novelctrlk6_8_coco_dataset/train/JPEGImages/00001980_61.jpg\")\n",
        "cv2_imshow(im)"
      ],
      "outputs": [],
      "metadata": {
        "id": "dq9GY37ml1kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image. First, we make a prediction using a standard Mask R-CNN model."
      ],
      "metadata": {
        "id": "uM1thbN-ntjI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "mask_rcnn_predictor = DefaultPredictor(cfg)\n",
        "mask_rcnn_outputs = mask_rcnn_predictor(im)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "VkPgSoYvVAEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we load a PointRend model and show its prediction."
      ],
      "metadata": {
        "id": "r7sw7uz9Vyo-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cfg = get_cfg()\n",
        "# Add PointRend-specific config\n",
        "point_rend.add_pointrend_config(cfg)\n",
        "# Load a config from file\n",
        "cfg.merge_from_file(\"detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.15  # set threshold for this model\n",
        "# Use a model from PointRend model zoo: https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend#pretrained-models\n",
        "cfg.MODEL.WEIGHTS = \"detectron2://PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_3c3198.pkl\"\n",
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(im)"
      ],
      "outputs": [],
      "metadata": {
        "id": "HUjkwRsOn1O0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Show and compare two predictions: \n",
        "v = Visualizer(im[:, :, ::-1], coco_metadata, scale=1.2, instance_mode=ColorMode.IMAGE_BW)\n",
        "mask_rcnn_result = v.draw_instance_predictions(mask_rcnn_outputs[\"instances\"].to(\"cpu\")).get_image()\n",
        "v = Visualizer(im[:, :, ::-1], coco_metadata, scale=1.2, instance_mode=ColorMode.IMAGE_BW)\n",
        "point_rend_result = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\")).get_image()\n",
        "print(\"Mask R-CNN with PointRend (top)     vs.     Default Mask R-CNN (bottom)\")\n",
        "cv2_imshow(np.concatenate((point_rend_result, mask_rcnn_result), axis=0)[:, :, ::-1])"
      ],
      "outputs": [],
      "metadata": {
        "id": "8IRGo8d0qkgR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "DATASET_NAME = f\"{os.path.basename(dataset).split('_')[0]}\" \n",
        "DATASET_DIR = f\"{dataset.replace('.zip','')}\""
      ],
      "outputs": [],
      "metadata": {
        "id": "1QWiQ9nETI15"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(f\"{DATASET_NAME}_train\", {}, f\"{DATASET_DIR}/train/annotations.json\", f\"{DATASET_DIR}/train/\")\n",
        "register_coco_instances(f\"{DATASET_NAME}_valid\", {}, f\"{DATASET_DIR}/valid/annotations.json\", f\"{DATASET_DIR}/valid/\")\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "0mmZAnYrS-mC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from detectron2.data import get_detection_dataset_dicts\n",
        "dataset_dicts = get_detection_dataset_dicts([f\"{DATASET_NAME}_train\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "t0xBcd_bTYhV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "_dataset_metadata = MetadataCatalog.get(f\"{DATASET_NAME}_train\")\n",
        "_dataset_metadata\n",
        "_dataset_metadata.thing_classes\n",
        "_dataset_metadata.thing_dataset_id_to_contiguous_id\n",
        "_dataset_metadata.thing_colors = coco_metadata.thing_colors"
      ],
      "outputs": [],
      "metadata": {
        "id": "n4maZCWOTsvz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "NUM_CLASSES = len(_dataset_metadata.thing_classes)\n",
        "print(f\"{NUM_CLASSES} Number of classes in the dataset\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "QFruYEfVUU1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import random\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=_dataset_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "outputs": [],
      "metadata": {
        "id": "5hk8BEejUaEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "PoS-aOTTUlqb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!nvidia-smi"
      ],
      "outputs": [],
      "metadata": {
        "id": "wPbJolUDUiTu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "cfg = get_cfg()\n",
        "point_rend.add_pointrend_config(cfg)\n",
        "cfg.merge_from_file(\"detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\")\n",
        "cfg.DATASETS.TRAIN = (f\"{DATASET_NAME}_train\",)\n",
        "cfg.DATASETS.TEST = (f\"{DATASET_NAME}_valid\")\n",
        "cfg.DATALOADER.NUM_WORKERS = 2 #@param\n",
        "cfg.DATALOADER.SAMPLER_TRAIN = \"RepeatFactorTrainingSampler\"\n",
        "cfg.DATALOADER.REPEAT_THRESHOLD = 0.3\n",
        "#cfg.MODEL.WEIGHTS = \"detectron2://PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_3c3198.pkl\"\n",
        "cfg.SOLVER.IMS_PER_BATCH =  4#@param\n",
        "cfg.SOLVER.BASE_LR = 0.0025 #@param # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 2000 #@param    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128 #@param   # faster, and good enough for this toy dataset (default: 512)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "8lyEmUE7Uq6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES  #  (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "cfg.MODEL.POINT_HEAD.NUM_CLASSES = NUM_CLASSES "
      ],
      "outputs": [],
      "metadata": {
        "id": "4LXIIY4oWrZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Z7Fei4YpWv7W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ],
      "outputs": [],
      "metadata": {
        "id": "LL5-1kEtXKYg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "trainer.train()"
      ],
      "outputs": [],
      "metadata": {
        "id": "2GVW_UgUXTiX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Inference should use the config with parameters that are used in training\n",
        "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
        "#cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "#cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.15   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "outputs": [],
      "metadata": {
        "id": "jcFYbnp5YtwV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "dataset_dicts = get_detection_dataset_dicts([f\"{DATASET_NAME}_valid\"])\n",
        "for d in dataset_dicts:    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=_dataset_metadata, \n",
        "                   scale=0.5, \n",
        "                   instance_mode=ColorMode.SEGMENTATION   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "outputs": [],
      "metadata": {
        "id": "Viaj3POyhybG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(f\"{DATASET_NAME}_valid\", cfg, False, output_dir=\"/content/eval_output/\")\n",
        "val_loader = build_detection_test_loader(cfg, f\"{DATASET_NAME}_valid\")\n",
        "print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
        "# another equivalent way to evaluate the model is to use `trainer.test`"
      ],
      "outputs": [],
      "metadata": {
        "id": "_7euLYqBh4OM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "VIDEO_INPUT=\"/content/novelobjectK6Fcontrol.mkv\" #@param {type: \"string\"}\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "x18y4yOPn4Dn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import cv2\n",
        "video = cv2.VideoCapture(VIDEO_INPUT)\n",
        "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frames_per_second = video.get(cv2.CAP_PROP_FPS)\n",
        "num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "basename = os.path.basename(VIDEO_INPUT)\n",
        "OUTPUT_DIR = \"/content/eval_output\"\n",
        "import os \n",
        "os.makedirs(OUTPUT_DIR,exist_ok=True)\n",
        "output_fname = os.path.join(OUTPUT_DIR, basename)\n",
        "output_fname = os.path.splitext(output_fname)[0] + \"_mask_tracked.mp4\"\n",
        "output_file = cv2.VideoWriter(\n",
        "                filename=output_fname,\n",
        "                # some installation of opencv may not support x264 (due to its license),\n",
        "                # you can try other format (e.g. MPEG)\n",
        "                fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "                fps=float(frames_per_second),\n",
        "                frameSize=(width, height),\n",
        "                isColor=True,\n",
        "            )"
      ],
      "outputs": [],
      "metadata": {
        "id": "mWdmfTDQh7_-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def _frame_from_video(video):\n",
        "  while video.isOpened():\n",
        "      success, frame = video.read()\n",
        "      if success:\n",
        "          yield frame\n",
        "      else:\n",
        "          break"
      ],
      "outputs": [],
      "metadata": {
        "id": "67EOSS-qiRlJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\n",
        "import pycocotools.mask as mask_util\n",
        "class_names = _dataset_metadata.thing_classes\n",
        "print(class_names)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "CepnUdQ6iUFr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "frame_number = 0\n",
        "tracking_results = []\n",
        "VIS = True\n",
        "for frame in _frame_from_video(video): \n",
        "    im = frame\n",
        "    outputs = predictor(im)\n",
        "    out_dict = {}  \n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    num_instance = len(instances)\n",
        "    if num_instance == 0:\n",
        "        out_dict['frame_number'] = frame_number\n",
        "        out_dict['x1'] = None\n",
        "        out_dict['y1'] = None\n",
        "        out_dict['x2'] = None\n",
        "        out_dict['y2'] = None\n",
        "        out_dict['instance_name'] = None\n",
        "        out_dict['class_score'] = None\n",
        "        out_dict['segmentation'] = None\n",
        "        tracking_results.append(out_dict)\n",
        "        out_dict = {}\n",
        "    else:\n",
        "        boxes = instances.pred_boxes.tensor.numpy()\n",
        "        boxes = boxes.tolist()\n",
        "        scores = instances.scores.tolist()\n",
        "        classes = instances.pred_classes.tolist()\n",
        "\n",
        "        has_mask = instances.has(\"pred_masks\")\n",
        "\n",
        "        if has_mask:\n",
        "            rles =[\n",
        "                   mask_util.encode(np.array(mask[:,:,None], order=\"F\", dtype=\"uint8\"))[0]\n",
        "                   for mask in instances.pred_masks\n",
        "            ]\n",
        "            for rle in rles:\n",
        "              rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "        assert len(rles) == len(boxes)\n",
        "        for k in range(num_instance):\n",
        "            box = boxes[k]\n",
        "            out_dict['frame_number'] = frame_number\n",
        "            out_dict['x1'] = box[0]\n",
        "            out_dict['y1'] = box[1]\n",
        "            out_dict['x2'] = box[2]\n",
        "            out_dict['y2'] = box[3]\n",
        "            out_dict['instance_name'] = class_names[classes[k]]\n",
        "            out_dict['class_score'] = scores[k]\n",
        "            out_dict['segmentation'] = rles[k]\n",
        "            if frame_number % 1000 == 0:\n",
        "              print(f\"Frame number {frame_number}: {out_dict}\")\n",
        "            tracking_results.append(out_dict)\n",
        "            out_dict = {}\n",
        "        \n",
        "    # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    if VIS:\n",
        "        v = Visualizer(im[:, :, ::-1],\n",
        "                    metadata=_dataset_metadata, \n",
        "                    scale=0.5, \n",
        "                    instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "         )\n",
        "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "        out_image = out.get_image()[:, :, ::-1]\n",
        "        output_file.write(out_image)\n",
        "        if frame_number % 1000 == 0:\n",
        "            cv2_imshow(out_image)\n",
        "    frame_number += 1\n",
        "    print(f\"Processing frame number {frame_number}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "q2crUiQ7icqL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "video.release()\n",
        "output_file.release()"
      ],
      "outputs": [],
      "metadata": {
        "id": "yFTV5OGcidlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = pd.DataFrame(tracking_results)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "63W-_tLzioci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "r1zbRa-KitCY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.tail()"
      ],
      "outputs": [],
      "metadata": {
        "id": "sIXs8AE9iuYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_top = df.groupby(['frame_number','instance_name'],sort=False).head(1)\n",
        "df_top.head()\n",
        "tracking_results_csv = \"/content/con_01_pointrend_tracking_results_with_segmenation.csv\" #@param\n",
        "df_top.to_csv(tracking_results_csv)"
      ],
      "outputs": [],
      "metadata": {
        "id": "4ZCiFcTui97E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize PointRend point sampling process"
      ],
      "metadata": {
        "id": "SPKTWi-IMDMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we show how PointRend's point sampling process works. To do this, we need to access intermediate representations of the predictor `model.forward(...)` function. Thus, we run forward step manually copying the code step by step."
      ],
      "metadata": {
        "id": "BxtfjespMah-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# First we define a simple function to help us plot the intermediate representations.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_mask(mask, title=\"\", point_coords=None, figsize=10, point_marker_size=5):\n",
        "  '''\n",
        "  Simple plotting tool to show intermediate mask predictions and points \n",
        "  where PointRend is applied.\n",
        "  \n",
        "  Args:\n",
        "    mask (Tensor): mask prediction of shape HxW\n",
        "    title (str): title for the plot\n",
        "    point_coords ((Tensor, Tensor)): x and y point coordinates\n",
        "    figsize (int): size of the figure to plot\n",
        "    point_marker_size (int): marker size for points\n",
        "  '''\n",
        "\n",
        "  H, W = mask.shape\n",
        "  plt.figure(figsize=(figsize, figsize))\n",
        "  if title:\n",
        "    title += \", \"\n",
        "  plt.title(\"{}resolution {}x{}\".format(title, H, W), fontsize=30)\n",
        "  plt.ylabel(H, fontsize=30)\n",
        "  plt.xlabel(W, fontsize=30)\n",
        "  plt.xticks([], [])\n",
        "  plt.yticks([], [])\n",
        "  plt.imshow(mask, interpolation=\"nearest\", cmap=plt.get_cmap('gray'))\n",
        "  if point_coords is not None:\n",
        "    plt.scatter(x=point_coords[0], y=point_coords[1], color=\"red\", s=point_marker_size, clip_on=True) \n",
        "  plt.xlim(-0.5, W - 0.5)\n",
        "  plt.ylim(H - 0.5, - 0.5)\n",
        "  plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "-XArt11tUGT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `predictor` and `im` loaded in the previous section we run backbone, bounding box prediction, and coarse mask segmenation head. We visualize mask prediction for the foreground plane on the image."
      ],
      "metadata": {
        "id": "yrzdyCaws5Mo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from detectron2.data import transforms as T\n",
        "model = predictor.model\n",
        "# In this image we detect several objects but show only the first one.\n",
        "instance_idx = 0\n",
        "# Mask predictions are class-specific, \"plane\" class has id 4.\n",
        "category_idx = 4\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Prepare input image.\n",
        "  height, width = im.shape[:2]\n",
        "  im_transformed = T.ResizeShortestEdge(800, 1333).get_transform(im).apply_image(im)\n",
        "  batched_inputs = [{\"image\": torch.as_tensor(im_transformed).permute(2, 0, 1)}]\n",
        "\n",
        "  # Get bounding box predictions first to simplify the code.\n",
        "  detected_instances = [x[\"instances\"] for x in model.inference(batched_inputs)]\n",
        "  [r.remove(\"pred_masks\") for r in detected_instances]  # remove existing mask predictions\n",
        "  pred_boxes = [x.pred_boxes for x in detected_instances] \n",
        "\n",
        "  # Run backbone.\n",
        "  images = model.preprocess_image(batched_inputs)\n",
        "  features = model.backbone(images.tensor)\n",
        "  \n",
        "  # Given the bounding boxes, run coarse mask prediction head.\n",
        "  mask_coarse_logits = model.roi_heads._forward_mask_coarse(features, pred_boxes)\n",
        "\n",
        "  plot_mask(\n",
        "      mask_coarse_logits[instance_idx, category_idx].to(\"cpu\"),\n",
        "      title=\"Coarse prediction\"\n",
        "  )\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "-meq_L0xLhr0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Prepare features maps to use later\n",
        "mask_features_list = [\n",
        "  features[k] for k in model.roi_heads.mask_point_in_features\n",
        "]\n",
        "features_scales = [\n",
        "  model.roi_heads._feature_scales[k] \n",
        "  for k in model.roi_heads.mask_point_in_features\n",
        "]"
      ],
      "outputs": [],
      "metadata": {
        "id": "XHK4aDIxG_oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Point sampling during training\n",
        "\n",
        "During training we select points where coarse prediction is uncertain to train PointRend head. See section 3.1 in the PointRend [paper](https://arxiv.org/abs/1912.08193) for more details.\n",
        "\n",
        "To visualize different sampling strategy change `oversample_ratio` and `importance_sample_ratio` parameters below."
      ],
      "metadata": {
        "id": "zdSy4Zvas9lt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from detectron2.projects.point_rend.roi_heads import calculate_uncertainty\n",
        "from detectron2.projects.point_rend.point_features import get_uncertain_point_coords_with_randomness\n",
        "\n",
        "# Change number of points to select\n",
        "num_points = 14 * 14\n",
        "# Change randomness parameters \n",
        "oversample_ratio = 3  # `k` in the paper\n",
        "importance_sample_ratio = 0.75  # `\\beta` in the paper\n",
        "\n",
        "with torch.no_grad():\n",
        "  # We take predicted classes, whereas during real training ground truth classes are used.\n",
        "  pred_classes = torch.cat([x.pred_classes for x in detected_instances])\n",
        "\n",
        "  # Select points given a corse prediction mask\n",
        "  point_coords = get_uncertain_point_coords_with_randomness(\n",
        "    mask_coarse_logits,\n",
        "    lambda logits: calculate_uncertainty(logits, pred_classes),\n",
        "    num_points=num_points,\n",
        "    oversample_ratio=oversample_ratio,\n",
        "    importance_sample_ratio=importance_sample_ratio\n",
        "  )\n",
        "\n",
        "  H, W = mask_coarse_logits.shape[-2:]\n",
        "  plot_mask(\n",
        "    mask_coarse_logits[instance_idx, category_idx].to(\"cpu\"),\n",
        "    title=\"Sampled points over the coarse prediction\",\n",
        "    point_coords=(\n",
        "      W * point_coords[instance_idx, :, 0].to(\"cpu\") - 0.5,\n",
        "      H * point_coords[instance_idx, :, 1].to(\"cpu\") - 0.5\n",
        "    ),\n",
        "    point_marker_size=50\n",
        "  )"
      ],
      "outputs": [],
      "metadata": {
        "id": "CIny-6sotDCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Point sampling during inference\n",
        "\n",
        "Starting from a 7x7 coarse prediction we bilinearly upsample it `num_subdivision_steps` times. At each step we find `num_subdivision_points` most uncertain points and make predictions for them using the PointRend head. See section 3.1 in the [paper](https://arxiv.org/abs/1912.08193) to know more details.\n",
        "\n",
        "Change `num_subdivision_steps` and `num_subdivision_points` parameters to change inference behavior."
      ],
      "metadata": {
        "id": "IP6fQaafs2He"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from detectron2.layers import interpolate\n",
        "from detectron2.projects.point_rend.roi_heads import calculate_uncertainty\n",
        "from detectron2.projects.point_rend.point_features import (\n",
        "    get_uncertain_point_coords_on_grid,\n",
        "    point_sample,\n",
        "    point_sample_fine_grained_features,\n",
        ")\n",
        "\n",
        "num_subdivision_steps = 5\n",
        "num_subdivision_points = 28 * 28\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  plot_mask(\n",
        "      mask_coarse_logits[0, category_idx].to(\"cpu\").numpy(), \n",
        "      title=\"Coarse prediction\"\n",
        "  )\n",
        "\n",
        "  mask_logits = mask_coarse_logits\n",
        "  for subdivions_step in range(num_subdivision_steps):\n",
        "    # Upsample mask prediction\n",
        "    mask_logits = interpolate(\n",
        "        mask_logits, scale_factor=2, mode=\"bilinear\", align_corners=False\n",
        "    )\n",
        "    # If `num_subdivision_points` is larger or equalt to the\n",
        "    # resolution of the next step, then we can skip this step\n",
        "    H, W = mask_logits.shape[-2:]\n",
        "    if (\n",
        "      num_subdivision_points >= 4 * H * W\n",
        "      and subdivions_step < num_subdivision_steps - 1\n",
        "    ):\n",
        "      continue\n",
        "    # Calculate uncertainty for all points on the upsampled regular grid\n",
        "    uncertainty_map = calculate_uncertainty(mask_logits, pred_classes)\n",
        "    # Select most `num_subdivision_points` uncertain points\n",
        "    point_indices, point_coords = get_uncertain_point_coords_on_grid(\n",
        "        uncertainty_map, \n",
        "        num_subdivision_points\n",
        "    )\n",
        "\n",
        "    # Extract fine-grained and coarse features for the points\n",
        "    fine_grained_features, _ = point_sample_fine_grained_features(\n",
        "      mask_features_list, features_scales, pred_boxes, point_coords\n",
        "    )\n",
        "    coarse_features = point_sample(mask_coarse_logits, point_coords, align_corners=False)\n",
        "\n",
        "    # Run PointRend head for these points\n",
        "    point_logits = model.roi_heads.mask_point_head(fine_grained_features, coarse_features)\n",
        "\n",
        "    # put mask point predictions to the right places on the upsampled grid.\n",
        "    R, C, H, W = mask_logits.shape\n",
        "    x = (point_indices[instance_idx] % W).to(\"cpu\")\n",
        "    y = (point_indices[instance_idx] // W).to(\"cpu\")\n",
        "    point_indices = point_indices.unsqueeze(1).expand(-1, C, -1)\n",
        "    mask_logits = (\n",
        "      mask_logits.reshape(R, C, H * W)\n",
        "      .scatter_(2, point_indices, point_logits)\n",
        "      .view(R, C, H, W)\n",
        "    )\n",
        "    plot_mask(\n",
        "      mask_logits[instance_idx, category_idx].to(\"cpu\"), \n",
        "      title=\"Subdivision step: {}\".format(subdivions_step + 1),\n",
        "      point_coords=(x, y)\n",
        "    )"
      ],
      "outputs": [],
      "metadata": {
        "id": "AhyYdj3psMni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize mask prediction obtained in the previous block."
      ],
      "metadata": {
        "id": "tWCMM2tV6z6b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from detectron2.modeling import GeneralizedRCNN\n",
        "from detectron2.modeling.roi_heads.mask_head import mask_rcnn_inference\n",
        "\n",
        "results = detected_instances\n",
        "mask_rcnn_inference(mask_logits, results)\n",
        "results = GeneralizedRCNN._postprocess(results, batched_inputs, images.image_sizes)[0]\n",
        "\n",
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "v = Visualizer(im_transformed[:, :, ::-1], coco_metadata)\n",
        "v = v.draw_instance_predictions(results[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(v.get_image()[:, :, ::-1])"
      ],
      "outputs": [],
      "metadata": {
        "id": "JWGAw75W6NSn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "rFqeMeF8RsQm"
      }
    }
  ]
}