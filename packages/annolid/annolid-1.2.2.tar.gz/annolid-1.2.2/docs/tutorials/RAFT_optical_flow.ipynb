{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RAFT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "69rHM_M3MamZ"
      },
      "source": [
        "!git clone https://github.com/princeton-vl/RAFT.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo6P-TUHOIfF"
      },
      "source": [
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PZzlKaJMhwB"
      },
      "source": [
        "%cd RAFT/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6V0BHnAMnjq"
      },
      "source": [
        "!./download_models.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRGLF3o2MptN"
      },
      "source": [
        "!python demo.py --model=models/raft-things.pth --path=demo-frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC91fo09NLzN"
      },
      "source": [
        "%matplotlib notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY0uDlr3Oe6w"
      },
      "source": [
        "import sys\n",
        "sys.path.append('core')\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from raft import RAFT\n",
        "from utils import flow_viz\n",
        "from utils.utils import InputPadder\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import  cv2_imshow\n",
        "\n",
        "DEVICE = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfjYO8-iTm8"
      },
      "source": [
        "!unzip /content/video.zip -d /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3qgcdSVN6qm"
      },
      "source": [
        "def viz(img, flo):\n",
        "    img = img[0].permute(1,2,0).cpu().numpy()\n",
        "    flo = flo[0].permute(1,2,0).cpu().numpy()\n",
        "    \n",
        "    # map flow to rgb image\n",
        "    flo = flow_viz.flow_to_image(flo)\n",
        "    img_flo = np.concatenate([img, flo], axis=0)\n",
        "    #plt.imshow(img_flo / 255.0)\n",
        "    #plt.show()\n",
        "    cv2_imshow(img_flo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9QrytkSOcdL"
      },
      "source": [
        "def load_image(imfile):\n",
        "    img = np.array(Image.open(imfile)).astype(np.uint8)\n",
        "    img = torch.from_numpy(img).permute(2, 0, 1).float()\n",
        "    return img[None].to(DEVICE)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k_aN2I2Ohrf"
      },
      "source": [
        "def demo(args):\n",
        "    model = torch.nn.DataParallel(RAFT(args))\n",
        "    model.load_state_dict(torch.load(args.model))\n",
        "\n",
        "    model = model.module\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        images = glob.glob(os.path.join(args.path, '*.png')) + \\\n",
        "                 glob.glob(os.path.join(args.path, '*.jpg'))\n",
        "        \n",
        "        images = sorted(images)\n",
        "        for imfile1, imfile2 in zip(images[:-1], images[1:]):\n",
        "            image1 = load_image(imfile1)\n",
        "            print(image1.shape)\n",
        "            image2 = load_image(imfile2)\n",
        "            padder = InputPadder(image1.shape)\n",
        "            image1, image2 = padder.pad(image1, image2)\n",
        "            print(image1.shape,image2.shape)\n",
        "\n",
        "            flow_low, flow_up = model(image1, image2, iters=20, test_mode=True)\n",
        "            viz(image1, flow_up)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UiZ2qBkXLnp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn7mWWTbVCww"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model', default=\"models/raft-things.pth\",help=\"restore checkpoint\")\n",
        "parser.add_argument('--path', default=\"/content/video\", help=\"dataset for evaluation\")\n",
        "parser.add_argument('--small', action='store_true', help='use small model')\n",
        "parser.add_argument('--mixed_precision', action='store_true', help='use mixed precision')\n",
        "parser.add_argument('--alternate_corr', action='store_true', help='use efficent correlation implementation')\n",
        "args = parser.parse_args(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfmC8AjkSh8e"
      },
      "source": [
        "demo(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrbqoYxhSzf3"
      },
      "source": [
        "model = torch.nn.DataParallel(RAFT(args))\n",
        "model.load_state_dict(torch.load(args.model))\n",
        "\n",
        "model = model.module\n",
        "model.to(DEVICE)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMf9BuVQXRbd"
      },
      "source": [
        "VIDEO_PATH = '/content/video.mp4'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSHj9lnYXw4S"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
        "from torchvision import transforms, utils\n",
        "class VideoFrameDataset(IterableDataset):\n",
        "    \"\"\"Video Frame dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, video_file, root_dir=None, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            video_file (string): Path to the video file.\n",
        "            root_dir (string): Directory with all the videos.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.video_file = video_file\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.cap = cv2.VideoCapture(self.video_file)\n",
        "        \n",
        "\n",
        "    def __iter__(self):\n",
        "        \n",
        "        ret, old_frame = self.cap.read()\n",
        "        num_frames = (int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "        old_frame = cv2.cvtColor(old_frame, cv2.COLOR_BGR2RGB)\n",
        "        for num in range(num_frames - 1):\n",
        "            ret,frame = self.cap.read()\n",
        "            #frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            #old_gray = frame_gray.copy()\n",
        "            \n",
        "\n",
        "            if self.transform:\n",
        "                x = self.transform(old_frame)\n",
        "                y = self.transform(frame)\n",
        "            else:\n",
        "                x = old_frame\n",
        "                y = frame\n",
        "            old_frame = frame.copy()\n",
        "\n",
        "            yield x, y\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        cv2.destroyAllWindows()\n",
        "        cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMNG0qNaYijK"
      },
      "source": [
        "video_dataset = VideoFrameDataset(VIDEO_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJrFc-LUYqJQ"
      },
      "source": [
        "loader = DataLoader(video_dataset,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ToYbkWiYtQ4"
      },
      "source": [
        "FRAMES = 1000\n",
        "counter = 0\n",
        "with torch.no_grad():\n",
        "    for image1,image2 in loader:\n",
        "        image1 = image1.permute(0,3,1,2).float()\n",
        "        image2 = image2.permute(0,3,1,2).float()\n",
        "        \n",
        "        #image1[None].to(DEVICE)\n",
        "        #image2[None].to(DEVICE)\n",
        "        image1 = image1.cuda()\n",
        "        image2 = image2.cuda()\n",
        "        padder = InputPadder(image1.shape)\n",
        "        image1, image2 = padder.pad(image1, image2)\n",
        "        flow_low, flow_up = model(image1, image2, iters=20, test_mode=True)\n",
        "        viz(image1, flow_up)\n",
        "        if counter == FRAMES:\n",
        "            break\n",
        "        counter += 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}