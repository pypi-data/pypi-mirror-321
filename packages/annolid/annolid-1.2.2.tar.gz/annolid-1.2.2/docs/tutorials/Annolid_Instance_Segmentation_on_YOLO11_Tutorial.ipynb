{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instance Segmenation based on YOLO11"
      ],
      "metadata": {
        "id": "kl8quffcowKu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
        "\n",
        "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/healthonrails/annolid/blob/main/docs/tutorials/Annolid_Instance_Segmentation_on_YOLO11_Tutorial.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG"
      },
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "fJwI06VXAd5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload custom dataset, labeled in annolid"
      ],
      "metadata": {
        "id": "GcCJTU46pMvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dataset = files.upload()"
      ],
      "metadata": {
        "id": "um5pNZOIoaQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip YOLO_dataset.zip"
      ],
      "metadata": {
        "id": "xaxatJbaFs7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the dataset directory\n",
        "image_dir = '/content/YOLO_dataset/images/val'  # Directory containing images\n",
        "label_dir = '/content/YOLO_dataset/labels/val'  # Directory containing YOLO segmentation labels (e.g., .txt files)\n"
      ],
      "metadata": {
        "id": "Iy5O6ksWC02L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional (check dataset)"
      ],
      "metadata": {
        "id": "e14h2gAfpVi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_segmentation_with_labels(image_path, label_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    if not os.path.exists(label_path):\n",
        "        print(f\"Label file {label_path} not found.\")\n",
        "        return\n",
        "\n",
        "    with open(label_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        data = line.strip().split()\n",
        "        class_id = int(data[0])\n",
        "        polygon = np.array(data[5:], dtype=np.float32).reshape(-1, 2)\n",
        "\n",
        "        polygon[:, 0] *= img.shape[1]\n",
        "        polygon[:, 1] *= img.shape[0]\n",
        "\n",
        "        # Convert to integers for drawing\n",
        "        polygon = polygon.astype(np.int32)\n",
        "\n",
        "        # Draw polygon (same as before)\n",
        "        cv2.polylines(img, [polygon], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "        overlay = img.copy()\n",
        "        cv2.fillPoly(overlay, [polygon], color=(0, 255, 0))\n",
        "        alpha = 0.4\n",
        "        img = cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n",
        "\n",
        "        # Display Class ID (same as before)\n",
        "        center_x, center_y = polygon.mean(axis=0).astype(int)\n",
        "        cv2.putText(img, f'Class {class_id}', (center_x, center_y),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Segmentation with Ground Truth: {os.path.basename(image_path)}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "# Loop through images and visualize them with labels\n",
        "for image_name in os.listdir(image_dir):\n",
        "    if image_name.endswith('.jpg') or image_name.endswith('.png'):\n",
        "        image_path = os.path.join(image_dir, image_name)\n",
        "        label_path = os.path.join(label_dir, os.path.splitext(image_name)[0] + '.txt')\n",
        "        visualize_segmentation_with_labels(image_path, label_path)"
      ],
      "metadata": {
        "id": "BQ8XV9xPCvQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# Predict without fine-tuning\n",
        "\n",
        "YOLO11 may be used directly in the Command Line Interface (CLI) with a `yolo` command for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See a full list of available `yolo` [arguments](https://docs.ultralytics.com/usage/cfg/) and other details in the [YOLO11 Predict Docs](https://docs.ultralytics.com/modes/train/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR9ZbuQCH7FX"
      },
      "source": [
        "# Run inference on an image with YOLO11n\n",
        "!yolo predict model=yolov8n-seg.pt source='/content/YOLO_dataset/images/val/Stimulus Mouse Sniffing FP Mouse_3.16-7.64_000000069.png'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkAzDWJ7cWTr"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select YOLO11 🚀 logger {run: 'auto'}\n",
        "logger = 'TensorBoard' #@param ['Comet', 'TensorBoard']\n",
        "\n",
        "if logger == 'Comet':\n",
        "  %pip install -q comet_ml\n",
        "  import comet_ml; comet_ml.init()\n",
        "elif logger == 'TensorBoard':\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir ."
      ],
      "metadata": {
        "id": "ktegpM42AooT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O"
      },
      "source": [
        "# Train YOLO11n-seg on custom dataset for 30 epochs\n",
        "!yolo train model=yolo11n-seg.pt data=/content/YOLO_dataset/data.yaml epochs=200 imgsz=640"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference and Save results to annolid json files"
      ],
      "metadata": {
        "id": "PZm1AsHDpgZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference on an image with YOLO11n\n",
        "!yolo predict model=runs/segment/train/weights/best.pt source='/content/YOLO_dataset/images/train/Stimulus Mouse Sniffing FP Mouse_3.16-7.64_000000000.png'"
      ],
      "metadata": {
        "id": "SmH0bbc9Rsz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "model = YOLO(\"runs/segment/train/weights/best.pt\")  # Update this to your model\n",
        "\n",
        "# Provide the path to your video file\n",
        "video_path = \"/content/Stimulus_Mouse_Sniffing_FP_Mouse_3.16-7.64.mp4\"  # Update this to your video file's path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Create an output directory named after the video file without the extension\n",
        "video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "output_dir = video_name\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Store the track history\n",
        "track_history = defaultdict(lambda: [])\n",
        "frame_id = 0  # Track frame number\n",
        "display_interval = 10  # Display visualization every n frames\n",
        "\n",
        "# Loop through the video frames\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if success:\n",
        "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
        "        results = model.track(frame, persist=True)\n",
        "\n",
        "        # Initialize LabelMe JSON structure\n",
        "        labelme_data = {\n",
        "            \"version\": \"5.0.1\",\n",
        "            \"flags\": {},\n",
        "            \"shapes\": [],\n",
        "            \"imagePath\": None,  # No image saved\n",
        "            \"imageHeight\": frame.shape[0],\n",
        "            \"imageWidth\": frame.shape[1],\n",
        "            \"imageData\": \"\",\n",
        "        }\n",
        "\n",
        "        # Get the boxes, track IDs, and segmentation masks\n",
        "        boxes = results[0].boxes.xywh.cpu()\n",
        "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        masks = results[0].masks  # Get segmentation masks\n",
        "\n",
        "        # Loop through detected objects\n",
        "        for box, track_id, mask in zip(boxes, track_ids, masks):\n",
        "            x, y, w, h = box.tolist()\n",
        "            track = track_history[track_id]\n",
        "            track.append((float(x), float(y)))  # x, y center point\n",
        "\n",
        "            if len(track) > 30:  # retain 30 tracks for 30 frames\n",
        "                track.pop(0)\n",
        "\n",
        "            # Bounding box coordinates\n",
        "            x1, y1 = x - w/2, y - h/2\n",
        "            x2, y2 = x + w/2, y + h/2\n",
        "\n",
        "            # Add bounding box annotation\n",
        "            bbox_shape = {\n",
        "                \"label\": f\"object_{track_id}\",\n",
        "                \"line_color\": None,\n",
        "                \"fill_color\": None,\n",
        "                \"shape_type\": \"rectangle\",\n",
        "                \"points\": [\n",
        "                    [x1, y1],\n",
        "                    [x2, y2]\n",
        "                ],\n",
        "                \"group_id\": track_id,\n",
        "                \"flags\": {}\n",
        "            }\n",
        "            labelme_data[\"shapes\"].append(bbox_shape)\n",
        "\n",
        "            # Add polygon for tracking history\n",
        "            if len(track) > 1:\n",
        "                points = np.array(track).tolist()\n",
        "                shape_polygon = {\n",
        "                    \"label\": f\"track_{track_id}\",\n",
        "                    \"line_color\": None,\n",
        "                    \"fill_color\": None,\n",
        "                    \"shape_type\": \"polygon\",\n",
        "                    \"points\": points,\n",
        "                    \"group_id\": track_id,\n",
        "                    \"flags\": {}\n",
        "                }\n",
        "                labelme_data[\"shapes\"].append(shape_polygon)\n",
        "\n",
        "            if mask is not None:\n",
        "              try:\n",
        "                  # Access polygon segments directly in pixel coordinates\n",
        "                  polygons = mask.xy  # Get list of polygons (one for each instance)\n",
        "\n",
        "                  # Iterate over each polygon and add it to the LabelMe JSON\n",
        "                  for polygon in polygons:\n",
        "                      contour_points = polygon.tolist()  # Convert to list for JSON compatibility\n",
        "\n",
        "                      # Only add polygons with enough valid points\n",
        "                      if len(contour_points) > 2:\n",
        "                          segmentation_shape = {\n",
        "                              \"label\": f\"segmentation_{track_id}\",\n",
        "                              \"line_color\": None,\n",
        "                              \"fill_color\": None,\n",
        "                              \"shape_type\": \"polygon\",\n",
        "                              \"points\": contour_points,\n",
        "                              \"group_id\": track_id,\n",
        "                              \"flags\": {\"segmentation\": True}\n",
        "                          }\n",
        "                          labelme_data[\"shapes\"].append(segmentation_shape)\n",
        "\n",
        "              except Exception as e:\n",
        "                  print(f\"Error processing mask for track {track_id}: {e}\")\n",
        "\n",
        "        # Save the JSON annotation file with zero-padded numbering (e.g., 000000001.json)\n",
        "        json_filename = os.path.join(output_dir, f\"{frame_id:09d}.json\")\n",
        "        with open(json_filename, \"w\") as json_file:\n",
        "            json.dump(labelme_data, json_file, indent=4)\n",
        "\n",
        "        # Visualization: Display annotated frame every 'display_interval' frames\n",
        "        if frame_id % display_interval == 0:\n",
        "            annotated_frame = results[0].plot()  # Get annotated frame\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.axis('off')\n",
        "            plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
        "            clear_output(wait=True)  # Clear previous output for smoother display\n",
        "            plt.show()\n",
        "\n",
        "        frame_id += 1  # Increment frame number\n",
        "    else:\n",
        "        # Break the loop if the end of the video is reached\n",
        "        break\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n",
        "print(f\"Annotations saved in '{output_dir}' directory.\")"
      ],
      "metadata": {
        "id": "EnE1-PXFBvM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zip and download json results can be loaded into annolid"
      ],
      "metadata": {
        "id": "KoVDypQ0px11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Provide the path to the folder you want to zip\n",
        "folder_to_zip = output_dir  # Update with your folder path\n",
        "\n",
        "# Output zip file path\n",
        "output_zip_file = folder_to_zip + \".zip\"\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(folder_to_zip, 'zip', folder_to_zip)\n",
        "\n",
        "print(f\"Folder '{folder_to_zip}' has been zipped to '{output_zip_file}'.\")\n",
        "files.download(output_zip_file)"
      ],
      "metadata": {
        "id": "T-OfoVvzWA4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zip and download runs folder which continas the saved best model"
      ],
      "metadata": {
        "id": "iUMFYuLk_waJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'folder_name' with the name of your folder\n",
        "folder_to_download = 'runs'\n",
        "output_filename = 'runs.zip'\n",
        "\n",
        "# Compress the folder\n",
        "shutil.make_archive(output_filename.replace('.zip', ''), 'zip', folder_to_download)\n",
        "\n",
        "# Download the zipped folder\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "YuAW2iF8_jbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Export\n",
        "\n",
        "Export a YOLO11 model to any supported format below with the `format` argument, i.e. `format=onnx`. See [YOLO11 Export Docs](https://docs.ultralytics.com/modes/export/) for more information.\n",
        "\n",
        "- 💡 ProTip: Export to [ONNX](https://docs.ultralytics.com/integrations/onnx/) or [OpenVINO](https://docs.ultralytics.com/integrations/openvino/) for up to 3x CPU speedup.  \n",
        "- 💡 ProTip: Export to [TensorRT](https://docs.ultralytics.com/integrations/tensorrt/) for up to 5x GPU speedup.\n",
        "\n",
        "| Format                                                                   | `format` Argument | Model                     | Metadata | Arguments                                                            |\n",
        "|--------------------------------------------------------------------------|-------------------|---------------------------|----------|----------------------------------------------------------------------|\n",
        "| [PyTorch](https://pytorch.org/)                                          | -                 | `yolo11n.pt`              | ✅        | -                                                                    |\n",
        "| [TorchScript](https://docs.ultralytics.com/integrations/torchscript)     | `torchscript`     | `yolo11n.torchscript`     | ✅        | `imgsz`, `optimize`, `batch`                                         |\n",
        "| [ONNX](https://docs.ultralytics.com/integrations/onnx)                   | `onnx`            | `yolo11n.onnx`            | ✅        | `imgsz`, `half`, `dynamic`, `simplify`, `opset`, `batch`             |\n",
        "| [OpenVINO](https://docs.ultralytics.com/integrations/openvino)           | `openvino`        | `yolo11n_openvino_model/` | ✅        | `imgsz`, `half`, `int8`, `batch`                                     |\n",
        "| [TensorRT](https://docs.ultralytics.com/integrations/tensorrt)           | `engine`          | `yolo11n.engine`          | ✅        | `imgsz`, `half`, `dynamic`, `simplify`, `workspace`, `int8`, `batch` |\n",
        "| [CoreML](https://docs.ultralytics.com/integrations/coreml)               | `coreml`          | `yolo11n.mlpackage`       | ✅        | `imgsz`, `half`, `int8`, `nms`, `batch`                              |\n",
        "| [TF SavedModel](https://docs.ultralytics.com/integrations/tf-savedmodel) | `saved_model`     | `yolo11n_saved_model/`    | ✅        | `imgsz`, `keras`, `int8`, `batch`                                    |\n",
        "| [TF GraphDef](https://docs.ultralytics.com/integrations/tf-graphdef)     | `pb`              | `yolo11n.pb`              | ❌        | `imgsz`, `batch`                                                     |\n",
        "| [TF Lite](https://docs.ultralytics.com/integrations/tflite)              | `tflite`          | `yolo11n.tflite`          | ✅        | `imgsz`, `half`, `int8`, `batch`                                     |\n",
        "| [TF Edge TPU](https://docs.ultralytics.com/integrations/edge-tpu)        | `edgetpu`         | `yolo11n_edgetpu.tflite`  | ✅        | `imgsz`                                                              |\n",
        "| [TF.js](https://docs.ultralytics.com/integrations/tfjs)                  | `tfjs`            | `yolo11n_web_model/`      | ✅        | `imgsz`, `half`, `int8`, `batch`                                     |\n",
        "| [PaddlePaddle](https://docs.ultralytics.com/integrations/paddlepaddle)   | `paddle`          | `yolo11n_paddle_model/`   | ✅        | `imgsz`, `batch`                                                     |\n",
        "| [NCNN](https://docs.ultralytics.com/integrations/ncnn)                   | `ncnn`            | `yolo11n_ncnn_model/`     | ✅        | `imgsz`, `half`, `batch`                                             |"
      ],
      "metadata": {
        "id": "nPZZeNrLCQG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo export model=runs/segment/train/weights/best.pt format=onnx"
      ],
      "metadata": {
        "id": "CYIjW4igCjqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Python Usage\n",
        "\n",
        "YOLO11 was reimagined using Python-first principles for the most seamless Python YOLO experience yet. YOLO11 models can be loaded from a trained checkpoint or created from scratch. Then methods are used to train, val, predict, and export the model. See detailed Python usage examples in the [YOLO11 Python Docs](https://docs.ultralytics.com/usage/python/)."
      ],
      "metadata": {
        "id": "kUMOQ0OeDBJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO('runs/segment/train/weights/best.pt')  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Use the model\n",
        "results = model('/content/YOLO_dataset/images/test/Stimulus Mouse Sniffing FP Mouse_3.16-7.64_000000049.png')  # predict on an image\n",
        "results = model.export(format='onnx')  # export the model to ONNX formats"
      ],
      "metadata": {
        "id": "bpF9-vS_DAaf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}