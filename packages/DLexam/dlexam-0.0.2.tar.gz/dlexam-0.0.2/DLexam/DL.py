print('Use DL.help()')

def help():
    print('''
    –°–ø–∏—Å–æ–∫ –Ω–æ—É—Ç–±—É–∫–æ–≤ –ø–æ —Ç–µ–º–∞–º:
          
1)01_tensor_v1_0_blank (tens)
2)02_NN_blocks_backprop_v1 (blocks)
3)blank_03_autograd_optim_nn_datasets_3 (autD)
4) 3 –Ω–æ—É—Ç–±—É–∫–∞
          1. 04_1_cnn_image_classification (cnnClas)
          2. 04_2_cnn_pretrained (cnnPre)
          3. 04_3_cnn_1d (cnn1D)
5) 4 –Ω–æ—É—Ç–±—É–∫–∞
          1. 05_1_RL (RL)
          2. 05_2_QL (QL)
          3. 05_3_dqn (QDN)
          4. 05_4_policy_gradients (polGr)
6) 3 –Ω–æ—É—Ç–±—É–∫–∞
          1. 06_1_object_detection (obDet)
          2. 06_2_image_segmentation (imSeg)
          3. 06_4_lightning_deploy (ligDep)

–î–ª—è –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ –Ω–æ—É—Ç–±—É–∫–∞–º –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: DL."–Ω–∞–∑–≤–∞–Ω–∏–µ_–Ω–æ—Ç—É–±—É–∫–∞_info() –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –ø–æ –Ω–æ—É—Ç–±—É–∫—É
''')
    

def tens_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.tens_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")

1.1.1 –°–æ–∑–¥–∞–π—Ç–µ –¥–≤—É–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ (8, 8). –ò—Å–ø–æ–ª—å–∑—É—è –∫–∞–∫ –º–æ–∂–Ω–æ –º–µ–Ω—å—à–µ –æ–ø–µ—Ä–∞—Ü–∏–π, –¥–æ–±–µ–π—Ç–µ—Å—å —Ä–∞—Å—Å—Ç–∞–Ω–æ–≤–∫–∏ –∫–æ–¥–æ–≤ "—à–∞—Ö–º–∞—Ç–Ω—ã—Ö —Ñ–∏–≥—É—Ä".
          
1.1.2 –°—Ä–µ–¥—Å—Ç–≤–∞–º–∏ torch —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è —á–µ—Ç–Ω—ã—Ö —á–∏—Å–µ–ª –æ—Ç 2 –¥–æ 20 –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–µ –∫ –Ω–∏–º –±–û–ª—å—à–∏–µ –Ω–µ—á–µ—Ç–Ω—ã–µ —á–∏—Å–ª–∞.
          
1.1.3 –°–æ–∑–¥–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ 11x7 –≤–∏–¥–∞: [[1, 2, 3, ..., 7], [11, 12, 13, ..., 17], [21, 22, 23, ..., 27], ..., [101, 102, 103, ..., 107]]
          
1.1.4 –ù–∞–ø–∏—Å–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –¥–ª—è —Ü–µ–ª—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π n –∏ m –±—É–¥–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ nxm, –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç—É—Ä–æ–π —Ä–∞–∑–º–µ—Ä–∞ 2x2, —Å–æ—Å—Ç–æ—è—â–µ–π –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π:
          
1.1.5 –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–≤—É–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä t —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (4, 7), —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö —á–∏—Å–µ–ª, —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≤ –¥–∏–ø–∞–∑–æ–Ω–µ –æ—Ç 0 –¥–æ 20. 
–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –º–∞—Å—Å–∏–≤–∞ —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤–∏–¥–∞  ùëéùë•+ùëè  —Ç–∞–∫, —á—Ç–æ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –º–∞—Å–∏–≤–∞ –±—É–¥–µ—Ç —Ä–∞–≤–µ–Ω 1.0, –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π 0.0
          
1.1.6 –ó–∞–¥–∞—Ç—å –¥–≤–∞ –¥–≤—É–º–µ—Ä–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–∞ ar1 –∏ ar2 —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (4, 7), —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ—Ç 0 –¥–æ 10. 
–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –¥–≤—É–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (4, 7), –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –∫–æ—Ç–æ—Ä–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–∞–∫—Å–∏–º—É–º –∏–∑ –¥–≤—É—Ö –∑–Ω–∞—á–µ–Ω–∏–π, –Ω–∞—Ö–æ–¥—è—â–∏—Ö—Å—è –Ω–∞ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏ –≤ –º–∞—Å—Å–∏–≤–∞—Ö ar1, ar2.
          
1.1.7 –°–æ–∑–¥–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä –∏–∑ 20 —Å–ª—É—á–∞–π–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –æ—Ç 0 –¥–æ 100. –ü–æ–ª—É—á–∏—Ç—å –≤—Ç–æ—Ä–æ–µ —Å–≤–µ—Ä—Ö—É –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä–µ. –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–Ω–¥–µ–∫—Å —ç—Ç–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.
          
1.2.1 –°–æ–∑–¥–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä 11x7 –≤–∏–¥–∞: [[1, 2, 3, ..., 7], [11, 12, 13, ..., 17], [21, 22, 23, ..., 27], ..., [101, 102, 103, ..., 107]]. 
–ü—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Ç–µ—Ö–Ω–∏–∫—É —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è.
          
1.2.2 –í—ã—á–µ—Å—Ç—å –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä b_1d –∏–∑ –¥–≤—É—Ö–º–µ—Ä–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ a_2d, —Ç–∞–∫,
—á—Ç–æ–±—ã –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ –≤—ã—á–∏—Ç–∞–ª—Å—è –∏–∑ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫ –¥–≤—É–º–µ—Ä–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞.
          
1.3.1 –ü–æ–ª—É—á–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —ç–ª–µ–º–µ–Ω—Ç—ã —Ç–µ–Ω–∑–æ—Ä–æ–≤ a –∏ b —Å–æ–≤–ø–∞–¥–∞—é—Ç.
          
1.3.2 –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ä—è–¥–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –¥–≤—É–º–µ—Ä–Ω–æ–º —Ç–µ–Ω–∑–æ—Ä–µ torch.arange(9).view(3,3).
          
1.3.3 –ò–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ a –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç—ã, –Ω–∞—Ö–æ–¥—è—â–∏–µ—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 5 –¥–æ 10.
          
1.3.4 –ü–æ–º–µ–Ω—è—Ç—å –º–µ—Å—Ç–∞–º–∏ —Å—Ç–æ–ª–±–µ—Ü 1 –∏ 2 —Ç–µ–Ω–∑–æ—Ä–∞ np.arange(9).reshape(3,3)
          
1.3.5 –°–æ–∑–¥–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä 8 –Ω–∞ 10 –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –∏–∑ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –æ—Ç 0 –¥–æ 10 –∏ –Ω–∞–π—Ç–∏ –≤ –Ω–µ–π —Å—Ç—Ä–æ–∫—É (–µ–µ –∏–Ω–¥–µ–∫—Å –∏ –≤—ã–≤–µ—Å—Ç–∏ —Å–∞–º—É —Å—Ç—Ä–æ–∫—É), –≤ –∫–æ—Ç–æ—Ä–æ–π —Å—É–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π –º–∏–Ω–∏–º–∞–ª—å–Ω–∞.
          
1.3.6 C–æ–∑–¥–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä –∏–∑ 20 —Å–ª—É—á–∞–π–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –æ—Ç 0 –¥–æ 100. 
–û–±—Ä–µ–∑–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è —Ç–µ–Ω–∑–æ—Ä–∞ (–∑–∞–º–µ–Ω–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è, –≤—ã—Ö–æ–¥—è—â–∏–µ –∑–∞ –¥–∏–∞–ø–∞–∑–æ–Ω, –Ω–∞ –∫—Ä–∞–π–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è) —Å–Ω–∏–∑—É –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é 30, —Å–≤–µ—Ä—Ö—É –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é 70.
          
1.3.7 –°–æ–∑–¥–∞—Ç—å –¥–≤–∞ —Ç–µ–Ω–∑–æ—Ä–∞ —Ä–∞–∑–º–µ—Ä–∞ 30 –Ω–∞ 3 –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –∏–∑ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –æ—Ç 0 –¥–æ 10 –∏ –Ω–∞–π—Ç–∏ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–≤–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞, 
–∫–æ—Ç–æ—Ä—ã–µ –±–æ–ª—å—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤—É—é—â–∏—Ö (–ø–æ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—é) –∑–Ω–∞—á–µ–Ω–∏–π –≤—Ç–æ—Ä–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞. –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å—É–º–º—É —ç—Ç–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π.
          
1.3.8 –ü—Ä–∏ –ø–æ–º–æ—â–∏ –ø—Ä–∏—Ö–æ—Ç–ª–∏–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–≤—É—Ö–º–µ—Ä–Ω–æ–≥–æ –º–∞—Å—Å–∏–≤–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (20, 20), 
—Å–æ—Å—Ç–æ—è—â–µ–≥–æ –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ—Ç 0 –¥–æ 10 –ø–æ–ª—É—á–∏—Ç—å –º–∞—Å—Å–∏–≤ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞—Ö–æ–¥—è—â–∏—Ö—Å—è –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–∏, –ø—Ä–æ—Ö–æ–¥—è—â–µ–π –Ω–∞–¥ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∏–∞–≥–æ–Ω–∞–ª—å—é.
          
1.3.9 –ó–∞–¥–∞—Ç—å –¥–≤–∞ –¥–≤—É—Ö–º–µ—Ä–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–∞ ar1 –∏ ar2 —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (5, 10), —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ—Ç 0 –¥–æ 10. 
–£–¥–≤–æ–∏—Ç—å –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è ar1, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ ar2, —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –Ω–∞ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏—è—Ö.
          
1.3.10 –ó–∞–¥–∞–Ω—ã —Ç—Ä–∏ –¥–≤—É—Ö–º–µ—Ä–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–∞ ar1, ar2 –∏ ar3 —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (4, 7), —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ—Ç 0 –¥–æ 10. 
–û–±–Ω—É–ª–∏—Ç—å –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã ar1, –∫–æ—Ç–æ—Ä—ã–µ –±–æ–ª—å—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö (–Ω–∞—Ö–æ–¥—è—â–∏—Ö—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–∑–∏—Ü–∏—è—Ö) —ç–ª–µ–º–µ–Ω—Ç–æ–≤ ar2 –∏ –º–µ–Ω—å—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ ar3.
          
1.3.11 –ó–∞–¥–∞–Ω –¥–≤—É–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä `ar1` —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (20, 5), —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ—Ç 0 –¥–æ 20. 
–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –≤ –∫–∞–∫–∏—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö –Ω–µ –º–µ–Ω–µ–µ 5 —Ä–∞–∑ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –ø–æ —Å–≤–æ–µ–π —Å—Ç—Ä–æ–∫–µ.
          
1.3.12 –ó–∞–¥–∞–Ω –¥–≤—É–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä ar1 —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (4, 7), —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ—Ç 0 –¥–æ 1. 
–û–±–Ω—É–ª–∏—Ç—å –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –º–∞—Å—Å–∏–≤–µ, —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–≥–æ –ø—Ä–∞–≤–µ–µ –∏ –Ω–∏–∂–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –º–∞—Å—Å–∏–≤–∞.
          
1.3.13 –ü–æ—Å—Ç—Ä–æ–∏—Ç—å "one-hot encoding" –¥–ª—è –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞, —Å–æ–¥–µ—Ä–∂–∞—â–µ–≥–æ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞ 
(–¥–ª–∏–Ω–∞ –≤–µ–∫—Ç–æ—Ä–∞ –∑–∞—Ä–∞–Ω–µ–µ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞, –Ω–∞–±–æ—Ä –∑–Ω–∞—á–µ–Ω–∏–π –∑–∞—Ä–∞–Ω–µ–µ –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω, –ø—Ä–∏ —ç—Ç–æ–º –≤ –∏—Ç–æ–≥–æ–≤–æ–π –º–∞—Ç—Ä–∏—Ü–µ 
–¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è –≤—Å–µ—Ö –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã—Ö —á–∏—Å–µ–ª –≤–ø–ª–æ—Ç—å –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–µ–≥–æ—Å—è –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –º–∞—Å—Å–∏–≤–µ).
          
1.3.14 –°–æ–∑–¥–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä arr –∏–∑ 20 —Å–ª—É—á–∞–π–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –æ—Ç 0 –¥–æ 100. –ù–∞–π—Ç–∏ —Å–∞–º–æ–µ —á–∞—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä–µ. 
–ù–∞–π—Ç–∏ –∏–Ω–¥–µ–∫—Å—ã –≤ —Ç–µ–Ω–∑–æ—Ä–µ, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–∞–º–æ–º—É —á–∞—Å—Ç–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é. 
–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∫–∞–∫ —Ä–∞–±–æ—Ç–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–∏ –¥–≤—É—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö, –∏–º–µ—é—â–∏—Ö –Ω–∞–∏–±–æ–ª—å—à—É—é –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç—å, –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø—Ä–∏–µ–º–ª–µ–º–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è —ç—Ç–æ–≥–æ —Å–ª—É—á–∞—è.
          
1.4.1 –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ (—Å –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å—é –ø–æ—Ä—è–¥–∫–∞ 1%) —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –Ω–∞ –∫–∞–∫–æ–π —á–∞—Å—Ç–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –æ—Ç 0 –¥–æ 10 –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ x * sin(x) –±–æ–ª—å—à–µ 0.5.
          
1.4.2 –ù–∞–π—Ç–∏ –≤—Å–µ –ø—Ä–æ—Å—Ç—ã–µ —á–∏—Å–ª–∞ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Å—Ç–∞. (–î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –†–µ—à–µ—Ç–æ –≠—Ä–∞—Ç–æ—Å—Ñ–µ–Ω–∞) –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ –±–æ–ª–µ–µ 1 —Ü–∏–∫–ª–∞ (–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ).
          
1.4.3 –ù–∞–π—Ç–∏ –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –¥–≤—É–º—è –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–º–∏ —Ç–µ–Ω–∑–æ—Ä–∞–º–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—è –≥–æ—Ç–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫.
          
1.4.4 –°–æ–∑–¥–∞—Ç—å –¥–≤—É–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä 20 –Ω–∞ 3, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å–ª—É—á–∞–π–Ω—ã–µ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞ –æ—Ç 0 –¥–æ 100.
–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É—è —Ç–µ–Ω–∑–æ—Ä –∫–∞–∫ 20 –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ 3—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –ø–æ –¥–ª–∏–Ω–µ –≤–µ–∫—Ç–æ—Ä–æ–≤.
          
1.4.5 –ù–∞–π—Ç–∏ "–ª–æ–∫–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã" –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–º —Ç–µ–Ω–∑–æ—Ä–µ (—Ç.–µ. –∑–Ω–∞—á–µ–Ω–∏—è, –±–æ–ª—å—à–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ) 
torch.tensor([1, 3, 7, 1, 2, 6, 0, 1]) –∏ –≤—ã–≤–µ—Å—Ç–∏ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã.
          
1.4.6 –ó–∞–¥–∞–Ω –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π –º–∞—Å—Å–∏–≤ numpy (–Ω–∞–ø—Ä–∏–º–µ—Ä –º–∞—Å—Å–∏–≤ –∏–∑ 100 —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–ª–µ –æ—Ç 0 –¥–æ 1). –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –Ω–∞–π—Ç–∏ –≤ –Ω–µ–º —á–∏—Å–ª–æ –Ω–∞–∏–±–æ–ª–µ–µ –±–ª–∏–∑–∫–æ–µ –∫ –∑–∞–¥–∞–Ω–Ω–æ–º—É.
          
1.4.7 –†–µ—à–∏—Ç—å –º–∞—Ç—Ä–∏—á–Ω–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ A*X*B=-C - –Ω–∞–π—Ç–∏ –º–∞—Ç—Ä–∏—Ü—É X. 
–ì–¥–µ A = [[-1, 2, 4], [-3, 1, 2], [-3, 0, 1]], B=[[3, -1], [2, 1]], C=[[7, 21], [11, 8], [8, 4]].
          
1.4.8 –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–∏—Å—Ç–µ–º–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ a1 = (3; ‚àí3; 0; 7), a2 = (2; 2; 4; 7), a3 = (1; 2; 3; 4), a4 = (5; ‚àí4; 1; 3) –ª–∏–Ω–µ–π–Ω–æ –∑–∞–≤–∏—Å–∏–º–æ–π?

1.4.9 –°–≥–µ–Ω–∏—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä –∏–∑ 200 —Å–ª—É—á–∞–π–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª, –Ω–æ—Ä–º–∞–ª—å–Ω–æ —Ä–∞—Å–ø—Ä–¥–µ–ª–µ–Ω–Ω—ã—Ö c–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ–º  ùúé=10  –∏ –º–∞—Ç–æ–∂–∏–¥–∞–Ω–∏–µ–º  ùúá=0 . 
–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç–µ–Ω–∑–æ—Ä –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã —Å 20 –±–∏–Ω–∞–º–∏.
''')
    
def tens_code(st):
    print('import torch')
    if st == '1.1.1':
        print('''
chessboard = torch.zeros((8, 8))
chessboard
''')
    elif st == '1.1.2':
        print('''
tensor_even_numbers = torch.arange(2, 21, 2)
tensor_even_numbers
              
tensor_odd_numbers = tensor_even_numbers + 1
tensor_odd_numbers
              
res = tensor_even_numbers * tensor_odd_numbers
res
    ''')
    elif st == '1.1.3':
        print('''
t = torch.arange(1,111).view(11,10)
t
              
t = t[:, :7]
t
    ''')
    elif st == '1.1.4':
        print('''
def function_texture(n, m):
    t = torch.tensor([[0, 1],
                      [2, 3]])

    tensor = t.repeat((n, m))[:n, :m]
    return tensor
              
res = function_texture(4,5)
res
    ''')
    elif st == '1.1.5':
        print('''
t = torch.rand(4, 7) * 20

t_min = t.min()
t_max = t.max()
              

t_norm = (t - t_min) / (t_max - t_min)

#ax + b - –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–¥–≤–∏–≥–∞ –∑–Ω–∞—á–µ–Ω–∏–π ( –≤ –Ω–æ–≤—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω )
#a = 1/(t_max - t_min) - –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
#b = -t_min/(t_max - t_min) - —Å–¥–≤–∏–≥
#x = t
              
t
              
t_norm
    ''')
    elif st == '1.1.6':
        print('''
array1 = torch.randint(0, 11, (4,7))
array2 = torch.randint(0, 11, (4,7))
array1, array2
              
res = torch.max(array1, array2)
res
    ''')
    elif st == '1.1.7':
        print('''
tens = torch.randint(0, 101, (20, ))
tens
              
sort_tens, indices = torch.sort(tens, descending = True)
sort_tens , indices
              
slv = sort_tens[1]
slv
              
index_of_slv = (tens == slv).nonzero(as_tuple = True)[0] #–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã —Ç–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ç–µ–Ω–∑–æ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Ä–∞–≤–Ω—ã –Ω—É–ª—é
index_of_slv
#as_tuple –∏–Ω–¥–µ–∫—Å—ã –≤ –≤–∏–¥–µ –∫–æ—Ä—Ç–µ–∂–∞
    ''')
    elif st == '1.2.1':
        print('''
tens = torch.arange(1, 8)
tens
              
tens_o = torch.arange(0, 110, 10).unsqueeze(1)
tens_o
              
new_tens = tens + tens_o
new_tens
    ''')
    elif st == '1.2.2':
        print('''
a_2d = torch.tensor([[3,3,3],[4,4,4],[5,5,5]], dtype = torch.float)
b_1d = torch.tensor([1,2,3], dtype = torch.float)
              
b_1d_unsqueezed = b_1d.unsqueeze(0)
res = a_2d - b_1d
res
#unsqueeze –º–µ–Ω—è–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    ''')
    elif st == '1.3.1':
        print('''
a = torch.tensor([1,2,3,2,3,4,3,4,5,6])
b = torch.tensor([7,2,10,2,7,4,9,4,9,8])

tensor_match = (a == b)
tensor_match
              
index_of_match = (a == b).nonzero(as_tuple = True)
index_of_match
    ''')
    elif st == '1.3.2':
        print('''
tens = torch.arange(9).view(3, 3)
tens

inv_tens = torch.flip(tens, dims = [0,1])
inv_tens
    ''')
    elif st == '1.3.3':
        print('''
a = torch.tensor([2, 6, 1, 9, 10, 3, 27])
a
              
mask = a[(a >= 5) & (a <= 10)]
mask
    ''')
    elif st == '1.3.4':
        print('''
tens = torch.arange(9).reshape(3,3)
tens
            
tens[:, [0, 1]] = tens[:, [1, 0]]
tens
    ''')
    elif st == '1.3.5':
        print('''
tens = torch.randint(0, 11, (8, 10))
tens
              
row_s = torch.sum(tens, dim = 1)
row_s
    
min_s_index = torch.argmin(row_s) #–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
min_s_index
              
tens[min_s_index]
    ''')
    elif st == '1.3.6':
        print('''
tens = torch.randint(0, 101, (20, ))
tens
              
new_tens = torch.clamp(tens, min = 30, max= 70)
new_tens
    ''')
    elif st == '1.3.7':
        print('''
a = torch.randint(0, 11, (30, 3))
b = torch.randint(0, 11, (30, 3))
              
mask = a > b
mask
              
new = a[mask]
new
              
new.sum()
    ''')
    elif st == '1.3.8':
        print('''
tens = torch.randint(0, 10, (20, 20))
tens
        
tens.diagonal(offset = 1) #–¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω–Ω—Ç—ã, —Å–¥–≤–∏–≥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≥–ª–∞–≤–Ω–æ–µ –¥–∏–∞–≥–æ–Ω–∞–ª–∏
    ''')
    elif st == '1.3.9':
        print('''
ar1 = torch.randint(0,11, (5, 10))
ar2 = torch.randint(0, 10, (5, 10))
ar1, ar2
              
mask = ar1 == ar2
mask
              
new = 2*ar1[mask]
new
    ''')
    elif st == '1.3.10':
        print('''
ar1 = torch.randint(0, 10, (4,7))
ar2 = torch.randint(0, 10, (4,7))
ar3 = torch.randint(0, 10, (4,7))
ar1, ar2, ar3
              
mask = (ar1 > ar2) & (ar1 < ar3)
              
ar1[mask] = 0
              
ar1
    ''')
    elif st == '1.3.11':
        print('''
ar1 = torch.randint(0, 21, (20, 5))
ar1
              
max_v, max_ind = ar1.max(dim = 1)
max_v, max_ind
              
#–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º
counts = torch.zeros(ar1.size(1), dtype=torch.int)
counts
              
for i in range(ar1.size(0)):
    counts[max_ind[i]] += 1
              
col = (counts >= 5).nonzero(as_tuple=True)[0]
col
    ''')
    elif st == '1.3.12':
        print('''
ar1 = torch.rand((4, 7))
ar1
              
max_v = ar1.max()
max_i = torch.nonzero(ar1 == max_v)
max_v, max_i
              
#–µ—Å–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω, –æ–±–Ω—É–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
if max_i.size(0) > 0:
    row_index = max_i[0, 0].item()
    col_index = max_i[0, 1].item()

    # –û–±–Ω—É–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –Ω–∏–∂–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
    ar1[row_index + 1:, :] = 0
    # –û–±–Ω—É–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã –ø—Ä–∞–≤–µ–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
    ar1[row_index, col_index + 1:] = 0
              
ar1
    ''')
    elif st == '1.3.13':
        print('''
a = torch.tensor([2, 3, 2, 2, 2, 1])
a
              
max_v = a.max().item()
max_v
              
one_hot = torch.zeros((a.size(0), max_v), dtype=torch.float)
one_hot
              
one_hot[torch.arange(a.size(0)), a - 1] = 1
one_hot
    ''')
    elif st == '1.3.14':
        print('''
arr = torch.randint(0, 101, (20,))
arr
              
unique_v, counts = arr.unique(return_counts=True)
unique_v, counts
              
max_count_index = torch.argmax(counts)
most_common_value = unique_v[max_count_index]
most_common_count = counts[max_count_index]
              
ind = (arr == most_common_value).nonzero(as_tuple=True)[0]
ind
    ''')
    elif st == '1.4.1':
        print('''
def f(x):
    return x * torch.sin(x)
              
x_values = torch.linspace(0, 10, 1000)
y_values = f(x_values)
              
threshold = 0.5 #–ø–æ—Ä–æ–≥
              
count_above = (y_values > threshold).sum()
count_above
              
total_points = y_values.numel() #–æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ —Ç–µ–Ω–∑–æ—Ä–µ
fraction_above_threshold = count_above.item() / total_points
fraction_above_threshold
    ''')
    elif st == '1.4.2':
        print('''
n = 100
              
tens1 = torch.ones(n + 1, dtype=torch.bool)
tens1
              
# 0 –∏ 1 –Ω–µ —è–≤–ª—è—é—Ç—Å—è –ø—Ä–æ—Å—Ç—ã–º–∏ —á–∏—Å–ª–∞–º–∏
tens1[0] = False
tens1[1] = False
              
# –†–µ—à–µ—Ç–æ –≠—Ä–∞—Ç–æ—Å—Ñ–µ–Ω–∞: –ø–æ–º–µ—á–∞–µ–º —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —á–∏—Å–ª–∞
for i in range(2, int(n**0.5) + 1):
    if tens1[i]:
        tens1[i*i:n+1:i] = False  # –û–±–Ω—É–ª—è–µ–º –≤—Å–µ –∫—Ä–∞—Ç–Ω—ã–µ i
              
primes = torch.nonzero(tens1).flatten() #–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π
primes
#–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è (–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è) –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä
    ''')
    elif st == '1.4.3':
        print('''
a = torch.tensor([1.0, 2.0, 3.0])
b = torch.tensor([4.0, 5.0, 6.0])
              
if a.size(0) != b.size(0):
    raise ValueError("–¢–µ–Ω–∑–æ—Ä—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å.")
              
distance = torch.sqrt(torch.sum((a - b) ** 2))
distance
    ''')
    elif st == '1.4.4':
        print('''
tensor = torch.randint(0, 101, (20, 3))
              
lengths = torch.sqrt(torch.sum(tensor**2, dim=1))

# –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–Ω–∑–æ—Ä –ø–æ –¥–ª–∏–Ω–∞–º –≤–µ–∫—Ç–æ—Ä–æ–≤
sorted_tensor, sorted_indices = torch.sort(tensor, dim=0)  # –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –º—ã —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å—É
              
tensor[sorted_indices]
    ''')
    elif st == '1.4.5':
        print('''
tensor = torch.tensor([1, 3, 7, 1, 2, 6, 0, 1])
              
local_maxima_indices = []
              
# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç, –Ω–∞—á–∏–Ω–∞—è —Å–æ –≤—Ç–æ—Ä–æ–≥–æ –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞—è –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–∏–º
for i in range(1, len(tensor) - 1):
    if tensor[i] > tensor[i - 1] and tensor[i] > tensor[i + 1]:
        local_maxima_indices.append(i)

local_maxima_indices
    ''')
    elif st == '1.4.6':
        print('''
import numpy as np
              
array = np.random.rand(100)
array
              
target_value = 0.5
              
# –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å —ç–ª–µ–º–µ–Ω—Ç–∞, –Ω–∞–∏–±–æ–ª–µ–µ –±–ª–∏–∑–∫–æ–≥–æ –∫ –∑–∞–¥–∞–Ω–Ω–æ–º—É —á–∏—Å–ª—É
closest_index = np.argmin(np.abs(array - target_value)) #–º–æ–¥—É–ª—å
closest_index
              
# –ü–æ–ª—É—á–∞–µ–º –±–ª–∏–∂–∞–π—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
closest_value = array[closest_index]
closest_value
    ''')
    elif st == '1.4.7':
        print('''
A = np.array([[-1, 2, 4],
              [-3, 1, 2],
              [-3, 0, 1]])

B = np.array([[3, -1],
              [2, 1]])

C = np.array([[7, 21],
              [11, 8],
              [8, 4]])
A, B, C
              
# –ù–∞—Ö–æ–¥–∏–º –æ–±—Ä–∞—Ç–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã A –∏ B
A_inv = np.linalg.inv(A)
B_inv = np.linalg.inv(B)
A_inv, B_inv
              
X = A_inv @ (-C) @ B_inv
X
    ''')
    elif st == '1.4.8':
        print('''
a1 = np.array([3, -3, 0, 7])
a2 = np.array([2, 2, 4, 7])
a3 = np.array([1, 2, 3, 4])
a4 = np.array([5, -4, 1, 3])
a1, a2, a3, a4
              
matrix = np.array([a1, a2, a3, a4])
matrix
              
rank = np.linalg.matrix_rank(matrix)
rank
              
# –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–Ω–µ–π–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
if rank < matrix.shape[0]:  # matrix.shape[0] - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤
    print("–í–µ–∫—Ç–æ—Ä—ã –ª–∏–Ω–µ–π–Ω–æ –∑–∞–≤–∏—Å–∏–º—ã.")
else:
    print("–í–µ–∫—Ç–æ—Ä—ã –ª–∏–Ω–µ–π–Ω–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã.")
    ''')
    elif st == '1.4.9':
        print('''
import matplotlib.pyplot as plt
              
mu = 0
sigma = 10
size = 200
              
tensor = torch.normal(mu, sigma, (size,))
tensor
              
# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞
tensor_int = tensor.round().long() #—Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
tensor_int
              
plt.hist(tensor_int.numpy(), bins=20, edgecolor='black', alpha=0.7)
plt.title('–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª')
plt.xlabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
plt.grid(axis='y')
plt.show()
    ''')
    else:
        print('no such index')



def blocks_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.blocks_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")
          
2.1.1. –ò—Å–ø–æ–ª—å–∑—É—è –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ torch, —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ–Ω —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ weights –∏ bias. 
–ü—Ä–æ–≥–Ω–∞—Ç—å –≤–µ–∫—Ç–æ—Ä inputs —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ–Ω –∏ –≤—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
          
2.1.2 –ò—Å–ø–æ–ª—å–∑—É—è –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ torch, —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ weights –∏ biases. 
–ü—Ä–æ–≥–Ω–∞—Ç—å –≤–µ–∫—Ç–æ—Ä inputs —á–µ—Ä–µ–∑ —Å–ª–æ–π –∏ –≤—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
          
2.1.3 –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π –∏–∑ 2.1.2 —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –æ–Ω –º–æ–≥ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ –º–∞—Ç—Ä–∏—Ü—É (–±–∞—Ç—á) —Å –¥–∞–Ω–Ω—ã–º–∏. 
–ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É. –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –ø—Ä–æ–≥–æ–Ω–∞ —Å–∫–≤–æ–∑—å —Å–ª–æ–π –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–∞ batch_size x n_neurons.
          
2.1.4 –ò—Å–ø–æ–ª—å–∑—É—è –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ torch, —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π –∏–∑ n_neurons –Ω–µ–π—Ä–æ–Ω–æ–≤ —Å n_features –≤–µ—Å–∞–º–∏ —É –∫–∞–∂–¥–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞ 
(–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è). –ü—Ä–æ–≥–Ω–∞—Ç—å –≤–µ–∫—Ç–æ—Ä inputs —á–µ—Ä–µ–∑ —Å–ª–æ–π –∏ –≤—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç. 
–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –ø—Ä–æ–≥–æ–Ω–∞ —Å–∫–≤–æ–∑—å —Å–ª–æ–π –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–∞ batch_size x n_neurons.
          
2.1.5 –ò—Å–ø–æ–ª—å–∑—É—è —Ä–µ—à–µ–Ω–∏–µ –∏–∑ 2.1.4, —Å–æ–∑–¥–∞—Ç—å 2 –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ—è –∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É inputs –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ —ç—Ç–∏ –¥–≤–∞ —Å–ª–æ—è. 
–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –ø–µ—Ä–≤–æ–º —Å–ª–æ–µ –≤—ã–±—Ä–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤–æ –≤—Ç–æ—Ä–æ–º —Å–ª–æ–µ –≤—ã–±—Ä–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –ø—Ä–æ–≥–æ–Ω–∞ —è–≤–ª—è–ª–∞—Å—å –º–∞—Ç—Ä–∏—Ü–∞ (3x7).
          
2.2.1 –ò—Å–ø–æ–ª—å–∑—É—è –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ torch, —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU:
–°–æ–∑–¥–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—É —Ä–∞–∑–º–µ—Ä–∞ (4,3), –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—É—é —á–∏—Å–ª–∞–º–∏ –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.
          
2.2.2 –ò—Å–ø–æ–ª—å–∑—É—è –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ torch, —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ softmax:
–°–æ–∑–¥–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—É —Ä–∞–∑–º–µ—Ä–∞ (4,3), –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—É—é —á–∏—Å–ª–∞–º–∏ –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. 
–°—Ç—Ä–æ–∫–∏ –º–∞—Ç—Ä–∏—Ü—ã —Ç—Ä–∞–∫—Ç–æ–≤–∞—Ç—å –∫–∞–∫ –≤—ã—Ö–æ–¥—ã –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è 4 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.
          
2.2.3 –ò—Å–ø–æ–ª—å–∑—É—è –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ torch, —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ELU:
–°–æ–∑–¥–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—É —Ä–∞–∑–º–µ—Ä–∞ (4,3), –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—É—é —á–∏—Å–ª–∞–º–∏ –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.
          
2.3.1 –ò—Å–ø–æ–ª—å–∑—É—è –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ torch, —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å MSE:
–°–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π —Å 1 –Ω–µ–π—Ä–æ–Ω–æ–º, –ø—Ä–æ–≥–Ω–∞—Ç—å —á–µ—Ä–µ–∑ –Ω–µ–≥–æ –±–∞—Ç—á inputs –∏ –ø–æ—Å—á–∏—Ç–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ MSE, —Ç—Ä–∞–∫—Ç—É—è –≤–µ–∫—Ç–æ—Ä y –∫–∞–∫ –≤–µ–∫—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.
          
2.3.2 –ò—Å–ø–æ–ª—å–∑—É—è –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ torch, —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å Categorical Cross-Entropy:
–°–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π —Å 3 –Ω–µ–π—Ä–æ–Ω–∞–º–∏ –∏ –ø—Ä–æ–≥–Ω–∞—Ç—å —á–µ—Ä–µ–∑ –Ω–µ–≥–æ –±–∞—Ç—á inputs. 
–ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ softmax. –ü–æ—Å—á–∏—Ç–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ CCE, —Ç—Ä–∞–∫—Ç—É—è –≤–µ–∫—Ç–æ—Ä y –∫–∞–∫ –≤–µ–∫—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.
          
2.3.3 –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å 2.3.1, –¥–æ–±–∞–≤–∏–≤ L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é.
          
2.4.1 –ò—Å–ø–æ–ª—å–∑—É—è –æ–¥–∏–Ω –Ω–µ–π—Ä–æ–Ω –∏ SGD (1 –ø—Ä–∏–º–µ—Ä –∑–∞ —à–∞–≥), —Ä–µ—à–∏—Ç–µ –∑–∞–¥–∞—á—É —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
          
2.4.1.1 –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫–ª–∞—Å—Å SquaredLoss
          
2.4.1.2. –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ –∫–ª–∞—Å—Å Neuron –∏–∑ 2.1.1:
1) –°–¥–µ–ª–∞–π—Ç–µ —Ç–∞–∫, —á—Ç–æ–±—ã –≤–µ—Å–∞ –Ω–µ–π—Ä–æ–Ω–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
2) –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ä–∞—Å—á–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤–µ—Å–æ–≤ weights –∏ bias
          
2.4.1.3 –î–æ–ø–∏—à–∏—Ç–µ —Ü–∏–∫–ª –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–µ—Å–æ–≤ –Ω–µ–π—Ä–æ–Ω–∞
SGD
          
2.4.2 –†–µ—à–∏—Ç–µ –∑–∞–¥–∞—á—É 2.4.1, –∏—Å–ø–æ–ª—å–∑—É—è –ø–∞–∫–µ—Ç–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫
          
2.4.1.1 –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ –∫–ª–∞—Å—Å MSELoss –∏–∑ 2.3.1, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–≤ —Ä–∞—Å—á–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è —Å —É—á–µ—Ç–æ–º —Ç–æ–≥–æ, 
—á—Ç–æ —Ç–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞ –≤–µ–¥–µ—Ç—Å—è —Å –±–∞—Ç—á–∞–º–∏, –∞ –Ω–µ —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏
          
2.4.2.2. –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ –∫–ª–∞—Å—Å Neuron –∏–∑ 2.4.1.2:
1) –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ forward —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –æ–Ω –º–æ–≥ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ –º–∞—Ç—Ä–∏—Ü—É (–±–∞—Ç—á) —Å –¥–∞–Ω–Ω—ã–º–∏.
2) –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ä–∞—Å—á–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤–µ—Å–æ–≤ weights –∏ bias —Å —É—á–µ—Ç–æ–º —Ç–æ–≥–æ, —á—Ç–æ —Ç–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞ –≤–µ–¥–µ—Ç—Å—è —Å –±–∞—Ç—á–∞–º–∏, –∞ –Ω–µ —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏
          
2.4.2.3 –î–æ–ø–∏—à–∏—Ç–µ —Ü–∏–∫–ª –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–µ—Å–æ–≤ –Ω–µ–π—Ä–æ–Ω–∞
          
2.4.3 –ò—Å–ø–æ–ª—å–∑—É—è –æ–¥–∏–Ω –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π –∏ –ø–∞–∫–µ—Ç–Ω—ã–π –≥—Ä–∞–¥–∏–µ—Ç–Ω—ã–π —Å–ø—É—Å–∫, —Ä–µ—à–∏—Ç–µ –∑–∞–¥–∞—á—É —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏–∑ 2.4.1
          
2.4.3.1 –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ –∫–ª–∞—Å—Å Linear –∏–∑ 2.1.4. (–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)
          
2.4.3.2 –°–æ–∑–¥–∞–π—Ç–µ —Å–ª–æ–π —Å –æ–¥–Ω–∏–º –Ω–µ–π—Ä–æ–Ω–æ–º. –ò—Å–ø–æ–ª—å–∑—É—è –∫–ª–∞—Å—Å MSELoss –∏–∑ 2.4.2, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è
          
2.4.4 –ò—Å–ø–æ–ª—å–∑—É—è –Ω–∞—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ 2.4, —Å–æ–∑–¥–∞–π—Ç–µ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –∏ —Ä–µ—à–∏—Ç–µ –∑–∞–¥–∞—á—É —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.
–ü—Ä–µ–¥–ª–∞–≥–∞–µ–º–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π —Å 10 –Ω–µ–π—Ä–æ–Ω–∞–º–∏
–ê–∫—Ç–∏–≤–∞—Ü–∏—è ReLU
–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π —Å 1 –Ω–µ–π—Ä–æ–Ω–æ–º
''')
    
def blocks_code(st):
    print('import torch')
    if st == '2.1.1':
        print('''
class Neuron:
  def __init__(self, weights, bias):
    self.weights = weights
    self.bias = bias

  def forward(self, inputs):
    return inputs.dot(self.weights) + self.bias
              
inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])
weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])
bias = 3.14
weights.size()
              
n = Neuron(weights, bias)
res = n.forward(inputs)
res
    ''')
    elif st == '2.1.2':
        print('''
class Linear:
  def __init__(self, weights, biases):
    self.weights = weights
    self.bias = biases

  def forward(self, inputs):
    return self.weights.mv(inputs) + self.bias

# torch.mv(inputs, self) —É–º–Ω–æ–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –Ω–∞ –≤–µ–∫—Ç–æ—Ä
# –≤ –∑–∞–¥–∞–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä, –∞ –Ω–µ –Ω–µ–π—Ä–æ–Ω
              
inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])
weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],
                        [0.5, -0.91, 0.26, -0.5],
                        [-0.26, -0.27, 0.17, 0.87]])

biases = torch.tensor([3.14, 2.71, 7.2])
              
l = Linear(weights, biases)
res = l.forward(inputs)
res
    ''')
    elif st == '2.1.3':
        print('''
class Linearbatch:
  def __init__(self, weights, biases):
    self.weights = weights
    self.bias = biases

  def forward(self, inputs):
    return self.weights.mm(inputs.T) + self.bias

# torch.mv(inputs, self)
#–¢, —á—Ç–æ–± –º–∞—Ç—Ä–∏—Ü–∞ –±—ã–ª–∞ —Ä–∞–∑–º–µ—Ä–æ–º inputs * butch
              
weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],
                        [0.5, -0.91, 0.26, -0.5],
                        [-0.26, -0.27, 0.17, 0.87]])

biases = torch.tensor([3.14, 2.71, 7.2])
              
inputs = torch.tensor([[1, 2, 3, 2.5],
                       [2, 5, -1, 2],
                       [-1.5, 2.7, 3.3, -0.8]])
              
l1 = Linearbatch(weights, biases)
res = l1.forward(inputs)
res
    ''')
    elif st == '2.1.4':
        print('''
class LinearS:
  def __init__(self, n_features, n_neurons):
    self.weights = torch.randn(n_features, n_neurons) #–º–∞—Ç—Ä–∏—Ü–∞
    self.bias = torch.randn(1, n_neurons) #–≤–µ–∫—Ç–æ—Ä

  def forward(self, inputs):
    return inputs.mm(self.weights) + self.bias

#–º–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ, –≥–¥–µ –≤ inputs —Å—Ç—Ä–æ–∫–∏ - –±–∞—Ç—á —Å–∞–π–∑, —Å—Ç–æ–ª–±—Ü—ã - –∫–æ–ª-–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤
              
inputs = torch.tensor([[1, 2, 3, 2.5],
                       [2, 5, -1, 2],
                       [-1.5, 2.7, 3.3, -0.8]])
              
l = LinearS(4, 3)
l.forward(inputs)
    ''')
    elif st == '2.1.5':
        print('''
inputs = torch.tensor([[1, 2, 3, 2.5],
                       [2, 5, -1, 2],
                       [-1.5, 2.7, 3.3, -0.8]])
              
l1 = LinearS(4,10) #4 –≤—Ö–æ–¥–∞, 10 –Ω–µ–π—Ä–æ–Ω–æ–≤
l2 = LinearS(10,7) #10 –≤—Ö–æ–¥–æ–≤ (–∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è), 7 –Ω–µ–π—Ä–æ–Ω–æ–≤

res1 = l1.forward(inputs)
res1.shape
              
res2 = l2.forward(res1)
res2.shape
              
res2
    ''')
    elif st == '2.2.1':
        print('''
#def replace(vector):
  #return [max(0,x) for x in vector]
              
#vector = [-5, 2, -10, -9, 6, 7, 0]
#replace(vector)
              
#–†–µ–∞–ª–∏–∑–∞—Ü–∏—è Rectified Linear Unit
              
class ReLU:
    def forward(self, inputs):
        return torch.maximum(inputs, torch.tensor(0.0))  # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è ReLU
#–±–µ—Ä–µ–º –º–∞–∫—Å–∏–º—É–º –º–µ–∂–¥—É –∫–∞–∂–¥—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º —Ç–µ–Ω–∑–æ—Ä–∞ –∏ 0
              
inputs = torch.randn(4, 3)
inputs
              
relu = ReLU()
              
go = relu.forward(inputs)
go
    ''')
    elif st == '2.2.2':
        print('''
class Softmax:
  def forward(self, inputs):
    values_exp = torch.exp(inputs)
    sum_exp = values_exp.sum(dim = 1, keepdim = True) # keepdim = True —Ç–µ–Ω–∑–æ—Ä –±—É–¥–µ—Ç –∏–º–µ—Ç—å —Ç—É –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, —á—Ç–æ –∏ –≤—Ö–æ–¥–Ω–æ–π, dim = 1 - —Å—É–º–º–∞ —Å—Ç–æ–ª–±—Ü–æ–≤
    #–¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
    return values_exp/sum_exp
              
inputs = torch.randn(4, 3)
inputs
              
softmax = Softmax()
              
go = softmax.forward(inputs)
go
    ''')
    elif st == '2.2.3':
        print('''
#–†–µ–∞–ª–∏–∑–∞—Ü–∏—è Exponential Linear Unit
              
class ELU:
  def __init__(self, alpha):
    self.alpha = alpha

  def forward(self, inputs):
    return torch.where(inputs > 0, inputs, self.alpha * (torch.exp(inputs) - 1))
              
inputs = torch.randn(4, 3)
inputs
              
elu = ELU(alpha = 0.5)
go = elu.forward(inputs)
go
    ''')
    elif st == '2.3.1':
        print('''
class MSELoss:
  def forward(self, y_pred, y_true):
    return ((y_pred - y_true) ** 2).sum() / len(y_pred)
              
inputs = torch.tensor([[1, 2, 3, 2.5],
                       [2, 5, -1, 2],
                       [-1.5, 2.7, 3.3, -0.8]])

y = torch.tensor([2, 3, 4])
              
s = LinearS(4, 1)
go = s.forward(inputs)
go
              
y_pred = ReLU().forward(go)
y_pred
              
MSELoss().forward(y_pred = y_pred, y_true = y)
    ''')
    elif st == '2.3.2':
        print('''
class CategoricalCrossentropyLoss:
  def forward(self, y_pred, y_true):
    L = -(y_true*torch.log(y_pred)).sum()/len(y_pred)
    return L
              
inputs = torch.tensor([[1, 2, 3, 2.5],
                        [2, 5, -1, 2],
                        [-1.5, 2.7, 3.3, -0.8]])
y = torch.tensor([1, 0, 0])
              
s = LinearS(4, 3)
go = s.forward(inputs)
go
              
y_pred = Softmax().forward(go)
y_pred
              
CategoricalCrossentropyLoss().forward(y_pred = y_pred, y_true = y)
    ''')
    elif st == '2.3.3':
        print('''
#L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
              
class MSELossL2:
  def __init__(self, lambda_ = 1):
    self.lambda_ = lambda_

  def data_loss(self, y_pred, y_true):
    return torch.sum((y_true - y_pred) ** 2) #–ø–æ–¥—Å—á–µ—Ç –ø–µ—Ä–≤–æ–≥–æ —Å–ª–∞–≥–∞–µ–º–æ–≥–æ –∏–∑ —Ñ–æ—Ä–º—É–ª—ã

  def reg_loss(self, layer):
    # –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞—Ç—Ä–∏–±—É—Ç—ã –æ–±—ä–µ–∫—Ç–∞ layer, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —Ö—Ä–∞–Ω—è—Ç—Å—è –≤–µ—Å–∞ —Å–ª–æ—è
    # <–ø–æ–¥—Å—á–µ—Ç –≤—Ç–æ—Ä–æ–≥–æ —Å–ª–∞–≥–∞–µ–º–æ–≥–æ –∏–∑ —Ñ–æ—Ä–º—É–ª—ã>
    return self.lambda_ * torch.sum(layer ** 2)

  def forward(self, y_pred, y_true, layer):
    return self.data_loss(y_pred, y_true) + self.reg_loss(layer)
              

y_pred = torch.tensor([-0.5, 1, 1.7])
y_true = torch.tensor([0, 0.6, 2.3])
layer = torch.normal(0, 5, (10, 1))
              
s = LinearS(4, 1)
go = s.forward(inputs)
go
              
mse2 = MSELossL2(0.4)
              
loss_value = mse2.forward(y_pred, y_true, layer)
loss_value
    ''')
    elif st == '2.4.1':
        print('''
learning_rate = 0.01  #—à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
epochs = 1000
x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])
y_train = torch.tensor([[3.0], [5.0], [7.0], [9.0]])
w = torch.randn(1, requires_grad=True)
b = torch.randn(1, requires_grad=True)
#requires_grad=True –≥—Ä–∞–¥–∏–µ–Ω—Ç, –∏–∑–º–µ–Ω–µ–Ω–∏–µ w b
              
def predict(x):
    return x * w + b

def mse_loss(y_pred, y_true):
    return ((y_pred - y_true) ** 2).mean()
              
for epoch in range(epochs):
    total_loss = 0

    for x, y in zip(x_train, y_train):
        y_pred = predict(x)
        loss = mse_loss(y_pred, y)

        loss.backward()  #–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤

        #–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
        with torch.no_grad():
            w -= learning_rate * w.grad
            b -= learning_rate * b.grad

        total_loss += loss.item() #–Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏

    if epoch % 100 == 0:
      print(f'Epoch {epoch}, Loss: {total_loss / len(x_train)}')

print(f'–û–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å—ã: {w.item()}')
print(f'–û–±—É—á–µ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ: {b.item()}')
    ''')
    elif st == '2.4.1.1':
        print('''
class SquaredLoss:
  def forward(self, y_pred, y_true):
    return (y_pred-y_true)**2.  #—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ª–æ–≥–∏–∫—É MSE

  def backward(self, y_pred, y_true):
    self.dinput = 2*(y_pred-y_true)  #df/dy_pred
    ''')
    elif st == '2.4.1.2':
        print('''
def forward(self, weights, bias):
  self.weights = weights
  self.bias = bias

def backward(self, dvalue):
  dweights = dvalue * self.inputs
  dinputs = dvalue * self.weights
  dbias = 1 * dvalue
              
class Neuron:
    def __init__(self, n_inputs):
        self.w = torch.randn(n_inputs)
        self.b = torch.randn(1)

    def forward(self, inputs):
      self.inputs = inputs
      return inputs.dot(self.w) + self.b


    def backward(self, dvalue):
    # –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤–µ—Å–∞–º (–∫–∞–∫ —Å–∏–ª—å–Ω–æ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è –æ—à–∏–±–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–∞–∂–¥–æ–≥–æ –≤–µ—Å–∞)
      self.dweights = dvalue * self.inputs  # dL/dW = dL/dy * dy/dW

    # –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤—Ö–æ–¥–∞–º (–∫–∞–∫ —Å–∏–ª—å–Ω–æ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è –æ—à–∏–±–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–∞–∂–¥–æ–≥–æ –≤—Ö–æ–¥–∞)
      self.dinput = dvalue * self.w  # dL/dX = dL/dy * dy/dX

    # –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ —Å–º–µ—â–µ–Ω–∏—é (–∫–∞–∫ —Å–∏–ª—å–Ω–æ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è –æ—à–∏–±–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–º–µ—â–µ–Ω–∏—è)
      self.dbias = dvalue * 1  # dL/db = dL/dy * dy/db = dL/dy
    ''')
    elif st == '2.4.1.3':
        print('''
class Neuron:
    def __init__(self, n_inputs):
        self.w = torch.randn(n_inputs)
        self.b = torch.randn(1)

    def forward(self, inputs):
        self.inputs = inputs
        return inputs.dot(self.w) + self.b

    def backward(self, dvalue):
        self.dweights = dvalue * self.inputs
        self.dinput = dvalue * self.w
        self.dbias = dvalue * 1

class SquaredLoss:
    def forward(self, y_pred, y_true):
        return (y_pred - y_true) ** 2  # MSE loss

    def backward(self, y_pred, y_true):
        self.dinput = 2 * (y_pred - y_true)  # dL/dy
              
X = torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])
y = torch.tensor([2.0, 3.0, 4.0])

n_inputs = X.size()[1]
lr = 0.1  #–Ω—é
n_epoch = 100

neuron = Neuron(n_inputs)
loss = SquaredLoss()

losses = []
              
for epoch in range(n_epoch):
    for x_example, y_example in zip(X, y):
        #forward pass
        y_pred = neuron.forward(x_example)
        curr_loss = loss.forward(y_pred, y_example)
        losses.append(curr_loss)

        #backpropagation
        loss.backward(y_pred, y_example)
        neuron.backward(loss.dinput)

        # –®–∞–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞
        neuron.w -= lr * neuron.dweights
        neuron.b -= lr * neuron.dbias
              
import matplotlib.pyplot as plt
plt.plot([tensor.item() for tensor in losses])
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Loss over time')
plt.show()
    ''')
    elif st == '2.4.2':
        print('''
import torch
from sklearn.datasets import make_regression

class Neuron:
    def __init__(self, n_inputs):
        self.w = torch.randn(n_inputs, 1, requires_grad=True)
        self.b = torch.randn(1, requires_grad=True)

    def forward(self, inputs):
        return inputs.matmul(self.w) + self.b

class MSELoss:
    def forward(self, y_pred, y_true):
        self.y_pred = y_pred
        self.y_true = y_true
        return ((y_pred - y_true) ** 2).mean()

    def backward(self):
        return 2 * (self.y_pred - self.y_true) / len(self.y_pred)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)
X = torch.from_numpy(X).to(dtype=torch.float32)
y = torch.from_numpy(y).to(dtype=torch.float32).view(-1, 1)

neuron = Neuron(n_inputs=4)
loss_fn = MSELoss()

learning_rate = 0.01
n_epochs = 500
batch_size = 10
losses = []
              

for epoch in range(n_epochs):
    permutation = torch.randperm(X.size(0)) #–ø–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤

    for i in range(0, X.size(0), batch_size): #–ø—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –¥–∞–Ω–Ω—ã–º , —Ä–∞–∑–±–∏–≤–∞–µ–º –∏—Ö –Ω–∞ –ø–∞–∫–µ—Ç—ã —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á —Å–∞–π–∑
        indices = permutation[i:i + batch_size] #–∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
        X_batch, y_batch = X[indices], y[indices]

        y_pred = neuron.forward(X_batch)

        #—Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        loss = loss_fn.forward(y_pred, y_batch)
        losses.append(loss.item()) #—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ

        #backpropagation
        loss_grad = loss_fn.backward()  #–≥—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å

        #–≤—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏—è
        loss_grad_w = X_batch.t() @ loss_grad #–ø–æ–ª—É—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –∫–∞–∂–¥–æ–º—É –≤–µ—Å—É
        loss_grad_b = loss_grad.mean() #–°—Ä –∑–Ω–∞—á –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ—Ç–µ—Ä—å –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–º–µ—â–µ–Ω–∏—è (–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–º–µ—â–µ–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç  –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–∫–∏)

        with torch.no_grad():
            neuron.w -= learning_rate * loss_grad_w
            neuron.b -= learning_rate * loss_grad_b

    if epoch % 100 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item()}')
              

plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss over time')
plt.show()
    ''')
    elif st == '2.4.2.1':
        print('''
class MSELoss:
    def forward(self, y_pred, y_true):
        # –í—ã—á–∏—Å–ª—è–µ–º MSE –¥–ª—è –±–∞—Ç—á–∞
        return ((y_pred - y_true) ** 2).mean()  # —É—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤—Å–µ–º—É –±–∞—Ç—á—É

    def backward(self, y_pred, y_true):
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ y_pred –¥–ª—è –±–∞—Ç—á–∞
        batch_size = len(y_pred)
        return 2 * (y_pred - y_true) / batch_size  # —É—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –±–∞—Ç—á—É
              

y_true = torch.tensor([[2.0], [3.0], [4.0]])
y_pred = torch.tensor([[2.5], [2.8], [4.2]])

loss = MSELoss()

loss_value = loss.forward(y_pred, y_true)
print(f'Loss: {loss_value}')

gradients = loss.backward(y_pred, y_true)
gradients
    ''')
    elif st == '2.4.2.2':
        print('''
class Neuron:
  def __init__(self, n_inputs):
    self.w = torch.randn(n_inputs, 1)
    self.b = torch.randn(1)

  def forward(self, inputs):
    self.inputs = inputs
    return inputs.matmul(self.w) + self.b #–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è

  def backward(self, dvalue):
    # dvalue - –∑–Ω–∞—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä–æ–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç –Ω–µ–π—Ä–æ–Ω—É –æ—Ç —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è —Å–µ—Ç–∏
    # –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ —ç—Ç–æ –±—É–¥–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç L –ø–æ y^ (—Å–æ–∑–¥–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥–æ–º backwards —É –æ–±—ä–µ–∫—Ç–∞ MSELoss)
    self.dweights = self.inputs.t().matmul(dvalue)  # df/dW
    self.dinput = dvalue.matmul(self.w.t())          # df/dX
    self.dbias = dvalue.sum(dim=0)# df/db
    ''')
    elif st == '2.4.2.3':
        print('''
class Neuron:
    def __init__(self, n_inputs):
        self.w = torch.randn(n_inputs, 1)
        self.b = torch.randn(1)

    def forward(self, inputs):
        self.inputs = inputs
        return inputs.matmul(self.w) + self.b

    def backward(self, dvalue):
        # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏
        self.dweights = self.inputs.t().matmul(dvalue)  # df/dW
        self.dinput = dvalue.matmul(self.w.t())  # df/dX
        self.dbias = dvalue.sum(dim=0)  # df/db
              
class MSELoss:
    def forward(self, y_pred, y_true):
        return ((y_pred - y_true) ** 2).mean()  # –°—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞

    def backward(self, y_pred, y_true):
        return 2 * (y_pred - y_true) / y_true.size(0)  # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è MSE
              
X = torch.tensor([[1.0, 2.0],
                  [3.0, 4.0],
                  [5.0, 6.0]], dtype=torch.float32)  # –ë–∞—Ç—á –∏–∑ 3 –ø—Ä–∏–º–µ—Ä–æ–≤ —Å 2 –≤—Ö–æ–¥–∞–º–∏
y = torch.tensor([[1.0],
                  [2.0],
                  [3.0]], dtype=torch.float32)
              
n_inputs = X.size(1)
lr = 0.01
n_epochs = 100

neuron = Neuron(n_inputs)
loss_fn = MSELoss()
              

for epoch in range(n_epochs):
    total_loss = 0

    for x_example, y_example in zip(X, y):  # –ü—Ä–æ—Ö–æ–¥ –ø–æ –∫–∞–∂–¥–æ–º—É –ø—Ä–∏–º–µ—Ä—É –≤ –±–∞—Ç—á–µ
        y_pred = neuron.forward(x_example.unsqueeze(0))  # –ü—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ–Ω
        curr_loss = loss_fn.forward(y_pred, y_example.unsqueeze(0))  #–¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        total_loss += curr_loss.item()

        # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
        dvalue = loss_fn.backward(y_pred, y_example.unsqueeze(0))  # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ—Ç–µ—Ä—å
        neuron.backward(dvalue)  # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ–π—Ä–æ–Ω–∞

        neuron.w -= lr * neuron.dweights
        neuron.b -= lr * neuron.dbias

    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {total_loss / len(X)}')

neuron.w
              

plt.figure(figsize=(10, 6))
plt.plot(losses, label='Training Loss', color='blue')
plt.title('Loss During Training')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.show()
    ''')
    elif st == '2.4.3':
        print('''
import torch.nn as nn
import torch.optim as optim
              
n_inputs = X.size(1)
n_outputs = y.size(1)
lr = 0.01
n_epochs = 100
batch_size = 2
              
model = nn.Linear(n_inputs, n_outputs)  # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π
              
criterion = nn.MSELoss()  # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å: —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞
optimizer = optim.SGD(model.parameters(), lr=lr)  #–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä - —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫
              
losses = []
              
for epoch in range(n_epochs):
    total_loss = 0

    for i in range(0, len(X), batch_size):
        X_batch = X[i:i + batch_size]  #–±–∞—Ç—á –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        y_batch = y[i:i + batch_size]  #–±–∞—Ç—á –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

        #—Ñ–æ—Ä–≤–∞—Ä–¥ –ø–∞—Å—Å
        y_pred = model(X_batch)  # –ü—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π

        curr_loss = criterion(y_pred, y_batch)
        total_loss += curr_loss.item()

        curr_loss.backward()  #–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        optimizer.step()  #–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤

    avg_loss = total_loss / (len(X) / batch_size)
    losses.append(avg_loss)

    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {avg_loss}')
              
model.weight.data
              

model.bias.data
    ''')
    elif st == '2.4.3.1':
        print('''
class Linear:
    def __init__(self, n_features, n_neurons):
        self.weights = torch.randn(n_features, n_neurons)
        self.biases = torch.zeros(1, n_neurons)

    def forward(self, inputs):
        self.inputs = inputs
        return inputs.matmul(self.weights) + self.biases

    def backward(self, dvalues):
        self.dweights = self.inputs.t().matmul(dvalues)  # df/dW = X^T * dL/dy
        self.dbiases = dvalues.sum(dim=0, keepdim=True)  # df/db = sum(dL/dy)
        self.dinputs = dvalues.matmul(self.weights.t())  # df/dX = dL/dy * W^T

        return self.dinputs  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è
              
    ''')
    elif st == '2.4.3.2':
        print('''
class MSELoss:
    def forward(self, y_pred, y_true):
        return ((y_pred - y_true) ** 2).sum() / len(y_pred)

    def backward(self, y_pred, y_true):
        # df/dy = 2*(y_pred - y_true) / N
        return 2 * (y_pred - y_true) / len(y_pred)
              
class Linear:
    def __init__(self, n_features, n_neurons):
        self.weights = torch.randn(n_features, n_neurons)
        self.biases = torch.zeros(1, n_neurons)

    def forward(self, inputs):
        self.inputs = inputs
        return inputs.matmul(self.weights) + self.biases

    def backward(self, dvalues):
        # df/dW = X^T * dL/dy
        self.dweights = self.inputs.t().matmul(dvalues)
        # df/db = sum(dL/dy)
        self.dbiases = dvalues.sum(dim=0, keepdim=True)
        # df/dX = dL/dy * W^T
        self.dinputs = dvalues.matmul(self.weights.t())
        return self.dinputs  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è
              

X = torch.tensor([[1.0, 2.0],
                  [3.0, 4.0],
                  [5.0, 6.0],
                  [7.0, 8.0]], dtype=torch.float32)  # –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (4 –ø—Ä–∏–º–µ—Ä–∞, 2 –ø—Ä–∏–∑–Ω–∞–∫–∞)
y = torch.tensor([[1.0],
                  [2.0],
                  [3.0],
                  [4.0]], dtype=torch.float32)
              
n_features = X.size(1)
n_neurons = 1  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–æ–≤
lr = 0.01
n_epochs = 100
              
linear_layer = Linear(n_features, n_neurons)

loss_fn = MSELoss()

losses = []
              

for epoch in range(n_epochs):
    y_pred = linear_layer.forward(X)

    curr_loss = loss_fn.forward(y_pred, y)
    losses.append(curr_loss.item())

    dvalues = loss_fn.backward(y_pred, y)  # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ—Ç–µ—Ä—å
    linear_layer.backward(dvalues)  # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å–ª–æ–π

    linear_layer.weights -= lr * linear_layer.dweights  #–≤–µ—Å–∞
    linear_layer.biases -= lr * linear_layer.dbiases  #—Å–º–µ—â–µ–Ω–∏—è

    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {curr_loss.item()}')
              

linear_layer.weights.data
              
linear_layer.biases.data
              
plt.figure(figsize=(10, 6))
plt.plot(losses, label='Training Loss', color='blue')
plt.title('Loss During Training')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.show()
    ''')
    elif st == '2.4.4':
        print('''
class MSELoss:
    def forward(self, y_pred, y_true):
        return ((y_pred - y_true) ** 2).sum() / len(y_pred)

    def backward(self, y_pred, y_true):
        return 2 * (y_pred - y_true) / len(y_pred)
              

class Linear:
    def __init__(self, n_features, n_neurons):
        self.weights = torch.randn(n_features, n_neurons) * 0.01  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self.biases = torch.zeros(1, n_neurons)  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–º–µ—â–µ–Ω–∏–π

    def forward(self, inputs):
        self.inputs = inputs
        return inputs.matmul(self.weights) + self.biases

    def backward(self, dvalues):
        self.dweights = self.inputs.t().matmul(dvalues)
        self.dbiases = dvalues.sum(dim=0, keepdim=True)
        self.dinputs = dvalues.matmul(self.weights.t())
        return self.dinputs
              
class ReLU:
    def forward(self, inputs):
        self.inputs = inputs
        return torch.maximum(inputs, torch.tensor(0.0))  # y = max(0, x)

    def backward(self, dvalues):
        self.dinputs = dvalues.clone()
        self.dinputs[self.inputs <= 0] = 0  # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è ReLU
        return self.dinputs
              

X = torch.tensor([[1.0, 2.0],
                  [3.0, 4.0],
                  [5.0, 6.0],
                  [7.0, 8.0],
                  [9.0, 10.0],
                  [11.0, 12.0],
                  [13.0, 14.0],
                  [15.0, 16.0]], dtype=torch.float32)  # –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (8 –ø—Ä–∏–º–µ—Ä–æ–≤, 2 –ø—Ä–∏–∑–Ω–∞–∫–∞)
y = torch.tensor([[2.0],
                  [4.0],
                  [6.0],
                  [8.0],
                  [10.0],
                  [12.0],
                  [14.0],
                  [16.0]], dtype=torch.float32)
              

n_features = X.size(1)
n_neurons_hidden = 10
n_neurons_output = 1  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–æ–≤ (–æ–¥–∏–Ω –Ω–µ–π—Ä–æ–Ω)
lr = 0.01
n_epochs = 100


hidden_layer = Linear(n_features, n_neurons_hidden)
activation = ReLU()
output_layer = Linear(n_neurons_hidden, n_neurons_output)

loss_fn = MSELoss()

losses = []

for epoch in range(n_epochs):
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    hidden_output = hidden_layer.forward(X)
    activated_output = activation.forward(hidden_output)
    y_pred = output_layer.forward(activated_output)


    curr_loss = loss_fn.forward(y_pred, y)
    losses.append(curr_loss.item())


    dvalues = loss_fn.backward(y_pred, y)  # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ—Ç–µ—Ä—å
    dactivated = output_layer.backward(dvalues)  # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    dhidden = activation.backward(dactivated)  # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏—é
    hidden_layer.backward(dhidden)  # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π


    output_layer.weights -= lr * output_layer.dweights  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è
    output_layer.biases -= lr * output_layer.dbiases  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–º–µ—â–µ–Ω–∏–π –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è
    hidden_layer.weights -= lr * hidden_layer.dweights  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è
    hidden_layer.biases -= lr * hidden_layer.dbiases  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–º–µ—â–µ–Ω–∏–π —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è

    # –í—ã–≤–æ–¥ –ø–æ—Ç–µ—Ä—å —á–µ—Ä–µ–∑ –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {curr_loss.item()}')

# –ü–µ—á–∞—Ç—å –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏—è
print("Final weights (output layer):", output_layer.weights.data)
print("Final bias (output layer):", output_layer.biases.data)
print("Final weights (hidden layer):", hidden_layer.weights.data)
print("Final bias (hidden layer):", hidden_layer.biases.data)
              

plt.figure(figsize=(10, 6))
plt.plot(losses, label='Training Loss', color='blue')
plt.title('Loss During Training')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.show()
              

X = torch.linspace(-1, 1, 100).view(-1, 1)
y = X.pow(2) + 0.2 * torch.rand(X.size())
              
class Linear:
    def __init__(self, n_features, n_neurons):
        self.weights = torch.randn(n_features, n_neurons)
        self.biases = torch.randn(1, n_neurons)

    def forward(self, inputs):
        self.inputs = inputs
        return inputs.matmul(self.weights) + self.biases

    def backward(self, dvalues):
        self.dweights = self.inputs.t().matmul(dvalues)  # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –≤–µ—Å–∞–º
        self.dbiases = dvalues.sum(dim=0, keepdim=True)   # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ —Å–º–µ—â–µ–Ω–∏—è–º
        self.dinputs = dvalues.matmul(self.weights.t())    # df/dX
              

class MSELoss:
    def forward(self, y_pred, y_true):
        self.y_pred = y_pred
        self.y_true = y_true
        return ((y_pred - y_true) ** 2).sum() / len(y_pred)

    def backward(self):
        return 2 * (self.y_pred - self.y_true) / len(self.y_pred)
              

class Activation_ReLU:
  def forward(self, inputs):
    self.inputs = inputs
    self.output = inputs.clip(min=0)
    return self.output

  def backward(self, dvalues):
    self.dinputs = dvalues.clone()
    self.dinputs[self.inputs <= 0] = 0
              

import torch.nn as nn
              

fc1 = Linear(n_features=1, n_neurons=20)
# relu = Activation_ReLU()
relu= nn.Sigmoid()
fc2 = Linear(n_features=20, n_neurons=1)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
loss = MSELoss()
lr = 0.1
ys = []
losses = []
batch = 10
num_epochs = 2500  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö

# –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
for epoch in range(num_epochs):
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    ind = torch.randperm(X.size(0))
    xn = X[ind]
    yn = y[ind]

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    total_dweights_fc1 = 0
    total_dbiases_fc1 = 0
    total_dweights_fc2 = 0
    total_dbiases_fc2 = 0

    for i in range(0, X.size(0), batch):
        x_ex = xn[i:i + batch]
        y_ex = yn[i:i + batch]

        out1 = fc1.forward(x_ex)
        out2 = relu.forward(out1)
        y_pred = fc2.forward(out2)

        curr_loss = loss.forward(y_pred, y_ex)
        losses.append(curr_loss.item())

        dvalue = loss.backward()  # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ—Ç–µ—Ä—å
        fc2.backward(dvalue)      # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ —Å–ª–æ—è
        relu.backward(fc2.dinputs) # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU
        fc1.backward(relu.dinputs) # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è

        # –°—É–º–º–∏—Ä—É–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        total_dweights_fc1 += fc1.dweights
        total_dbiases_fc1 += fc1.dbiases
        total_dweights_fc2 += fc2.dweights
        total_dbiases_fc2 += fc2.dbiases

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –±–∞—Ç—á–µ–π
    fc1.weights -= lr * total_dweights_fc1 / (X.size(0) // batch)
    fc1.biases -= lr * total_dbiases_fc1 / (X.size(0) // batch)
    fc2.weights -= lr * total_dweights_fc2 / (X.size(0) // batch)
    fc2.biases -= lr * total_dbiases_fc2 / (X.size(0) // batch)

    if epoch % 200 == 0:
        out1 = fc1.forward(X)
        out2 = relu.forward(out1)
        y_pred = fc2.forward(out2)
        data_loss = loss.forward(y_pred, y)
        print(f'Epoch {epoch} mean loss {data_loss.item()}')
        ys.append(y_pred.detach().clone())
              

fig, axs = plt.subplots(len(ys), 1, figsize=(10, 40))

for ax, y_, epoch in zip(axs, ys, range(0, len(ys) * 200, 200)):
    ax.scatter(X.numpy(), y.numpy(), color="orange", label="–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
    ax.plot(X.numpy(), y_.numpy(), 'g-', lw=3, label=f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
    ax.set_xlim(-1.05, 1.5)
    ax.set_ylim(-0.25, 1.25)
    ax.set_title(f"Epoch {epoch}")
    ax.legend()

plt.tight_layout()
plt.show()
    ''')
    else:
        print('no such index')

def autD_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.autD_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")
          
3.1.1 –í–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏—Å—å –∫–ª–∞—Å—Å–∞–º–∏ Neuron –∏ SquaredLoss –∏–∑ –∑–∞–¥–∞—á–∏ 2.4.1 –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–æ–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç torch, —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É —Ä–µ–≥—Ä–µ—Å—Å–∏–∏. 
–î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ—Ç–Ω—ã–π —Å–ø—É—Å–∫.
          
3.1.2 –í–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏—Å—å –∫–ª–∞—Å—Å–∞–º–∏ Linear –∏ MSELoss –∏–∑ –∑–∞–¥–∞—á–∏ 2.1.4 –∏ 2.3.1, ReLU –∏–∑ 2.2.1 –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ–º, 
–∫–æ—Ç–æ—Ä–æ–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç torch, —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É —Ä–µ–≥—Ä–µ—Å—Å–∏–∏. –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞–∫–µ—Ç–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫. 
–í—ã–≤–µ—Å—Ç–∏ –≥—Ä–∞—Ñ–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–æ–º–µ—Ä–∞ —ç–ø–æ—Ö–∏. –í—ã–≤–µ—Å—Ç–∏ –Ω–∞ –æ–¥–Ω–æ–º –≥—Ä–∞—Ñ–∏–∫–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.
          
3.2 –ê–ª–≥–æ—Ä–∏—Ç–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ torch.optim
          
3.2.1 –†–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É 3.1.1, –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏—Å—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º optim.SDG –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≥–æ —Å–ø—É—Å–∫–∞
          
3.2.2 –†–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É 3.1.2, –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏—Å—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º optim.Adam –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≥–æ —Å–ø—É—Å–∫–∞. 
–í—ã–≤–µ—Å—Ç–∏ –≥—Ä–∞—Ñ–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–æ–º–µ—Ä–∞ —ç–ø–æ—Ö–∏. –í—ã–≤–µ—Å—Ç–∏ –Ω–∞ –æ–¥–Ω–æ–º –≥—Ä–∞—Ñ–∏–∫–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.
          
3.3.1 –†–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, —Å–æ–±–ª—é–¥–∞—è —Å–ª–µ–¥—É—é—â–∏–µ —É—Å–ª–æ–≤–∏—è:
–û—Ñ–æ—Ä–º–∏—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –≤ –≤–∏–¥–µ –∫–ª–∞—Å—Å–∞ - –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∞ nn.Module
–ü—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å–µ—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏ –∏–∑ torch.nn: —Å–ª–æ–∏, —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ —Ç.–¥.
–î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–∑ torch.optim
          

3.3.2 –†–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, —Å–æ–±–ª—é–¥–∞—è —Å–ª–µ–¥—É—é—â–∏–µ —É—Å–ª–æ–≤–∏—è:
–û—Ñ–æ—Ä–º–∏—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –≤ –≤–∏–¥–µ –æ–±—ä–µ–∫—Ç–∞ nn.Sequential
–ü—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å–µ—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏ –∏–∑ torch.nn: —Å–ª–æ–∏, —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ —Ç.–¥.
–î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–∑ torch.optim
          
3.4. Datasets and dataloaders
          
3.4.1 –°–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –ø–æ—Å—Ç–∞–≤–ª—è—é—â–∏–π –¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–¥–∞—á–∏ 3.1.2.
–°–æ–∑–¥–∞—Ç—å DataLoader –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å.
–í–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ 3.3.1 (–∏–ª–∏ 3.3.2) –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å, –ø–æ–ª—å–∑—É—è—Å—å –º–∏–Ω–∏-–ø–∞–∫–µ—Ç–Ω—ã–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º —Å —Ä–∞–∑–º–µ—Ä–æ–º –ø–∞–∫–µ—Ç–∞ (batch_size) = 10
          

3.4.2.1 –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–π–ª–∞ diamonds.csv.
–£–¥–∞–ª–∏—Ç–µ –≤—Å–µ –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã
–¶–µ–ª–µ–≤–æ–π —Å—Ç–æ–ª–±–µ—Ü (y) - price
–ü—Ä–µ–æ–±—Ä–∞–∑—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä
          
3.4.2.2 –†–∞–∑–±–µ–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –æ–±—É—á–∞—é—â–∏–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø—Ä–∏ –ø–æ–º–æ—â–∏ torch.utils.data.random_split.
          
3.4.2.3 –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω –ø—Ä–∏ –ø–æ–º–æ—â–∏ –º–∏–Ω–∏-–ø–∞–∫–µ—Ç–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ (batch_size = 256).
          
3.4.3 –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ –º–µ—Ç–æ–¥ __init__ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ 3.4.2 —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –æ–Ω –º–æ–≥ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä transform: callable. 
–†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫–ª–∞—Å—Å DropColsTransform –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –º–∞—Å—Å–∏–≤–∞. 
–†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫–ª–∞—Å—Å ToTensorTransorm –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –º–∞—Å—Å–∏–≤–∞ –≤ —Ç–µ–Ω–∑–æ—Ä.
          
3.4.2.4 –í—ã–≤–µ–¥–∏—Ç–µ –≥—Ä–∞—Ñ–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–æ–º–µ—Ä–∞ —ç–ø–æ—Ö–∏ (–∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å –¥–ª—è —ç–ø–æ—Ö–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–π—Ç–µ –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –Ω–∞ –∫–∞–∂–¥–æ–º –±–∞—Ç—á–µ). 
–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ.
''')
    
def autD_code(st):
    print('''
import torch
import numpy as np
from matplotlib import pyplot as plt
''')
    if st == '3.1.1':
        print('''
from sklearn.datasets import make_regression

X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)
X = torch.from_numpy(X).to(dtype=torch.float32)
y = torch.from_numpy(y).to(dtype=torch.float32)
              
class Neuron:
    def __init__(self, n_inputs: int):
        self.weights = torch.randn(n_inputs, 1, requires_grad=True)
        self.bias = torch.randn(1, requires_grad=True)

    def __call__(self, inputs: torch.Tensor):
        return inputs @ self.weights + self.bias

  ## def backward(self):
  ##     self.dweights = self.weights.grad
  ##     self.dbias = self.bias.grad
              

## –∫–ª–∞—Å—Å squared loss - —Ä–∞–∑–Ω–∏—Ü–∞ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏
class SquaredLoss:
    def __call__(self, predictions: torch.Tensor, targets: torch.Tensor):
        return torch.mean((predictions - targets) ** 2)
              

def train(model, loss_fn, X, y, lr=0.01, epochs=1000):
    for epoch in range(epochs):


        pred = model(X)  #call –∏–∑ –∫–ª–∞—Å—Å–∞ Neuron
        loss = loss_fn(pred, y)


        loss.backward()



        with torch.no_grad():
            model.weights -= lr * model.weights.grad
            model.bias -= lr * model.bias.grad


            model.weights.grad.zero_() #–∑–∞–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, —Ç–∫ –æ–Ω–∏ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç—Å—è
            model.bias.grad.zero_()


        if epoch % 100 == 0:
            print(f'Epoch {epoch}, Loss: {loss.item()}')
              

##train(neuron, loss_fn, X, y, lr = 0.1, epochs = 1000)
              

neuron = Neuron(n_inputs=X.shape[1])
loss_fn = SquaredLoss()

train(neuron, loss_fn, X, y, lr = 0.01, epochs = 100)
              

neuron = Neuron(X.shape[1])
criterion = SquaredLoss()
lr = 0.01


for epoch in range(100):
    for x_example, y_example in zip(X, y):
        y_pred = neuron(x_example)
        loss = criterion(y_pred, y_example)
        loss.backward()
        with torch.no_grad():
            neuron.weights -= lr * neuron.weights.grad
            neuron.bias -= lr * neuron.bias.grad
        neuron.weights.grad.zero_()
        neuron.bias.grad.zero_()
    if epoch % 10 == 0:
        print(f"Epoch: {epoch} | Loss: {loss.item()}")
              

neuron.weights, coef
''')
    elif st == '3.1.2':
        print('''
X = torch.linspace(0, 1, 100).view(-1, 1)
y = torch.sin(2 * np.pi * X) + 0.5 * torch.rand(X.size())
              

class Linear:
  def __init__(self, n_inputs: int, n_outputs: int):
    self.weights = torch.randn(n_inputs, n_outputs, requires_grad = True)
    self.bias = torch.randn(n_outputs, requires_grad = True)

  def __call__(self, inputs: torch.Tensor):
    return inputs @ self.weights + self.bias


class ReLU:
  def __call__(self, x: torch.Tensor):
    return torch.maximum(torch.zeros_like(x), x)


class MSE:
  def __call__(self, predictions: torch.Tensor, targets: torch.Tensor):
    return torch.mean((predictions - targets) ** 2)


class Model:
  def __init__(self):
    self.linear = Linear(n_inputs = 4, n_outputs = 1)
    self.relu = ReLU()

  def __call__(self, x: torch.Tensor):
    x = self.linear(x)
    return self.relu(x)
              


def train(model, loss_fn, X, y, lr = 0.01, epochs = 1000):
  losses = []

  for epoch in range(epochs):
    predictions = model(X)
    loss = loss_fn(predictions, y)
    losses.append(loss.item())



    loss.backward()


    with torch.no_grad():
      model.linear.weights -= lr * model.linear.weights.grad
      model.linear.bias -= lr * model.linear.bias.grad


      model.linear.weights.zero_()
      model.linear.bias.zero_()

    if epoch % 100 == 0:
      print(f'epoch {epoch}, loss {loss}')

  return losses
              


import matplotlib.pyplot as plt

def plot_loss(losses):
    plt.plot(losses)  # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ—Ç–µ—Ä—å
    plt.title('Loss vs Epochs')  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä–∞—Ñ–∏–∫–∞
    plt.xlabel('Epoch')  # –ú–µ—Ç–∫–∞ –æ—Å–∏ X
    plt.ylabel('Loss')  # –ú–µ—Ç–∫–∞ –æ—Å–∏ Y
    plt.grid()  # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Ç–∫–∏
    plt.show()  # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
              

def plot_predictions(X, y, model):
    with torch.no_grad():
        predictions = model(X)  # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

    plt.scatter(X[:, 0], y, label='True Values', color='blue')  # –ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    plt.scatter(X[:, 0], predictions, label='Predictions', color='red')  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    plt.legend()  # –õ–µ–≥–µ–Ω–¥–∞
    plt.title('True Values vs Predictions')  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä–∞—Ñ–∏–∫–∞
    plt.xlabel('Input Feature')  # –ú–µ—Ç–∫–∞ –æ—Å–∏ X
    plt.ylabel('Output Value')  # –ú–µ—Ç–∫–∞ –æ—Å–∏ Y
    plt.grid()  # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Ç–∫–∏
    plt.show()  # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
              


from sklearn.datasets import make_regression


X, y, coef = make_regression(n_features=4, n_informative=4, noise=10, coef=True, bias=0.5)


X = torch.from_numpy(X).to(dtype=torch.float32)
y = torch.from_numpy(y).to(dtype=torch.float32).view(-1, 1)


model = Model()
loss_fn = MSE()


losses = train(model, loss_fn, X, y, lr=0.01, epochs=1000)


plot_loss(losses)
plot_predictions(X, y, model)
              


class Linear():
    def __init__(self,n_f,n_n):
        self.weights = torch.randn(n_f,n_n, requires_grad=True)
        self.bias = torch.randn(1, n_n, requires_grad=True)
    def __call__(self, inputs: torch.Tensor):
        output = inputs @ self.weights + self.bias
        return output
class MSELoss:
    def __call__(self,
                 y_pred: torch.Tensor,
                 y_true: torch.Tensor):
        return torch.mean((y_true - y_pred) ** 2)
class Relu:
    def __call__(self,x):
        x[x<0]=0
        return x
              


import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
X = torch.linspace(0, 1, 100).view(-1, 1)  # 100 –ø—Ä–∏–º–µ—Ä–æ–≤, 1 –ø—Ä–∏–∑–Ω–∞–∫
y = torch.sin(2 * np.pi * X) + 0.1 * torch.randn(X.size())  # –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º —à—É–º–∞

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
class RegressionModel(nn.Module):
    def __init__(self):
        super(RegressionModel, self).__init__()
        self.fc1 = nn.Linear(1, 20)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(20, 1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏, —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
model = RegressionModel()
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ—Ç–µ—Ä—å
losses = []
epochs = 2000  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
for epoch in range(epochs):
    model.train()  # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
    optimizer.zero_grad()  # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤

    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    y_pred = model(X)

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
    loss = criterion(y_pred, y)
    losses.append(loss.item())  # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å

    # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:  # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫–∞–∂–¥—ã–µ 100 —ç–ø–æ—Ö
        print(f'Epoch {epoch}, Loss: {loss.item()}')

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ—Ç–µ—Ä—å
plt.figure(figsize=(10, 5))
plt.plot(losses, label='Training Loss')
plt.title('Loss per Epoch')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
model.eval()  # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
with torch.no_grad():
    y_pred = model(X)

plt.figure(figsize=(10, 5))
plt.scatter(X.numpy(), y.numpy(), label='True Data', color='blue')
plt.scatter(X.numpy(), y_pred.numpy(), label='Predicted Data', color='red')
plt.title('True vs Predicted Data')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()
    ''')
    elif st == '3.2':
        print('''
import torch.optim as optim
import torch.nn as nn
              
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
X = torch.randn(100, 4)  # 100 –ø—Ä–∏–º–µ—Ä–æ–≤ —Å 4 –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
y = torch.randn(100, 1)  # 100 –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å
model = nn.Sequential(
    nn.Linear(4, 20),  # –ò–∑–º–µ–Ω–∏–ª–∏ –Ω–∞ 4, —Ç–∞–∫ –∫–∞–∫ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç 4 –ø—Ä–∏–∑–Ω–∞–∫–∞
    nn.ReLU(),
    nn.Linear(20, 10),
    nn.ReLU(),
    nn.Linear(10, 1)
)

# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
optim = torch.optim.SGD(model.parameters(), lr=0.01)
criterion = nn.MSELoss()

# –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ç–µ—Ä—å
los = []

# –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
for epoch in range(20000):
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)
    y_pred = model(X)

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
    loss = criterion(y_pred, y)

    # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
    loss.backward()

    # –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    optim.step()

    # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    optim.zero_grad()

    # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –ø–æ—Ç–µ—Ä–∏ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
    los.append(loss.item())

    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∫–∞–∂–¥—ã–µ 100 —ç–ø–æ—Ö
    if epoch % 100 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item()}')

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
plt.plot(los)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss over Time')
plt.show()

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
with torch.no_grad():
    predicted = model(X)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
plt.scatter(X[:, 0].numpy(), y.numpy(), label='True Data')
plt.scatter(X[:, 0].numpy(), predicted.numpy(), label='Predictions', color='red')
plt.xlabel('X')
plt.ylabel('y')
plt.title('True Data and Model Predictions')
plt.legend()
plt.show()
''')
    elif st == '3.2.1':
        print('''
class Neuron:


  def __init__(self, n_inputs: int):
    self.weights = torch.randn(n_inputs, 1, requires_grad = True)
    self.bias = torch.randn(1, 1, requires_grad = True)

  def __call__(self, inputs: torch.Tensor):
    output = inputs @ self.weights + self.bias
    return output


class SquaredLoss:

  def __call__(self, y_pred: torch.Tensor, y_true: torch.Tensor):
    return ((y_pred - y_true) ** 2).mean()
              

X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)
X = torch.from_numpy(X).to(dtype=torch.float32)
y = torch.from_numpy(y).to(dtype=torch.float32).view(-1, 1)
              

def train(model, loss_fn, X, y, epochs= 10000, lr = 0.01):
  optimizer = torch.optim.SGD([model.weights, model.bias], lr = lr)


  for epoch in range(epochs):

    pred = model(X)

    loss = loss_fn(pred, y)

    loss.backward()

    optimizer.step()

    optimizer.zero_grad()

    if epoch % 100 == 0:
      print(f'epoch {epoch}, loss {loss}')
              

neuron = Neuron(n_inputs = X.shape[1])
loss_fn = SquaredLoss()

train(neuron, loss_fn, X, y, epochs = 1000, lr = 0.01)
    ''')
    elif st == '3.2.2':
        print('''
X, y, coef = make_regression(n_features=1, n_informative=1, noise=10, coef=True, bias=0.5)
X = torch.from_numpy(X).float()
y = torch.from_numpy(y).float().view(-1, 1)
              
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt


class RegressionModel(nn.Module):
    def __init__(self):
        super(RegressionModel, self).__init__()
        self.fc1 = nn.Linear(1, 20)
        self.fc2 = nn.Linear(20, 10)
        self.fc3 = nn.Linear(10, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x


model = RegressionModel()


criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)



losses = []

for epoch in range(1000):
    y_pred = model(X)

    loss = criterion(y_pred, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()



    losses.append(loss.item())

    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")


plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss over Time')
plt.show()


with torch.no_grad():
    predicted = model(X)

plt.scatter(X.numpy(), y.numpy(), label='True Data')
plt.plot(X.numpy(), predicted.numpy(), label='Predictions', color='red')
plt.xlabel('X')
plt.ylabel('y')
plt.title('True Data and Model Predictions')
plt.legend()
plt.show()
    ''')
    elif st == '3.3.1':
        print('''
import torch.nn as nn              

class mymodel(nn.Module):
    def __init__(self,n_f):
        super(mymodel,self).__init__()
        self.fc1= nn.Linear(n_f,20)
        self.fc12= nn.Linear(20,30)
        self.fc2= nn.Linear(30,1)
        self.relu = nn.ReLU()
    def __call__(self,x):
        return self.fc2(self.relu(self.fc12(self.relu(self.fc1(x)))))
              
X = torch.linspace(0, 1, 100).view(-1, 1)
y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size())
              
import torch.optim as optim
              

# my= mymodel(1)
my = nn.Sequential(
                nn.Linear(X.shape[1], 20),
                nn.ReLU(),
                nn.Linear(20, 30),
                nn.ReLU(),
                nn.Linear(30, 1))
opt= optim.SGD(my.parameters(),lr=0.01)
criterion = nn.MSELoss()
los = []

for epoch in range(20000):
        y_pred = my(X)
        loss = criterion(y_pred, y)
        loss.backward()
        opt.step()
        opt.zero_grad()
        los.append(loss.item())
        if epoch % 1000 == 0:
            print(f"Epoch: {epoch} | Loss: {loss.item()}")
              

plt.plot(X.detach(),y_pred.detach())
plt.scatter(X, y,color="r");
    ''')
    elif st == '3.3.2':
        print('''
X = torch.linspace(0, 1, 100).view(-1, 1)
y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size())
              

model = nn.Sequential(
    nn.Linear(1, 20),
    nn.ReLU(),
    nn.Linear(20, 10),
    nn.ReLU(),
    nn.Linear(10, 1)
)


criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr = 0.01)


losses = []
epochs = 2000


for epoch in range(epochs):
  y_pred = model(X)

  loss = criterion(y_pred, y)

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()


  losses.append(loss.item())

if epoch % 100 == 0:
  print(f'epoch {epoch}, loss {loss}')



plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss over time')
plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
with torch.no_grad():  # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    predicted = model(X)

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
plt.scatter(X.numpy(), y.numpy(), label='True Data')
plt.plot(X.numpy(), predicted.numpy(), label='Predictions', color='red')
plt.xlabel('X')
plt.ylabel('y')
plt.title('True Data and Model Predictions')
plt.legend()
plt.show()
    ''')
    elif st == '3.4':
        print('''
from torch.utils.data import Dataset, DataLoader, TensorDataset
import numpy as np
              
X = torch.linspace(0, 1, 100).view(-1, 1)
y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size())

dataset = TensorDataset(X, y)
dataloader= DataLoader(dataset,20,shuffle=True)
              

my = nn.Sequential(
                nn.Linear(X.shape[1], 20),
                nn.ReLU(),
                nn.Linear(20, 30),
                nn.ReLU(),
                nn.Linear(30, 1))
opt= optim.SGD(my.parameters(),lr=0.01)
criterion = nn.MSELoss()
los = []

for epoch in range(1000):
        for i, (x, y_) in enumerate(dataloader):
            y_pred = my(x)
            loss = criterion(y_pred, y_)
            loss.backward()
            opt.step()
            opt.zero_grad()
            los.append(loss.item())
            print(f"epoch {epoch}--Batch {i}: Loss: {loss.item()}")
            # if epoch % 1000 == 0:
            #     print(f"Epoch: {epoch} | Loss: {loss.item()}")
''')
    elif st == '3.4.1':
        print('''
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt


class SinusoidalDataset(Dataset):
    def __init__(self):
        self.X = torch.linspace(0, 1, 100).view(-1, 1)
        self.y = torch.sin(2 * np.pi * self.X) + 0.1 * torch.rand(self.X.size())

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]



batch_size = 10
dataset = SinusoidalDataset()
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)



model = nn.Sequential(
    nn.Linear(1, 20),
    nn.ReLU(),
    nn.Linear(20, 10),
    nn.ReLU(),
    nn.Linear(10, 1)
)


criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)



losses = []
epochs = 2000


for epoch in range(epochs):
    epoch_loss = 0.0  #–¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—â–µ–π –ø–æ—Ç–µ—Ä–∏ –∑–∞ —ç–ø–æ—Ö—É

    for batch_X, batch_y in dataloader:
        y_pred = model(batch_X)


        loss = criterion(y_pred, batch_y)


        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


        epoch_loss += loss.item() #–ø–æ—Ç–µ—Ä–∏ –∑–∞ –∫–∞–∂–¥—ã–π –±–∞—Ç—á


    epoch_loss / len(dataloader)
    losses.append(epoch_loss)


    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')


plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss over epochs')
plt.show()


with torch.no_grad():
    predicted = model(dataset.X)


plt.scatter(dataset.X.numpy(), dataset.y.numpy(), label='True Data')
plt.plot(dataset.X.numpy(), predicted.numpy(), label='Predictions', color='red')
plt.xlabel('X')
plt.ylabel('y')
plt.title('True Data and Model Predictions')
plt.legend()
plt.show()
    ''')
    elif st == '3.4.2.1':
        print('''
import pandas as pd
import torch
              
df = pd.read_csv('/content/diamonds.csv')
df
              
df_num = df.select_dtypes(['float64', 'int64'])
df_num
              
y = df_num['price'].values
y
              

X = df_num.drop(columns = ['price']).values
X
              

X_tens = torch.tensor(X, dtype = torch.float32)
y_tens = torch.tensor(y, dtype = torch.float32).view(-1, 1)
X_tens, y_tens
              

X_tens.size()
              

y_tens.size()
    ''')
    elif st == '3.4.2.2':
        print('''
from torch.utils.data import random_split
              
train_size = int(0.8 * len(df))
test_size = len(dataset) - train_size

train_df, test_df = random_split(dataset, [train_size, test_size])
    ''')
    elif st == '3.4.2.3':
        print('''
model = nn.Sequential(
    nn.Linear(1, 128),  # –ø–æ—á–µ–º—É x.shape[1] –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 1)
)




batch_size = 256
train_loader = DataLoader(train_df, batch_size = batch_size, shuffle = True)
test_loader = DataLoader(test_df, batch_size = batch_size, shuffle = False)


criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr = 0.01)


losses = []
epochs = 100


for epoch in range(epochs):
    running_loss = 0.0
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        y_pred = model(batch_X)
        loss = criterion(y_pred, batch_y)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)
    losses.append(avg_loss)

    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {avg_loss}')
    ''')
    elif st == '3.4.3':
        print('''
from torch.utils.data import Dataset


class DropColsTransform:
    def __call__(self, df):
        return df.select_dtypes(include=[float, int])


class ToTensorTransform:
    def __call__(self, df):
        return torch.tensor(df.values, dtype=torch.float32)


class MyDataset(Dataset):
    def __init__(self, df, transform=None):
        if transform:
            df = transform(df)
        self.df = df


    def __len__(self):
        return len(self.df)


    def __getitem__(self, idx):
        data = self.df.iloc[idx] #–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö

        X = data.drop('price').values
        y = data['price']

        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)


drop_transform = DropColsTransform()
to_tensor_transform = ToTensorTransform()
dataset = MyDataset(df, transform=drop_transform)



for idx in range(5):
    x, y = dataset[idx]
    print(f'Features: {x}, Target: {y}')

    ''')
    elif st == '3.4.2.4':
        print('''
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import TensorDataset

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö

dataset = df.select_dtypes(include=[np.number])  # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
print(f'Length of original dataset: {len(dataset)}')

X = dataset.drop('price', axis=1).values  # –í—Å–µ —Å—Ç–æ–ª–±—Ü—ã, –∫—Ä–æ–º–µ 'price'
y = dataset['price'].values  # –¶–µ–ª–µ–≤–æ–π —Å—Ç–æ–ª–±–µ—Ü

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â–∏–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä—ã
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# –°–æ–∑–¥–∞–Ω–∏–µ TensorDataset
train_df = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
test_df = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –æ–±—É—á–∞—é—â–µ–≥–æ –∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–æ–≤
print(f'Length of train dataset: {len(train_df)}')
print(f'Length of test dataset: {len(test_df)}')

              

print(f'Length of train dataset: {len(train_df)}')
print(f'Length of test dataset: {len(test_df)}')
              


from torch.utils.data import DataLoader

batch_size = 256
train_loader = DataLoader(train_df, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_df, batch_size=batch_size, shuffle=False)
              

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score  # –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è R^2

model = nn.Sequential(
    nn.Linear(X.shape[1], 128),  # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 1)
)

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

epochs = 100
train_losses = []
test_losses = []

for epoch in range(epochs):
    run_loss = 0.0
    model.train()

    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        y_pred = model(batch_X)
        loss = criterion(y_pred, batch_y)
        loss.backward()
        optimizer.step()

        run_loss += loss.item()

    average_train_loss = run_loss / len(train_loader)
    train_losses.append(average_train_loss)

    model.eval()
    test_loss = 0.0

    all_preds = []  # –°–ø–∏—Å–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
    all_targets = []  # –°–ø–∏—Å–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ

    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            y_pred = model(batch_X)
            loss = criterion(y_pred, batch_y)
            test_loss += loss.item()

            all_preds.append(y_pred)
            all_targets.append(batch_y)

    average_test_loss = test_loss / len(test_loader)
    test_losses.append(average_test_loss)

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Train Loss: {average_train_loss:.4f}, Test Loss: {average_test_loss:.4f}")

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.title('Train and Test Loss per Epoch')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# –†–∞—Å—Å—á–∏—Ç–∞–µ–º R^2
# –û–±—ä–µ–¥–∏–Ω–∏–º –≤—Å–µ –±–∞—Ç—á–∏ –≤ –æ–¥–∏–Ω —Ç–µ–Ω–∑–æ—Ä
all_preds = torch.cat(all_preds).cpu().numpy()  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
all_targets = torch.cat(all_targets).cpu().numpy()  # –ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

# –í—ã—á–∏—Å–ª—è–µ–º R^2
r2 = r2_score(all_targets, all_preds)
print(f'R^2 on test set: {r2:.4f}')

              


from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from torch.utils.data import DataLoader, TensorDataset

batch_size = 256
train_loader = DataLoader(train_df, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_df, batch_size=batch_size, shuffle=False)

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è X –∏ y
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))  # y –≤ —Ñ–æ—Ä–º–µ (n_samples, 1)

# –°–æ–∑–¥–∞–Ω–∏–µ TensorDataset –∏ DataLoader
train_dataset = TensorDataset(torch.tensor(X_scaled, dtype=torch.float32), torch.tensor(y_scaled, dtype=torch.float32))
test_dataset = TensorDataset(torch.tensor(X_scaled, dtype=torch.float32), torch.tensor(y_scaled, dtype=torch.float32))

train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

# –ú–æ–¥–µ–ª—å
model = nn.Sequential(
    nn.Linear(X_scaled.shape[1], 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 1)  # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –¥–æ–ª–∂–µ–Ω –≤—ã–¥–∞–≤–∞—Ç—å –æ–¥–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
)

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

epochs = 100
train_losses = []
test_losses = []

# –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏
for epoch in range(epochs):
    run_loss = 0.0
    model.train()

    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        y_pred = model(batch_X)
        y_pred = y_pred.view(-1)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        batch_y = batch_y.view(-1)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        loss = criterion(y_pred, batch_y)
        loss.backward()
        optimizer.step()
        run_loss += loss.item()

    average_train_loss = run_loss / len(train_loader)
    train_losses.append(average_train_loss)

    model.eval()
    test_loss = 0.0
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            y_pred = model(batch_X)
            y_pred = y_pred.view(-1)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
            batch_y = batch_y.view(-1)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
            loss = criterion(y_pred, batch_y)
            test_loss += loss.item()
            all_preds.append(y_pred)
            all_targets.append(batch_y)

    average_test_loss = test_loss / len(test_loader)
    test_losses.append(average_test_loss)

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Train Loss: {average_train_loss:.4f}, Test Loss: {average_test_loss:.4f}")

# –ì—Ä–∞—Ñ–∏–∫ –æ—à–∏–±–∫–∏ –ø–æ —ç–ø–æ—Ö–∞–º
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.title('Train and Test Loss per Epoch')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –æ–¥–∏–Ω –º–∞—Å—Å–∏–≤
all_preds = torch.cat(all_preds).cpu().numpy()
all_targets = torch.cat(all_targets).cpu().numpy()

# –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∏ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∏—Å—Ö–æ–¥–Ω—ã–π –º–∞—Å—à—Ç–∞–±
all_preds_rescaled = scaler_y.inverse_transform(all_preds.reshape(-1, 1))  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º—É (n_samples, 1)
all_targets_rescaled = scaler_y.inverse_transform(all_targets.reshape(-1, 1))


r2 = r2_score(all_targets_rescaled, all_preds_rescaled)
print(f'R^2 on test set: {r2:.4f}')
              


plt.figure(figsize=(10, 5))
plt.scatter(all_targets_rescaled, all_preds_rescaled, alpha=0.5, label='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
plt.plot([all_targets_rescaled.min(), all_targets_rescaled.max()],
         [all_targets_rescaled.min(), all_targets_rescaled.max()],
         'r--', linewidth=2, label='–ò–¥–µ–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (y=x)')
plt.title('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
plt.xlabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
plt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
plt.legend()
plt.show()
    ''')
        
def cnnClas_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.cnnClas_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")
          
1. –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç CatBreeds –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∞—Ä—Ö–∏–≤–∞ cat_breeds_4.zip. 
–ò—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è torchvision, –ø—Ä–∏–≤–µ–¥–∏—Ç–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∫ —Ä–∞–∑–º–µ—Ä—É 300—Ö300 –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–π—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏ –ø–∏–∫—Å–µ–ª–µ–π 
(—Ä–∞—Å—Å—á–∏—Ç–∞–π—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ç–¥–µ–ª—å–Ω–æ). 
–í—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, —Ä–∞–∑–º–µ—Ä –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤. 
–†–∞–∑–±–µ–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –æ–±—É—á–∞—é—â–µ–µ –∏ —Ç–µ—Å—Ç–æ–≤–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ 80 –Ω–∞ 20%.
          
2. –†–µ—à–∏—Ç–µ –∑–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–¥–∞–Ω–∏—è, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—è —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏. 
–ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–æ–º–µ—Ä–∞ —ç–ø–æ—Ö–∏, 
–≥—Ä–∞—Ñ–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ accuracy –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —ç–ø–æ—Ö–∏. 
–í—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –∏—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ accuracy –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ. –í—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.
          
3. –ù–∞–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–±–∏—Ä–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–≤–æ–¥–∏—Ç –∏—Ö –Ω–∞ —ç–∫—Ä–∞–Ω –≤ –≤–∏–¥–µ —Å–µ—Ç–∫–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –Ω–∞–¥ 
–Ω–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∏ –∫–ª–∞—Å—Å–∞, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –º–æ–¥–µ–ª—å—é. 
–í–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏—Å—å –¥–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π, –≤—ã–≤–µ–¥–∏—Ç–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –∏—Ç–æ–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏ –ø–æ 6 —Å–ª—É—á–∞–π–Ω—ã–º –∫–∞—Ä—Ç–∏–Ω–∫–∞–º.
          
4. –†–µ—à–∏—Ç–µ –∑–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏. 
–ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–æ–º–µ—Ä–∞ —ç–ø–æ—Ö–∏, –≥—Ä–∞—Ñ–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ 
accuracy –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —ç–ø–æ—Ö–∏. 
–í—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –∏—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ accuracy –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ. 
–í—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.
–í–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏—Å—å —Ñ—É–Ω–∫—Ü–∏–µ–π –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–¥–∞–Ω–∏—è, –≤—ã–≤–µ–¥–∏—Ç–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –∏—Ç–æ–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø–æ 6 —Å–ª—É—á–∞–π–Ω—ã–º –∫–∞—Ä—Ç–∏–Ω–∫–∞–º.
          
5. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –æ–±—É—á–µ–Ω–Ω—É—é –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–µ –º–æ–¥–µ–ª—å, –∏—Å—Å–ª–µ–¥–æ–≤–∞–≤ –æ–±—É—á–µ–Ω–Ω—ã–µ —è–¥—Ä–∞ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤. 
–í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ —á–µ—Ä–µ–∑ –ø–µ—Ä–≤—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π –º–æ–¥–µ–ª–∏. 
–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
''')
    
def cnnClas_code(st):
    if st == '1':
        print('''
!unzip cat_breeds_4.zip
              

zip_file_path = '/content/cat_breeds_4.zip'
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall()  # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤
              

transform = transforms.Compose([
    transforms.Resize((300, 300)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  #–∑–Ω–∞—á–µ–Ω–∏—è –ø–∏–∫—Å–µ–ª–µ–π [-1, 1]
])
              

dataset = ImageFolder(root = 'cat_breeds_4', transform = transform)
dataset
              

num_images = len(dataset)
num_images
              

image_size = dataset[0][0].size()
image_size
              

num_classes = len(dataset.classes) #–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–ø–æ–∫
num_classes
              

train_size = int(0.8 * num_images)
test_size = num_images - train_size
              
train_dataset, test_dataset = random_split(dataset, [train_size, test_size]) #—Ä–∞–Ω–¥–æ–º–Ω–æ –¥–µ–ª–∏–º –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ —Ç–µ—Å—Ç –∏ –æ–±—É—á
              
len(train_dataset)
              
len(test_dataset)
''')
    elif st == '2':
        print('''
train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)
test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False)
              
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
              

#–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ + –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
              
class CNN(nn.Module):
  def __init__(self, num_classes):
    super(CNN, self).__init__()

    self.flatten = nn.Flatten() #–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä

    self.fc1 = nn.Linear(3 * 300 * 300, 1024)

    self.bn1 = nn.BatchNorm1d(1024)

    self.fc2 = nn.Linear(1024, 256)

    self.bn2 = nn.BatchNorm1d(256)

    self.fc3 = nn.Linear(256, 64)

    self.dropout = nn.Dropout(0.5)

    self.fc4 = nn.Linear(64, num_classes)


  def forward(self, x):

    x = self.flatten(x)

    x = torch.relu(self.bn1(self.fc1(x)))

    x = torch.relu(self.bn2(self.fc2(x)))

    x = torch.relu(self.fc3(x))

    x = self.dropout(x)

    x = self.fc4(x)

    return x

num_classes = len(dataset.classes)
model = CNN(num_classes).to(device)
              
num_params = sum(p.numel() for p in model.parameters())
num_params
              

#–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
              
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr = 0.001)
              

def train(model, loader, optimizer, criterion):
    model.train()
    total_loss, correct = 0, 0

    for images, labels in loader:

        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()  #backpropogation –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã => c–±—Ä–∞—Å—ã–≤–∞–µ–º, —á—Ç–æ–± –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ —Å–º–µ—à–∏–≤–∞–ª–∏—Å—å –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏

        outputs = model(images)  #forward pass (predictions)

        loss = criterion(outputs, labels)  #–æ—Ç–ª–∏—á–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏ –æ—Ç –∏—Å—Ö–æ–¥–Ω—ã—Ö –º–µ—Ç–æ–∫

        loss.backward()

        optimizer.step()

        total_loss += loss.item()  #–ø–æ—Ç–µ—Ä–∏ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
        correct += (outputs.argmax(1) == labels).sum().item()

    avg_loss = total_loss / len(loader)
    accuracy = correct / len(loader.dataset)  #—Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ
    return avg_loss, accuracy


def evaluate(model, loader, criterion):
  model.eval()

  total_loss, correct = 0, 0

  with torch.no_grad():
    for images, labels in loader:

      images, labels = images.to(device), labels.to(device)

      outputs = model(images)

      loss = criterion(outputs, labels)

      total_loss += loss.item()
      correct += (outputs.argmax(1) == labels).sum().item() #–ø–æ–¥—Å—á–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

  avg_loss = total_loss / len(loader)
  accuracy = correct / len(loader.dataset)
  return avg_loss, accuracy
              
#with torch.no_grad() –æ—Ç–∫–ª—é—á–∞–µ—Ç –ø–æ–¥—Å—á–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ–±—ä–µ–º–∞ –ø–∞–º—è—Ç–∏
#train() - –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å = –û–ë–ù–û–í–õ–Ø–ï–¢ –í–ï–°–ê (—Ç—É—Ç –≤–∫–ª—é—á–∞–µ—Ç—Å—è dropout, –æ–Ω –æ–±–Ω—É–ª—è–µ—Ç —Ä–∞–Ω–¥–æ–º–Ω–æ —á–∞—Å—Ç—å –Ω–µ–π—Ä–æ–Ω–æ–≤)
#eval() - –æ—Ü–µ–Ω–∏–≤–∞–µ–º, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ(–æ—Ç–∫–ª—é—á–∞–µ—Ç dropout –¥–ª—è –±–æ–ª–µ–µ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤), –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å–∞
              
epochs = 10
train_losses, test_losses = [], []
train_accuracies, test_accuracies = [], []
for epoch in range(epochs):

    train_loss, train_acc = train(model, train_loader, optimizer, criterion)
    test_loss, test_acc = evaluate(model, test_loader, criterion)

    train_losses.append(train_loss)
    test_losses.append(test_loss)
    train_accuracies.append(train_acc)
    test_accuracies.append(test_acc)

    print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, "
          f"–ü–æ—Ç–µ—Ä—è: {train_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {train_acc:.4f}, "
          f"–¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {test_acc:.4f}")
''')
    elif st == '3':
        print('''
def show_examples(model, subset, original_dataset, k=6):
    model.eval()

    indices = np.random.choice(len(subset), k, replace=False)

    fig, axes = plt.subplots(1, k, figsize=(15, 5))

    with torch.no_grad():
        for i, idx in enumerate(indices):
            image, label = subset[idx]
            output = model(image.unsqueeze(0).to(device))
            predicted_label = output.argmax(1).item()

            #–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è Matplotlib (CHW -> HWC)
            axes[i].imshow(image.permute(1, 2, 0).numpy())
            axes[i].axis('off')

            true_class = original_dataset.classes[label]
            pred_class = original_dataset.classes[predicted_label]
            axes[i].set_title(f'True: {true_class}\nPred: {pred_class}', fontsize=10)

    plt.show()
              

show_examples(model, test_dataset, dataset, k=6)
''')
    elif st == '4':
        print('''
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
              
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])
              

dataset = ImageFolder(root='cat_breeds_4', transform=transform)
              

train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])
              

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
              

#–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏
              
class CNN(nn.Module):
    def __init__(self, num_classes):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(32 * 56 * 56, 64)
        self.fc2 = nn.Linear(64, num_classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.pool(x)


        x = x.view(x.size(0), -1) #flatten
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

#–Ω–∞ –≤—Ö–æ–¥–µ 224, –∑–∞—Ç–µ–º –¥–≤–∞ –ø—É–ª–∏–Ω–≥ —Å–ª–æ—è -> 56, out_channels = 16 , out_channels = 32
              

num_classes = len(dataset.classes)
model = CNN(num_classes)
              

optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()
              

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
              

#–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
              
def train(model, loader, optimizer, criterion):
    model.train()
    total_loss, correct = 0, 0
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()


        total_loss += loss.item()
        correct += (outputs.argmax(1) == labels).sum().item()

    avg_loss = total_loss / len(loader)
    accuracy = correct / len(loader.dataset)
    return avg_loss, accuracy

def evaluate(model, loader, criterion):
    model.eval()
    total_loss, correct = 0, 0
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            correct += (outputs.argmax(1) == labels).sum().item()

    avg_loss = total_loss / len(loader)
    accuracy = correct / len(loader.dataset)
    return avg_loss, accuracy
              

#–û–±—É—á–µ–Ω–∏–µ + –æ—Ü–µ–Ω–∫–∞
              
epochs = 10
train_losses, test_losses = [], []
train_accuracies, test_accuracies = [], []

for epoch in range(epochs):
    train_loss, train_acc = train(model, train_loader, optimizer, criterion)
    test_loss, test_acc = evaluate(model, test_loader, criterion)


    train_losses.append(train_loss)
    test_losses.append(test_loss)
    train_accuracies.append(train_acc)
    test_accuracies.append(test_acc)

    print(f"–≠–ø–æ—Ö–∞ {epoch + 1}/{epochs}, –ü–æ—Ç–µ—Ä—è: {train_loss:.4f}, "
          f"–¢–æ—á–Ω–æ—Å—Ç—å: {train_acc:.4f}, –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {test_acc:.4f}")
              

#–¢–æ—á–Ω–æ—Å—Ç—å + –≤—ã–≤–æ–¥
train_accuracies[-1]
              
test_accuracies[-1]
              
num_params = sum(p.numel() for p in model.parameters())
num_params
              

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
              
def show_examples(model, dataset, k=6):
    model.eval()
    indices = random.sample(range(len(dataset)), k)
    images, labels = zip(*[dataset[i] for i in indices])

    #–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä—ã
    images = torch.stack(images).to(device)

    with torch.no_grad():
        outputs = model(images)
        _, preds = torch.max(outputs, 1)


    plt.figure(figsize=(10, 10))
    for i in range(k):
        plt.subplot(3, 2, i + 1)
        plt.imshow(np.transpose(images[i].cpu().numpy(), (1, 2, 0)))
        plt.title(f'–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–ª–∞—Å—Å: {dataset.classes[labels[i]]}\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {dataset.classes[preds[i]]}')
        plt.axis('off')
    plt.show()
              
show_examples(model, dataset, k=6)
''')
    elif st == '5':
        print('''
model.to(device)
              
def visualize_feature_maps(model, image):
    image = image.unsqueeze(0).to(device)


    conv1 = model.conv1


    with torch.no_grad():
        feature_maps = conv1(image)


    feature_maps = feature_maps.cpu()


    num_feature_maps = feature_maps.shape[1]


    plt.figure(figsize=(15, 15))
    for i in range(num_feature_maps):
        plt.subplot(4, 4, i + 1)
        plt.imshow(feature_maps[0, i].numpy(), cmap='gray')
        plt.axis('off')
        plt.title(f'Feature Map {i+1}')
    plt.show()
              

image, label = test_dataset[random.randint(0, len(test_dataset) - 1)]  #—Å–ª—É—á–∞–π–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
visualize_feature_maps(model, image)
''')

def cnnPre_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.cnnPre_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")
          
1. –ò—Å–ø–æ–ª—å–∑—É—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –∏–∑ torchvision, c–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å vgg16 –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ IMAGENET1K_V1. 
–í—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω —Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã—Ö (requires_grad==True) –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.
          
2. –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç CatBreeds –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∞—Ä—Ö–∏–≤–∞ cat_breeds_4.zip. –†–∞–∑–±–µ–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –æ–±—É—á–∞—é—â–µ–µ –∏ —Ç–µ—Å—Ç–æ–≤–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ 80 –Ω–∞ 20%.
–ö –æ–±—É—á–∞—é—â–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –ø—Ä–∏–º–µ–Ω–∏—Ç–µ —Å–ª–µ–¥—É—é—â–µ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: –ø—Ä–∏–≤–µ–¥–∏—Ç–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∫ —Ä–∞–∑–º–µ—Ä—É 256x256, 
–∑–∞—Ç–µ–º –æ–±—Ä–µ–∂—å—Ç–µ –ø–æ —Ü–µ–Ω—Ç—Ä—É —Å —Ä–∞–∑–º–µ—Ä–æ–º 224—Ö224, –∑–∞—Ç–µ–º –ø–µ—Ä–µ–≤–µ–¥–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–π—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏ –ø–∏–∫—Å–µ–ª–µ–π 
(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)).
–ö —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ VGG16_Weights.IMAGENET1K_V1.transforms.
          
3. –ó–∞–º–æ—Ä–æ–∑—å—Ç–µ –≤—Å–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–¥–∞–Ω–∏—è. –ó–∞–º–µ–Ω–∏—Ç–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π Linear –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ –Ω–æ–≤—ã–π —Å–ª–æ–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∑–∞–¥–∞—á–µ. 
–ü–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è –≤—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã—Ö (requires_grad==True) –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏. 
–†–µ—à–∏—Ç–µ –∑–∞–¥–∞—á—É, –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å —Å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –∏ –∏–∑–º–µ–Ω–Ω–Ω—ã–º –ø–æ—Å–ª–µ–¥–Ω–∏–º —Å–ª–æ–µ–º.
–ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–æ–º–µ—Ä–∞ —ç–ø–æ—Ö–∏, 
–≥—Ä–∞—Ñ–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ accuracy –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —ç–ø–æ—Ö–∏. 
–í—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –∏—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ accuracy –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ.
          
4. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏, –∑–∞–º–æ—Ä–æ–∑–∏–≤ –≤—Å–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏, –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ (—Å–ª–æ–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–µ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–π—Ç–µ). 
–°—Ä–∞–≤–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏, –∞ —Ç–∞–∫–∂–µ –≤—Ä–µ–º—è, –∑–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. 
–ü–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ä–∞–±–æ—Ç—ã —Å–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å –∑–∞–Ω–æ–≤–æ.
          
5. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ 3, —Ä–∞—Å—à–∏—Ä–∏–≤ –æ–±—É—á–∞—é—â–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –ø–æ–º–æ—â–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –∏–∑ torchvision, –∏–∑–º–µ–Ω—è—é—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 
(–ø–æ–≤–æ—Ä–æ—Ç—ã, –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏ –ø–∏–∫—Å–µ–ª–µ–π, –æ–±—Ä–µ–∑–∞–Ω–∏–µ –∏ —Ç.–¥.). 
–ü—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –Ω–µ –¥–æ–ª–∂–Ω—ã. 
–†–µ—à–µ–Ω–∏–µ –æ —Ç–æ–º, —Å–∫–æ–ª—å–∫–æ –∏ –∫–∞–∫–∏—Ö —Å–ª–æ–µ–≤ –º–æ–¥–µ–ª–∏ –±—É–¥–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è, –ø—Ä–∏–º–∏—Ç–µ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. 
–ü–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ä–∞–±–æ—Ç—ã —Å–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å –∑–∞–Ω–æ–≤–æ.
''')
def cnnPre_code(st):
    print('''
import torch
from torchvision import models
import os
import zipfile
import torch
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, random_split
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import time
''')
    if st == '1':
        print('''
model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)
              
model
              
num_layers = len(list(model.features)) + len(list(model.classifier))
num_layers
              
num_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
num_train_params
''')
    elif st == '2':
        print('''
zip_file_path = '/content/cat_breeds_4.zip'
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall()
              
extracted_dir = '/content/cat_breeds_4'
              
train_transforms = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
])
              
test_transforms = models.VGG16_Weights.IMAGENET1K_V1.transforms()
              
data_dir = extracted_dir
              
#–≥–æ—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
full_dataset = datasets.ImageFolder(root = data_dir, transform = train_transforms)
              
train_size = int(0.8 * len(full_dataset))
test_size = len(full_dataset) - train_size
train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])
              
test_dataset.dataset.transforms = test_transforms #–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
              
train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)
test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False)
              
model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)
model = model.to(DEVICE)
#–∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ VGG16
              
len(train_dataset)
              
len(test_dataset)
''')
    elif st == '3':
        print('''
#–∑–∞–º–æ—Ä–æ–∑–∫–∞ –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
for param in model.parameters():
  param.requires_grad = False #–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è
              
num_classes = len(train_dataset.dataset.classes) #–º–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
model.classifier[6] = nn.Linear(4096, num_classes) #–º–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π —Å –Ω—É–∂–Ω—ã–º —á–∏—Å–ª–æ–º —ã—Ö–æ–¥–æ–≤
#4096 - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–µ
#VGG16 –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ ImageNet, –≥–¥–µ 1000 –∫–ª–∞—Å—Å–æ–≤, –ø–æ—ç—Ç–æ–º—É –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ ‚Äî nn.Linear(4096, 1000)
              
num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
num_trainable_params
              
model = model.to(DEVICE)
              
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.classifier[6].parameters(), lr = 0.001)
              
epochs = 10
train_loss_history = []
train_acc_history = []
test_acc_history = []

def calculate_accuracy(loader, model):
    correct = 0
    total = 0
    model.eval()
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total
              

train_loss_history = []
train_acc_history = []
test_acc_history = []

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct_train = 0
    total_train = 0

    for images, labels in train_loader:
        images, labels = images.to(DEVICE), labels.to(DEVICE)


        outputs = model(images) #forward
        loss = criterion(outputs, labels)


        optimizer.zero_grad() #backward
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()


    train_loss_history.append(running_loss / len(train_loader))
    train_acc = correct_train / total_train
    train_acc_history.append(train_acc)


    test_acc = calculate_accuracy(test_loader, model)
    test_acc_history.append(test_acc)

    print(f"–≠–ø–æ—Ö–∞ [{epoch + 1}/{epochs}], –ü–æ—Ç–µ—Ä–∏: {running_loss / len(train_loader):.4f}, –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {train_acc:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {test_acc:.4f}")

              
plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs + 1), train_loss_history, label="Train Loss")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.title("–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ")
plt.legend()
plt.show()
              

plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs + 1), train_acc_history, label="Train Accuracy")
plt.plot(range(1, epochs + 1), test_acc_history, label="Test Accuracy")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–¢–æ—á–Ω–æ—Å—Ç—å")
plt.title("–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ (accuracy) –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞—Ö")
plt.legend()
plt.show()
              
train_acc_history[-1]
              
test_acc_history[-1]
''')
    elif st == '4':
        print('''
from torchvision import models
vgg16 = models.vgg16(weights="IMAGENET1K_V1")
print(vgg16.features)
              
model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)
              
for name, param in model.features.named_parameters():
    if "28" not in name:
        param.requires_grad = False
              
for param in model.classifier.parameters(): #–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–±—É—á–∞–µ–º—ã–π
    param.requires_grad = True
              
num_classes = len(train_dataset.dataset.classes) #–ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –Ω–æ–≤—ã–π
model.classifier[6] = nn.Linear(4096, num_classes)
              
model = model.to(DEVICE)
              
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)
              
#–ü–æ–¥—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏
              
epochs = 10
train_loss_history = []
train_acc_history = []
test_acc_history = []

start_time = time.time()

def calculate_accuracy(loader, model):
    correct = 0
    total = 0
    model.eval()
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total
              
for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct_train = 0
    total_train = 0

    for images, labels in train_loader:
        images, labels = images.to(DEVICE), labels.to(DEVICE)


        outputs = model(images) #forward
        loss = criterion(outputs, labels)


        optimizer.zero_grad() #backward
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()


    train_loss_history.append(running_loss / len(train_loader))
    train_acc = correct_train / total_train
    train_acc_history.append(train_acc)


    test_acc = calculate_accuracy(test_loader, model)
    test_acc_history.append(test_acc)

    print(f"–≠–ø–æ—Ö–∞ [{epoch + 1}/{epochs}], –ü–æ—Ç–µ—Ä–∏: {running_loss / len(train_loader):.4f}, –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {train_acc:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {test_acc:.4f}")

#early stop

#–∑–∞ 10 —ç–ø–æ—Ö –Ω–µ –æ–±—É—á–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å
              
training_time = time.time() - start_time
training_time
              
final_train_acc = train_acc_history[-1]
final_test_acc = test_acc_history[-1]
final_train_acc
              

final_test_acc
              

plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs + 1), train_loss_history, label="Train Loss")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.title("–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ")
plt.legend()
plt.show()
              

plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs + 1), train_acc_history, label="Train Accuracy")
plt.plot(range(1, epochs + 1), test_acc_history, label="Test Accuracy")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–¢–æ—á–Ω–æ—Å—Ç—å")
plt.title("–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞—Ö")
plt.legend()
plt.show()
''')
    elif st == '5':
        print('''
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
              
train_transforms = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),    #—Å–ª—É—á–∞–π–Ω–æ–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏ –ø–∏–∫—Å–µ–ª–µ–π
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])


test_transforms = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
])
              
train_dataset = datasets.ImageFolder(root='/content/cat_breeds_4', transform=train_transforms)
test_dataset = datasets.ImageFolder(root='/content/cat_breeds_4', transform=test_transforms)


train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
              
model = models.vgg16(weights='IMAGENET1K_V1')


for param in model.features.parameters():
    param.requires_grad = False


num_classes = len(train_dataset.classes)
model.classifier[6] = nn.Linear(4096, num_classes)

model = model.to(DEVICE)
              
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
              

num_epochs = 10
train_loss_history = []
train_acc_history = []
test_acc_history = []

def calculate_accuracy(loader, model):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total
              

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_train = 0
    total_train = 0

    for images, labels in train_loader:
        images, labels = images.to(DEVICE), labels.to(DEVICE)

        outputs = model(images)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()

    train_loss_history.append(running_loss / len(train_loader))
    train_acc = correct_train / total_train
    train_acc_history.append(train_acc)

    test_acc = calculate_accuracy(test_loader, model)
    test_acc_history.append(test_acc)

    print(f"–≠–ø–æ—Ö–∞ [{epoch + 1}/{num_epochs}], –ü–æ—Ç–µ—Ä–∏: {running_loss / len(train_loader):.4f}, "
          f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {train_acc:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {test_acc:.4f}")
              

plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs + 1), train_loss_history, label="Train Loss")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.title("–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ")
plt.legend()
plt.show()
              

plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs + 1), train_acc_history, label="Train Accuracy")
plt.plot(range(1, num_epochs + 1), test_acc_history, label="Test Accuracy")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–¢–æ—á–Ω–æ—Å—Ç—å")
plt.title("–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ")
plt.legend()
plt.show()
''')
    else:
        print('no such index')

def cnn1D_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.cnn1D_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")
          
1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞ ts.csv. 
–ò—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å, —Å–æ—Å—Ç–æ—è—â—É—é –∏–∑ –æ–¥–Ω–æ–≥–æ –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è, —Ä–µ—à–∏—Ç–µ –∑–∞–¥–∞—á—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è  ùë¶ùë°  –ø–æ k –ø—Ä–µ–¥—ã–¥—É—â–∏–º —Ç–æ—á–∫–∞–º –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞  ùë•ùë°‚àíùëò...ùë•ùë°‚àí1 . 
–ò—Å—Å–ª–µ–¥—É–π—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è  ùëò‚àà[1,7] . –î–ª—è –∫–∞–∂–¥–æ–≥–æ  ùëò  –≤—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –∏—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –≤–µ—Å–∞ —è–¥—Ä–∞ —Å–≤–µ—Ä—Ç–∫–∏. 
–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –∏ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã.
          
2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª PV_Elec_Gas2.csv. –û–ø–∏—à–∏—Ç–µ –∫–ª–∞—Å—Å ElectricityDataset, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–±–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–∫–Ω–∞ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ö–µ–º–æ–π:
          
3. –†–µ—à–∏—Ç–µ –∑–∞–¥–∞—á—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–∞ Gas_mxm –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–æ–ª–±—Ü–æ–≤ cum_power –∏ Elec_kW —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–¥–Ω–æ–º–µ—Ä–Ω—ã—Ö —Å–≤–µ—Ä—Ç–æ–∫. 
–î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–∏–Ω–∏-–ø–∞–∫–µ—Ç–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DataLoader. 
–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ DataLoader –≤—ã –Ω–µ –º–æ–∂–µ—Ç–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ.
–ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–æ–º–µ—Ä–∞ —ç–ø–æ—Ö–∏. 
–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –Ω–∞ –æ–¥–Ω–æ–º –≥—Ä–∞—Ñ–∏–∫–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥.
''')
def cnn1D_code(st):
    print('''
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
''')
    if st == '1':
        print('''
data = pd.read_csv('ts.csv')
x_series = data['x'].values
y_series = data['y'].values
data
              
def create_dataset(x, y, k):
    X, Y = [], []
    for i in range(k, len(x)):
        X.append(x[i-k:i])
        Y.append(y[i])
    return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)
              
class ConvModel(nn.Module):
    def __init__(self, k):
        super(ConvModel, self).__init__()
        self.conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=k, padding=k-1)

    def forward(self, x):
        x = self.conv(x)
        return x[:, :, -1].view(-1)
              
learning_rate = 0.01
epochs = 100

losses = {}
kernels = {}

for k in range(1, 8):
    X, Y = create_dataset(x_series, y_series, k)
    X = X.unsqueeze(1)
    dataset = TensorDataset(X, Y)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

    model = ConvModel(k)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    for epoch in range(epochs):
        for X_batch, Y_batch in dataloader:
            optimizer.zero_grad()
            predictions = model(X_batch)
            loss = criterion(predictions, Y_batch)
            loss.backward()
            optimizer.step()

    losses[k] = loss.item()
    kernels[k] = model.conv.weight.detach().numpy()

    print(f"k={k}: –ò—Ç–æ–≥–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å = {loss.item():.4f}")
    print(f"–í–µ—Å —è–¥—Ä–∞ —Å–≤–µ—Ä—Ç–∫–∏ –¥–ª—è k={k}:\n{kernels[k]}\n")
              
plt.figure(figsize=(14, 7))
plt.plot(y_series, label="–ò—Å—Ö–æ–¥–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ y", color='blue')

best_k = min(losses, key=losses.get)
X_test, Y_test = create_dataset(x_series, y_series, best_k)
X_test = X_test.unsqueeze(1)
predicted = model(X_test).detach().numpy()

plt.plot(range(best_k, len(y_series)), predicted, label=f"–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–∏ k={best_k}", color='red')
plt.legend()
plt.xlabel("–í—Ä–µ–º—è")
plt.ylabel("–ó–Ω–∞—á–µ–Ω–∏–µ")
plt.title("–ò—Å—Ö–æ–¥–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã")
plt.show()
''')
    elif st == '2':
        print('''
from datetime import datetime
from torch.utils.data import Dataset, DataLoader


data = pd.read_csv('PV_Elec_Gas2.csv', parse_dates=['Unnamed: 0'])
data = data.rename(columns={'Unnamed: 0': 'Date'})

train_data = data[data['Date'].dt.year < 2019]
test_data = data[data['Date'].dt.year == 2019]

# –ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–∫–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
class ElectricityDataset(Dataset):
    def __init__(self, df, window_size):
        self.data = df
        self.window_size = window_size

    def __len__(self):
        return len(self.data) - self.window_size

    def __getitem__(self, idx):
        x = self.data.iloc[idx:idx + self.window_size][['cum_power', 'Elec_kW']].values
        y = self.data.iloc[idx + self.window_size]['Gas_mxm']
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

# –£—Å—Ç–Ω–æ–≤–∫–∞ –æ–∫–Ω–∞
window_size = 30  # –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å –ø–æ –∑–∞–¥–∞—á–µ
train_dataset = ElectricityDataset(train_data, window_size)
test_dataset = ElectricityDataset(test_data, window_size)
''')
    elif st == '3':
        print('''
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import ReduceLROnPlateau

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
window_size = 30
batch_size = 32
learning_rate = 0.001
num_epochs = 70

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Å–ª–æ—è–º–∏
class EnhancedConvModel(nn.Module):
    def __init__(self, window_size):
        super(EnhancedConvModel, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=2, out_channels=32, kernel_size=3)
        self.bn1 = nn.BatchNorm1d(32)
        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)
        self.bn2 = nn.BatchNorm1d(64)
        self.pool = nn.MaxPool1d(2)
        self.fc1 = nn.Linear(64 * ((window_size - 4) // 2), 64)  # –ü–æ–¥–±–∏—Ä–∞–µ–º —Ä–∞–∑–º–µ—Ä
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)

    def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = torch.relu(self.bn2(self.conv2(x)))
        x = self.pool(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x.view(-1)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
train_dataset = ElectricityDataset(train_data, window_size)
test_dataset = ElectricityDataset(test_data, window_size)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏, —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
model = EnhancedConvModel(window_size)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è learning rate
scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
train_losses = []
test_losses = []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for x_batch, y_batch in train_loader:
        x_batch = x_batch.permute(0, 2, 1)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–ª—è Conv1d
        optimizer.zero_grad()
        outputs = model(x_batch)
        loss = criterion(outputs.squeeze(), y_batch)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    train_losses.append(train_loss / len(train_loader))

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    model.eval()
    test_loss = 0.0
    with torch.no_grad():
        for x_batch, y_batch in test_loader:
            x_batch = x_batch.permute(0, 2, 1)
            outputs = model(x_batch)
            loss = criterion(outputs.squeeze(), y_batch)
            test_loss += loss.item()

    test_losses.append(test_loss / len(test_loader))

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ learning rate
    scheduler.step(test_loss)

    print(f'Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}')

# –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label="Train Loss")
plt.plot(test_losses, label="Test Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Loss over epochs")
plt.show()

# –ü—Ä–æ–≥–Ω–æ–∑ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
model.eval()
predictions = []
real_values = []

with torch.no_grad():
    for x_batch, y_batch in test_loader:
        x_batch = x_batch.permute(0, 2, 1)
        outputs = model(x_batch)
        predictions.extend(outputs.squeeze().tolist())
        real_values.extend(y_batch.tolist())

plt.figure(figsize=(12, 6))
plt.plot(real_values, label="Real Gas_mxm", color='blue')
plt.plot(predictions, label="Predicted Gas_mxm", color='red')
plt.xlabel("Time")
plt.ylabel("Gas_mxm")
plt.legend()
plt.title("Real vs Predicted Gas_mxm")
plt.show()
''')
    else:
        print('no such index')
def RL_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.RL_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")
          
1. –°–æ–∑–¥–∞–π—Ç–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ Blackjack-v1. –°—ã–≥—Ä–∞–π—Ç–µ N=10000 –∏–≥—Ä, –≤—ã–±–∏—Ä–∞—è –¥–µ–π—Å—Ç–≤–∏–µ —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º. –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –∏ –≤—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –¥–æ–ª—é –≤—ã–∏–≥—Ä–∞–Ω–Ω—ã—Ö –∏–≥—Ä.
          
2. –°–æ–∑–¥–∞–π—Ç–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ Blackjack-v1. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª–∏—Ç, –≤ —Å—Ä–µ–¥–Ω–µ–º, –≤—ã–∏–≥—Ä—ã–≤–∞—Ç—å —á–∞—â–µ, —á–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è. 
–†–µ–∞–ª–∏–∑—É–π—Ç–µ —ç—Ç—É —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏ —Å—ã–≥—Ä–∞–π—Ç–µ N=10000 –∏–≥—Ä, –≤—ã–±–∏—Ä–∞—è –¥–µ–π—Å—Ç–≤–∏–µ —Å–æ–≥–ª–∞—Å–Ω–æ —ç—Ç–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏. –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –∏ –≤—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –¥–æ–ª—é –≤—ã–∏–≥—Ä–∞–Ω–Ω—ã—Ö –∏–≥—Ä.
          
3. –°–æ–∑–¥–∞–π—Ç–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è –∏–≥—Ä—ã –≤ –∫—Ä–µ—Å—Ç–∏–∫–∏-–Ω–æ–ª–∏–∫–∏, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å gym.Env. –†–µ—à–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—Ç—å —Å–ª–µ–¥—É—é—â–∏–º —É—Å–ª–æ–≤–∏—è–º:
–¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è spaces.Box;
–¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è spaces.MultiDiscrete;
–∏–≥—Ä–∞ –ø—Ä–µ–∫—Ä–∞—â–∞–µ—Ç—Å—è, –µ—Å–ª–∏:
–Ω–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–¥–µ–ª–∞—Ç—å —Ö–æ–¥;
–∏–≥—Ä–æ–∫ –ø—ã—Ç–∞–µ—Ç—Å—è –æ—Ç–º–µ—Ç–∏—Ç—å —É–∂–µ –≤—ã–±—Ä–∞–Ω–Ω—É—é —è—á–µ–π–∫—É.
–ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ö–æ–¥–∞ –∏–≥—Ä–æ–∫ –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞–≥—Ä–∞–¥—É:
0, –µ—Å–ª–∏ –∏–≥—Ä–∞ –Ω–µ –∑–∞–∫–æ–Ω—á–µ–Ω–∞;
1, –µ—Å–ª–∏ –∏–≥—Ä–æ–∫ –≤—ã–∏–≥—Ä–∞–ª;
-1, –µ—Å–ª–∏ –∏–≥—Ä–æ–∫ –ø—Ä–æ–∏–≥—Ä–∞–ª.
—Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ –∏–≥—Ä–æ–∫–∞ (–º–∞—à–∏–Ω—ã) –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ.
–°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –º–∞—à–∏–Ω—ã —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –≤–Ω—É—Ç—Ä–∏ –Ω–µ–≥–æ. –°–¥–µ–ª–∞–π—Ç–µ –≤—Å–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –º–µ—Ç–æ–¥—ã –ø—Ä–∏–≤–∞—Ç–Ω—ã–º–∏ (–Ω–∞–∑–≤–∞–Ω–∏—è –≤—Å–µ—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å __), –ø–æ–¥—á–µ—Ä–∫–Ω—É–≤, —á—Ç–æ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∫ –Ω–∏–º –¥–æ—Å—Ç—É–ø–∞ –∏–∑–≤–Ω–µ.
–°—ã–≥—Ä–∞–π—Ç–µ –æ–¥–Ω—É –∏–≥—Ä—É, –≤—ã–±–∏—Ä–∞—è –¥–µ–π—Å—Ç–≤–∏—è —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º. –í—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ö–æ–¥–∞ –∏ –∏—Ç–æ–≥–æ–≤—É—é –Ω–∞–≥—Ä–∞–¥—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∑–∞ —Å–µ—Å—Å–∏—é.
          
4. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é (–≤ –≤–∏–¥–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª–∏—Ç, 
–≤ —Å—Ä–µ–¥–Ω–µ–º, –≤—ã–∏–≥—Ä—ã–≤–∞—Ç—å –≤ –∫—Ä–µ—Å—Ç–∏–∫–∏-–Ω–æ–ª–∏–∫–∏ —á–∞—â–µ, —á–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è. 
–†–µ–∞–ª–∏–∑—É–π—Ç–µ —ç—Ç—É —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏ —Å—ã–≥—Ä–∞–π—Ç–µ –∏–≥—Ä—É, –≤—ã–±–∏—Ä–∞—è –¥–µ–π—Å—Ç–≤–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ —ç—Ç–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏. 
–í—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ö–æ–¥–∞ –∏ –∏—Ç–æ–≥–æ–≤—É—é –Ω–∞–≥—Ä–∞–¥—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∑–∞ —Å–µ—Å—Å–∏—é.
          
5. –°–æ–∑–¥–∞–π—Ç–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ MountainCar-v0. –ü—Ä–æ–∏–≥—Ä–∞–π—Ç–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –Ω–∞ –¥–∏—Å–∫ —Ñ–∞–π–ª —Å –∑–∞–ø–∏—Å—å—é –∫–∞–∂–¥–æ–≥–æ –ø—è—Ç–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞. 
–î–ª—è –∑–∞–ø–∏—Å–∏ –≤–∏–¥–µ–æ –≤–æ—Å–ø–æ–ª—å–∑—É–π—Ç–µ—Å—å –æ–±—ë—Ä—Ç–∫–æ–π RecordVideo. –í—Å—Ç–∞–≤—å—Ç–µ —Å–∫—Ä–∏–Ω—à–æ—Ç, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –≤–∏–¥–Ω–æ, —á—Ç–æ —Ñ–∞–π–ª—ã –±—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã.
''')
    
def RL_code(st):
    if st == '1':
        print('''
!pip install gymnasium
              
import gymnasium as gym #–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º
import numpy as np
import gym
from gym import spaces
              
#–∑–∞–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
#c–∏–º—É–ª—è—Ü–∏—è –∏–≥—Ä—ã, –≥–¥–µ –∞–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å–æ —Å—Ä–µ–¥–æ–π
#render_mode=None - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏–≥—Ä—ã –æ—Ç–∫–ª—é—á–µ–Ω–∞ ( –≤–∏–¥–∏–º —Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç )
env = gym.make("Blackjack-v1", render_mode=None)

N = 10000  #–∫–æ–ª-–≤–æ –∏–≥—Ä , 10—Ç—ã—Å , —á—Ç–æ–± —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ã–ª–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–º–∏
wins = 0   #—Å—á–µ—Ç—á–∏–∫ –≤—ã–∏–≥—Ä—ã—à–µ–π

# –ò–≥—Ä–æ–≤–æ–π —Ü–∏–∫–ª
for _ in range(N):
    observation, _ = env.reset()  #–æ–∫—Ä—É–∂–µ–Ω–∏–µ –≤ –Ω–∞—á —Å–æ—Å—Ç–æ—è–Ω–∏–µ ( —Ä–∞–∑–¥–∞–µ—Ç –∫–∞—Ä—Ç—ã )
    done = False #—Ñ–ª–∞–≥, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è true, –∫–æ–≥–¥–∞ –∏–≥—Ä–∞ –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è

    while not done:
        action = env.action_space.sample()  #—Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ: 0 - stay, 1 - –±–µ—Ä–µ–º –∫–∞—Ä—Ç—É
        observation, reward, done, _, _ = env.step(action)  #–≤—ã–ø –¥–µ–π—Å—Ç–≤–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç s , r

    if reward > 0:
        wins += 1


win_rate = wins / N
print(f"–î–æ–ª—è –≤—ã–∏–≥—Ä–∞–Ω–Ω—ã—Ö –∏–≥—Ä: {win_rate:.4f}")
env.close()
''')
    elif st == '2':
        print('''
env = gym.make("Blackjack-v1", render_mode=None)
N = 10000
wins = 0


def blackjack_strategy(player_sum, dealer_card, usable_ace):
  #player_sum ‚Äî —ç—Ç–æ —Å—É–º–º–∞ –∫–∞—Ä—Ç –∏–≥—Ä–æ–∫–∞
  #dealer_card ‚Äî —ç—Ç–æ –∫–∞—Ä—Ç–∞, –∫–æ—Ç–æ—Ä—É—é –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–∏–ª–µ—Ä
  #usable_ace ‚Äî —Ç—É–∑, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å –∫–∞–∫ 1 –∏–ª–∏ 11.

    if player_sum < 14:
        return 1  #–±–µ—Ä–µ–º
    else:
        return 0  #–æ—Å—Ç–∞–Ω–æ–≤–∫–∞


for _ in range(N):
    observation, _ = env.reset()  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    done = False

    while not done:
        player_sum, dealer_card, usable_ace = observation

        #—Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–µ—à–∞–µ—Ç: –≤–∑—è—Ç—å –∫–∞—Ä—Ç—É –∏–ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è
        action = blackjack_strategy(player_sum, dealer_card, usable_ace)


        #–≤—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –∏ –ø–æ–ª—É—á–∞–µ–º r, s', done ?
        observation, reward, done, _, _ = env.step(action)


    if reward > 0:
        wins += 1


win_rate = wins / N
print(f"–î–æ–ª—è –≤—ã–∏–≥—Ä–∞–Ω–Ω—ã—Ö –∏–≥—Ä (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è): {win_rate:.4f}")
env.close()
''')
    elif st =='3':
        print('''
class TicTacToeEnv(gym.Env):
  def __init__(self):
    super(TicTacToeEnv, self).__init__()

    #–ò–≥—Ä–æ–≤–æ–µ –ø–æ–ª–µ: 0 - –ø—É—Å—Ç–æ, 1 - –∏–≥—Ä–æ–∫, 2 - –º–∞—à–∏–Ω–∞
    self.__state = np.zeros((3,3), dtype=int)
    self.__current_player = 1 #–ø–µ—Ä–≤—ã–º —Ö–æ–¥–∏—Ç –∏–≥—Ä–æ–∫

    #–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π
    self.observation_space = spaces.Box(low=0, high=2, shape=(3,3), dtype=int) #–≤—Å–µ–≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ –ø–æ–ª–µ
    self.action_space = spaces.MultiDiscrete([3,3]) #–∏–≥—Ä–æ–∫ –º–æ–∂–µ—Ç –≤—ã–±–∏—Ä–∞—Ç—å –ª—é–±—É—é –∫–ª–µ—Ç–∫—É –Ω–∞ –ø–æ–ª–µ

  #–æ–±–Ω–æ–≤–ª—è–µ–º –∏–≥—Ä—É
  def reset(self):
    self.__state = np.zeros((3,3), dtype=int) #–∏–≥—Ä–æ–≤–æ–µ –ø–æ–ª–µ —Å–Ω–æ–≤–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ø—É—Å—Ç—ã–º
    self.__current_player = 1 #–∏–≥—Ä–æ–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç –ø–µ—Ä–≤—ã–º
    return self.__state


  #–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è
  def __is_winner(self, player):
    for i in range(3):
      #–ø—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–Ω—è—Ç–∞ –ª–∏ –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –∏ —Å—Ç–æ–ª–±–µ—Ü –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫—Ä–µ—Å—Ç–∏–∫–∞–º–∏ (1) –∏–ª–∏ –Ω–æ–ª–∏–∫–∞–º–∏ (2).
      if np.all(self.__state[i, :] == player) or np.all(self.__state[:, i] == player):
        return True

      #–ø—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–Ω—è—Ç—ã –ª–∏ –¥–∏–∞–≥–æ–Ω–∞–ª–∏
    if np.all(np.diag(self.__state) == player) or np.all(np.diag(np.fliplr(self.__state)) == player):
      return True
    return False

  #–°—Ç—Ä–∞—Ç–µ–Ω–∏—è –º–∞—à–∏–Ω—ã
  #–ú–∞—à–∏–Ω–∞ —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ –æ–Ω–∞ –≤—ã–∏–≥—Ä–∞—Ç—å.
  #–ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ—Ç –≤—ã–∏–≥—Ä–∞—Ç—å, –æ–Ω–∞ –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–æ–±–µ–¥—É –∏–≥—Ä–æ–∫–∞.
  #–ï—Å–ª–∏ –∏ —ç—Ç–æ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è, –æ–Ω–∞ –∑–∞–Ω–∏–º–∞–µ—Ç —Ü–µ–Ω—Ç—Ä.
  #–ï—Å–ª–∏ —Ü–µ–Ω—Ç—Ä –∑–∞–Ω—è—Ç, –º–∞—à–∏–Ω–∞ –≤—ã–±–∏—Ä–∞–µ—Ç —É–≥–æ–ª.
  #–ï—Å–ª–∏ —É–≥–ª—ã —Ç–æ–∂–µ –∑–∞–Ω—è—Ç—ã, –º–∞—à–∏–Ω–∞ —Ö–æ–¥–∏—Ç –≤ –ø–µ—Ä–≤—É—é —Å–≤–æ–±–æ–¥–Ω—É—é –∫–ª–µ—Ç–∫—É.

  def __machine_move(self):
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è –º–∞—à–∏–Ω—ã: –ø–æ–±–µ–∂–¥–∞–µ—Ç, –±–ª–æ–∫–∏—Ä—É–µ—Ç, –∏–Ω–∞—á–µ —Å–ª—É—á–∞–π–Ω—ã–π —Ö–æ–¥.
        """
        # 1. –ü–æ–±–µ–¥–Ω—ã–π —Ö–æ–¥ –¥–ª—è –º–∞—à–∏–Ω—ã
        for i in range(3):
            for j in range(3):
                if self.__state[i, j] == 0: #—Å–≤–æ–±–æ–¥–Ω–∞ –ª–∏ –∫–ª–µ—Ç–∫–∞
                    self.__state[i, j] = 2
                    if self.__is_winner(2): #–≤—ã–∏–≥—Ä–∞–µ—Ç –ª–∏ –º–∞—à–∏–Ω–∞
                        return #–µ—Å–ª–∏ –ø–æ–±–µ–¥–Ω—ã–π —Ö–æ–¥ , —Ç–æ –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º
                    self.__state[i, j] = 0

        # 2. –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Ö–æ–¥–∞ –∏–≥—Ä–æ–∫–∞
        for i in range(3):
            for j in range(3):
                if self.__state[i, j] == 0:
                    self.__state[i, j] = 1 #—Å—Ç–∞–≤–∏–º –•
                    if self.__is_winner(1):
                        self.__state[i, j] = 2 #–µ—Å–ª–∏ –∏–≥—Ä–æ–∫ –º–æ–∂–µ—Ç –≤—ã–∏–≥—Ä–∞—Ç—å —Å—Ç–∞–≤–∏–º 0 –∏ –±–ª–æ–∫–∏—Ä—É–µ–º
                        return
                    self.__state[i, j] = 0

        # 3. –í—ã–±–æ—Ä —Ü–µ–Ω—Ç—Ä–∞
        if self.__state[1, 1] == 0: #—Å–≤–æ–±–æ–¥–µ–Ω –ª–∏ —Ü–µ–Ω—Ç—Ä
            self.__state[1, 1] = 2
            return

        # 4. –í—ã–±–æ—Ä —É–≥–ª–æ–≤
        for i, j in [(0, 0), (0, 2), (2, 0), (2, 2)]: #–ø–µ—Ä–µ–±–∏—Ä–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —É–≥–ª–æ–≤
            if self.__state[i, j] == 0:
                self.__state[i, j] = 2
                return

        # 5. –õ—é–±–∞—è –¥–æ—Å—Ç—É–ø–Ω–∞—è –∫–ª–µ—Ç–∫–∞
        for i in range(3):
            for j in range(3):
                if self.__state[i, j] == 0:
                    self.__state[i, j] = 2
                    return

  def step(self, action):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ö–æ–¥ –∏–≥—Ä–æ–∫–∞ –∏ –º–∞—à–∏–Ω—ã.
        """
        row, col = action
        #action ‚Äî —ç—Ç–æ —Ö–æ–¥ –∏–≥—Ä–æ–∫–∞

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ö–æ–¥–∞
        #–ï—Å–ª–∏ –∫–ª–µ—Ç–∫–∞ —É–∂–µ –∑–∞–Ω—è—Ç–∞, –∏–≥—Ä–æ–∫ –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞–∫–∞–∑–∞–Ω–∏–µ -1, –∏ –∏–≥—Ä–∞ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è.
        if self.__state[row, col] != 0:
            return self.__state, -1, True, {}  # –ù–∞–∫–∞–∑–∞–Ω–∏–µ –∑–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ö–æ–¥

        # –ò–≥—Ä–æ–∫ –¥–µ–ª–∞–µ—Ç —Ö–æ–¥
        #–ò–≥—Ä–æ–∫ –¥–µ–ª–∞–µ—Ç —Ö–æ–¥ (—Å—Ç–∞–≤–∏—Ç 1 –≤ –≤—ã–±—Ä–∞–Ω–Ω—É—é –∫–ª–µ—Ç–∫—É).
        self.__state[row, col] = 1

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–±–µ–¥—É –∏–≥—Ä–æ–∫–∞
        if self.__is_winner(1): #–ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä–æ–∫–∏, —Å—Ç–æ–ª–±—Ü—ã –∏ –¥–∏–∞–≥–æ–Ω–∞–ª–∏ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            return self.__state, 1, True, {}  # –ò–≥—Ä–æ–∫ –ø–æ–±–µ–¥–∏–ª

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∏—á—å—é
        if not np.any(self.__state == 0): #–µ—Å–ª–∏ –Ω–∞ –¥–æ—Å–∫–µ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å 0
            return self.__state, 0, True, {}  # –ù–∏—á—å—è

        # –ú–∞—à–∏–Ω–∞ –¥–µ–ª–∞–µ—Ç —Ö–æ–¥
        self.__machine_move()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–±–µ–¥—É –º–∞—à–∏–Ω—ã
        if self.__is_winner(2):
            return self.__state, -1, True, {}  # –ò–≥—Ä–æ–∫ –ø—Ä–æ–∏–≥—Ä–∞–ª

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∏—á—å—é
        if not np.any(self.__state == 0):
            return self.__state, 0, True, {}

        #–Ω–∏ –æ–¥–Ω–∞ –∏–∑ –ø—Ä–æ–≤–µ—Ä–æ–∫ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞ -> –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏–≥—Ä—É
        return self.__state, 0, False, {}

  def render(self):
        """
        –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–≥—Ä—ã.
        """
        symbols = {0: ".", 1: "X", 2: "O"}
        for row in self.__state:
            print(" ".join(symbols[cell] for cell in row))
        print()
              

# –ò–≥—Ä–æ–≤–æ–π —Ü–∏–∫–ª
env = TicTacToeEnv()
state = env.reset() #—Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∏–≥—Ä–æ–≤–æ–µ –ø–æ–ª–µ
done = False
total_reward = 0

print("–ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:")
env.render()

while not done:
    # –•–æ–¥ –∏–≥—Ä–æ–∫–∞ (–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä: –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—É—é —Å–≤–æ–±–æ–¥–Ω—É—é –∫–ª–µ—Ç–∫—É)
    action = None
    for i in range(3):
        for j in range(3):
            if state[i, j] == 0:
                action = (i, j)
                break
        if action:
            break

    state, reward, done, _ = env.step(action)

    print("–¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:")
    env.render()
    total_reward = reward

# –ò—Ç–æ–≥–æ–≤—ã–π –≤—ã–≤–æ–¥
if total_reward == 1:
    print("–ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º! –í—ã –ø–æ–±–µ–¥–∏–ª–∏!")
    print('reward = 1')
elif total_reward == -1:
    print("–ú–∞—à–∏–Ω–∞ –ø–æ–±–µ–¥–∏–ª–∞. –£–¥–∞—á–∏ –≤ —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑!")
    print('reward = -1')
else:
    print("–ò–≥—Ä–∞ –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å –≤–Ω–∏—á—å—é.")
    print('reward = 0')
''')
    elif st == '4':
        print('''
class TicTacToeEnv(gym.Env):
    """
    –û–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è –∏–≥—Ä—ã –≤ –∫—Ä–µ—Å—Ç–∏–∫–∏-–Ω–æ–ª–∏–∫–∏.
    """
    def __init__(self):
        super(TicTacToeEnv, self).__init__()
        self.__state = np.zeros((3, 3), dtype=int)  # –ü–æ–ª–µ 3x3: 0 - –ø—É—Å—Ç–æ, 1 - –∏–≥—Ä–æ–∫, 2 - –º–∞—à–∏–Ω–∞
        self.__current_player = 1  # –ò–≥—Ä–æ–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç –ø–µ—Ä–≤—ã–º

        self.observation_space = spaces.Box(low=0, high=2, shape=(3, 3), dtype=int) #–≤—Å–µ–≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ –ø–æ–ª–µ
        self.action_space = spaces.MultiDiscrete([3, 3]) #–∏–≥—Ä–æ–∫ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—é–±–æ–µ –º–µ—Å—Ç–æ –¥–ª—è —Ö–æ–¥–∞

    def reset(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–≥—Ä—ã."""
        self.__state = np.zeros((3, 3), dtype=int)
        self.__current_player = 1
        return self.__state

    def is_winner(self, player):  # –ü—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–±–µ–¥—ã –∏–≥—Ä–æ–∫–∞."""
        for i in range(3):
            if np.all(self.__state[i, :] == player) or np.all(self.__state[:, i] == player):
                return True
        if np.all(np.diag(self.__state) == player) or np.all(np.diag(np.fliplr(self.__state)) == player):
            return True
        return False

    def __machine_move(self):
        """–°—Ç—Ä–∞—Ç–µ–≥–∏—è –º–∞—à–∏–Ω—ã: –ø–æ–±–µ–¥–∞ -> –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ -> –ø–µ—Ä–≤—ã–π —Å–≤–æ–±–æ–¥–Ω—ã–π —Ö–æ–¥."""
        for i in range(3):
            for j in range(3):
                if self.__state[i, j] == 0:
                    # –ü–æ–±–µ–¥–Ω—ã–π —Ö–æ–¥
                    self.__state[i, j] = 2
                    if self.is_winner(2):
                        return
                    self.__state[i, j] = 0

        for i in range(3):
            for j in range(3):
                if self.__state[i, j] == 0:
                    # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∏–≥—Ä–æ–∫–∞
                    self.__state[i, j] = 1
                    if self.is_winner(1):
                        self.__state[i, j] = 2
                        return
                    self.__state[i, j] = 0

        # –õ—é–±–æ–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Ö–æ–¥
        #–µ—Å–ª–∏ –Ω–µ—Ç –ø–æ–±–µ–¥–Ω—ã—Ö –∏–ª–∏ –±–ª–æ–∫–∏—Ä—É—é—â–∏—Ö —Ö–æ–¥–æ–≤ , –º–∞—à–∏–Ω–∞ –∑–∞–Ω–∏–º–∞–µ—Ç –ª—é–±—É—é –∫–ª–µ—Ç–∫—É
        for i in range(3):
            for j in range(3):
                if self.__state[i, j] == 0:
                    self.__state[i, j] = 2
                    return

    def step(self, action):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —à–∞–≥ –∏–≥—Ä–æ–∫–∞ –∏ –º–∞—à–∏–Ω—ã."""
        row, col = action
        if self.__state[row, col] != 0:
            return self.__state, -1, True, {}  # –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ö–æ–¥: –ø—Ä–æ–∏–≥—Ä—ã—à

        self.__state[row, col] = 1  # –•–æ–¥ –∏–≥—Ä–æ–∫–∞
        if self.is_winner(1):
            return self.__state, 1, True, {}  # –ü–æ–±–µ–¥–∞ –∏–≥—Ä–æ–∫–∞
        if not np.any(self.__state == 0):
            return self.__state, 0, True, {}  # –ù–∏—á—å—è

        self.__machine_move()  # –•–æ–¥ –º–∞—à–∏–Ω—ã
        if self.is_winner(2):
            return self.__state, -1, True, {}  # –ü–æ–±–µ–¥–∞ –º–∞—à–∏–Ω—ã
        if not np.any(self.__state == 0):
            return self.__state, 0, True, {}  # –ù–∏—á—å—è

        return self.__state, 0, False, {}

    def render(self):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–≥—Ä—ã."""
        symbols = {0: ".", 1: "X", 2: "O"}
        for row in self.__state:
            print(" ".join(symbols[cell] for cell in row))
        print()

def optimal_player_move(state, env):
    """
    –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–≥—Ä–æ–∫–∞.
    """
    # –ü–æ–±–µ–¥–Ω—ã–π —Ö–æ–¥
    for i in range(3):
        for j in range(3):
            if state[i, j] == 0:
                state[i, j] = 1
                if env.is_winner(1):
                    state[i, j] = 0
                    return (i, j)
                state[i, j] = 0

    # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –ø–æ–±–µ–¥—ã –º–∞—à–∏–Ω—ã
    for i in range(3):
        for j in range(3):
            if state[i, j] == 0:
                state[i, j] = 2
                if env.is_winner(2):
                    state[i, j] = 0
                    return (i, j)
                state[i, j] = 0

    # –¶–µ–Ω—Ç—Ä
    if state[1, 1] == 0:
        return (1, 1)

    # –£–≥–ª—ã
    for i, j in [(0, 0), (0, 2), (2, 0), (2, 2)]:
        if state[i, j] == 0:
            return (i, j)

    # –õ—é–±–æ–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Ö–æ–¥
    for i in range(3):
        for j in range(3):
            if state[i, j] == 0:
                return (i, j)
              

# –ò–≥—Ä–æ–≤–æ–π —Ü–∏–∫–ª —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
env = TicTacToeEnv()
state = env.reset()
done = False
print("–ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:")
env.render()

while not done:
    action = optimal_player_move(state, env)
    state, reward, done, _ = env.step(action)
    env.render()

if reward == 1:
    print("–ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º! –í—ã –ø–æ–±–µ–¥–∏–ª–∏!")
elif reward == -1:
    print("–ú–∞—à–∏–Ω–∞ –ø–æ–±–µ–¥–∏–ª–∞. –£–¥–∞—á–∏ –≤ —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑!")
else:
    print("–ò–≥—Ä–∞ –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å –≤–Ω–∏—á—å—é.")
''')
    elif st == '5':
        print('''
from google.colab import drive
drive.mount('/content/drive')
              
import gymnasium as gym
from gymnasium.wrappers import RecordVideo
import os


video_folder = "./videos"  #–ø–∞–ø–∫–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∏–¥–µ–æ
os.makedirs(video_folder, exist_ok=True)

env = gym.make("MountainCar-v0", render_mode="rgb_array")  #–æ–∫—Ä—É–∂–µ–Ω–∏–µ
env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda x: x % 5 == 0) #–≤–∏–¥–µ–æ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—è—Ç–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞


num_episodes = 10
for episode in range(num_episodes):
    state, _ = env.reset() #–Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞
    done = False
    total_reward = 0
    while not done:
        action = env.action_space.sample()  #–≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
        state, reward, done, truncated, _ = env.step(action)
        total_reward += reward
    print(f"–≠–ø–∏–∑–æ–¥ {episode + 1} –∑–∞–≤–µ—Ä—à—ë–Ω —Å –Ω–∞–≥—Ä–∞–¥–æ–π: {total_reward}")

env.close()

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ
print(f"–í–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: {video_folder}")
print("–°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ:", os.listdir(video_folder))
    
a = '/content/videos/rl-video-episode-0.mp4'
a
''')
def QL_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.QL_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")

1. –û–±—É—á–∏—Ç–µ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∏–≥—Ä—ã –≤ –±–ª—ç–∫–¥–∂–µ–∫ (–æ–∫—Ä—É–∂–µ–Ω–∏–µ Blackjack-v1), –∏—Å–ø–æ–ª—å–∑—É—è –∞–ª–≥–æ—Ä–∏—Ç–º Q-learning. 
–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã Q-—Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã—è—Å–Ω–∏—Ç–µ —Ä–∞–∑–º–µ—Ä—ã –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–≥—Ä—ã –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏–≥—Ä–æ–∫–∞ –∏ –≤—ã–≤–µ–¥–∏—Ç–µ —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —ç–∫—Ä–∞–Ω. 
–í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤—ã—á–∏—Å–ª–∏—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞ print_every –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–∏–≥—Ä–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–∏–≥—Ä–∞–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π. 
–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –ò–∑—É—á–∏—Ç–µ, –∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç Q-—Ñ—É–Ω–∫—Ü–∏—è (–≤ –∫–∞–∫–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö –∏–≥—Ä–æ–∫ –±—É–¥–µ—Ç –±—Ä–∞—Ç—å –∫–∞—Ä—Ç—É, –≤ –∫–∞–∫–∏—Ö - –Ω–µ—Ç). 
C—ã–≥—Ä–∞–π—Ç–µ N=10000 –∏–≥—Ä, –ø—Ä–∏–º–µ–Ω—è—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—é, –≤—ã–≤–µ–¥–µ–Ω–Ω—É—é –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π Q-—Ñ—É–Ω–∫—Ü–∏–∏, –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –∏ –≤—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –¥–æ–ª—é –≤—ã–∏–≥—Ä–∞–Ω–Ω—ã—Ö –∏–≥—Ä.
C—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è:
ùëéùë°+1(ùë†ùë°)=ùëéùëüùëîùëöùëéùë•ùëéùëÑ(ùë†ùë°,ùëé) 

2. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∞–ª–≥–æ—Ä–∏—Ç–º  ùúñ -greedy Q-learning. 
–ò—Å—Å–ª–µ–¥—É–π—Ç–µ, –∫–∞–∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —Å–ø–æ—Å–æ–± –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–Ω–∞—á–µ–Ω–∏–π Q-—Ñ—É–Ω–∫—Ü–∏–∏ –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
C—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è:
–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —á–∏—Å–ª–æ  ùëù  –∏–∑  ùëà(0,1) ;
–ï—Å–ª–∏  ùëù<ùúñ , —Ç–æ –≤—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º;
–í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ  ùëéùë°+1(ùë†ùë°)=ùëéùëüùëîùëöùëéùë•ùëéùëÑ(ùë†ùë°,ùëé) .
              
3. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ 1, –∏—Å–ø–æ–ª—å–∑—É—è –∞–ª–≥–æ—Ä–∏—Ç–º double Q-learning.
C—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è:
–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —á–∏—Å–ª–æ  ùëù  –∏–∑  ùëà(0,1) ;
–ï—Å–ª–∏  ùëù<ùúñ , —Ç–æ –≤—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º;
–í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ  ùëéùë°+1(ùë†ùë°)=ùëéùëüùëîùëöùëéùë•ùëé((ùëÑùê¥ùë°+ùëÑùêµùë°)(ùë†ùë°,ùëé))) .
–ü—Ä–∞–≤–∏–ª–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Q-—Ñ—É–Ω–∫—Ü–∏–∏:
ùëÑùê¥ùë°+1(ùë†ùë°,ùëéùë°)=ùëÑùê¥ùë°(ùë†ùë°,ùëéùë°)+ùõºùë°(ùë†ùë°,ùëéùë°)(ùëüùë°+ùõæùëÑùêµùë°(ùë†ùë°+1,ùëéùëüùëî ùëöùëéùë•ùëéùëÑùê¥ùë°(ùë†ùë°+1,ùëé))‚àíùëÑùê¥ùë°(ùë†ùë°,ùëéùë°)) 
ùëÑùêµùë°+1(ùë†ùë°,ùëéùë°)=ùëÑùêµùë°(ùë†ùë°,ùëéùë°)+ùõºùë°(ùë†ùë°,ùëéùë°)(ùëüùë°+ùõæùëÑùê¥ùë°(ùë†ùë°+1,ùëéùëüùëî ùëöùëéùë•ùëéùëÑùêµùë°(ùë†ùë°+1,ùëé))‚àíùëÑùêµùë°(ùë†ùë°,ùëéùë°)) 

4. –û–±—É—á–∏—Ç–µ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–∞—à–∏–Ω–æ–π (–æ–∫—Ä—É–∂–µ–Ω–∏–µ MountainCar-v0) –ø—Ä–∏ –ø–æ–º–æ—â–∏ –ª—é–±–æ–≥–æ –∏–∑ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ Q-learning. 
–î–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –æ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π –∫ –∫–æ–Ω–µ—á–Ω–æ–º—É —Ä–∞–∑–±–µ–π—Ç–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π –Ω–∞ –æ–∫–Ω–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–∫–æ–Ω –≤—ã–±–µ—Ä–∏—Ç–µ —Å–∞–º–∏). 
–î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤–æ—Å–ø–æ–ª—å–∑—É–π—Ç–µ—Å—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –æ–∫—Ä—É–∂–µ–Ω–∏–∏. 
–í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤—ã—á–∏—Å–ª–∏—Ç–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥ –∑–∞ —ç–ø–∏–∑–æ–¥ –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω—ã—Ö —Å–µ—Å—Å–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —ç–ø–∏–∑–æ–¥—ã. 
–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
–°–¥–µ–ª–∞–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –∑–∞–ø–∏—Å—å—é —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.
''')
def QL_code(st):
    if st == '1':
        print('''
pip install gymnasium
              
import gymnasium as gym
import numpy as np
from tqdm import tqdm
from dataclasses import dataclass
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from gymnasium.wrappers import RecordVideo
import shutil
              
#–∫–ª–∞—Å—Å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
@dataclass
class Config:
    discount: float = 0.95
    lr: float = 0.005
    n_episodes: int = 10_000
    print_every: int = 5000
              
class Agent:
    def __init__(self, env: gym.Env, config: Config) -> None:
        self.env = env
        self.cfg = config
        self._create_q_table()

##############################################################
    def _create_q_table(self):
        self.q_table = np.zeros((32, 11, 2, 2))

#############################################################

    #e-–∂–∞–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
    def get_action(self, state: tuple) -> int:
        if np.random.rand() < 0.1:
            return self.env.action_space.sample()
        player_sum, dealer_card, usable_ace = state
        return np.argmax(self.q_table[player_sum, dealer_card, int(usable_ace)]) # => Q_0 Q_1

#########################################################################

    def update_q_table(self, state: tuple, new_state: tuple, reward: float, action: int, done: bool) -> None:
        player_sum, dealer_card, usable_ace = state
        player_sum_new, dealer_card_new, usable_ace_new = new_state

        current_q = self.q_table[player_sum, dealer_card, int(usable_ace), action]
        future_q = 0 if done else np.max(self.q_table[player_sum_new, dealer_card_new, int(usable_ace_new)])

        #TD
        self.q_table[player_sum, dealer_card, int(usable_ace), action] = \
            current_q + self.cfg.lr * (reward + self.cfg.discount * future_q - current_q)

#########################################################################

    def run_episode(self) -> float:
        done = False
        state, _ = self.env.reset()
        while not done:
            action = self.get_action(state)
            new_state, reward, terminated, truncated, _ = self.env.step(action)
            done = terminated or truncated
            self.update_q_table(state, new_state, reward, action, done)
            state = new_state
        return reward

###################################################################

    def train(self):
        ep_rewards = []
        stats = {"wins": 0, "losses": 0, "draws": 0}

        for ep in tqdm(range(self.cfg.n_episodes)):
            reward = self.run_episode()
            ep_rewards.append(reward)

            if reward > 0:
                stats["wins"] += 1
            elif reward < 0:
                stats["losses"] += 1
            else:
                stats["draws"] += 1

            if (ep + 1) % self.cfg.print_every == 0:
                print(f"–≠–ø–∏–∑–æ–¥ {ep + 1}: –ü–æ–±–µ–¥: {stats['wins']}, –ü–æ—Ä–∞–∂–µ–Ω–∏–π: {stats['losses']}, –ù–∏—á—å–∏—Ö: {stats['draws']}")

        return ep_rewards, stats
#–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–µ–º
#–ø—Ä–æ–≥–æ–Ω—è–µ–º —Ü–∏–∫–ª –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —ç–ø–∏–∑–æ–¥–æ–≤
#–≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –Ω–∞–≥—Ä–∞–¥ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
              
state_space = (32, 11, 2)
action_space = 2
state_space, action_space
              
env = gym.make("Blackjack-v1")
cfg = Config(n_episodes=50000, lr=0.005, discount=0.95, print_every=1000)
agent = Agent(env, cfg)
rewards, stats = agent.train()
              
player_sum = np.arange(1, 22)
dealer_card = np.arange(1, 11)
usable_ace = [0, 1]

for ace in usable_ace:
    strategy = np.argmax(agent.q_table[1:22, :, ace, :], axis=2)
    plt.imshow(strategy, cmap="coolwarm", interpolation="nearest")
    plt.colorbar(label="–î–µ–π—Å—Ç–≤–∏–µ (0: –≤–∑—è—Ç—å –∫–∞—Ä—Ç—É, 1: –æ—Å—Ç–∞–≤–∏—Ç—å)")
    plt.title(f"–°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å —Ç—É–∑–æ–º = {bool(ace)}")
    plt.xlabel("–û—Ç–∫—Ä—ã—Ç–∞—è –∫–∞—Ä—Ç–∞ –¥–∏–ª–µ—Ä–∞")
    plt.ylabel("–°—É–º–º–∞ –∏–≥—Ä–æ–∫–∞")
    plt.show()
              
wins = 0
n_games = 10000

for _ in range(n_games):
    state, _ = env.reset()
    done = False
    while not done:
        action = agent.get_action(state)
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
    if reward > 0:
        wins += 1

win_rate = wins / n_games
win_rate
''')
    elif st == '2':
        print('''
@dataclass
class Config:
    discount: float = 0.95
    lr: float = 0.005
    n_episodes: float = 100_000
    epsilon: float = 1.0
    final_epsilon: float = 0.3
    print_every: int = 5000
              
class Agent:
    def __init__(self, env: gym.Env, config: Config):
        self.env = env
        self.config = config
        self._create_q_table()
        self.epsilon = config.epsilon

#######################################################

    def _create_q_table(self):
        obs_space = (32, 11, 2)
        action_space = self.env.action_space.n
        self.q_table = np.zeros(obs_space + (action_space,))

#############################################################

    def get_action(self, state: tuple) -> int:
        if np.random.rand() < self.epsilon:
            return self.env.action_space.sample()
        else:
            return np.argmax(self.q_table[state])

############################################################

    def update_q_table(self, state, new_state, reward, action, done):
        current_q = self.q_table[state][action]
        max_future_q = 0 if done else np.max(self.q_table[new_state])
        td_target = reward + self.config.discount * max_future_q
        self.q_table[state][action] = (1 - self.config.lr) * current_q + self.config.lr * td_target

##############################################################

    def decay_epsilon(self, episode: int):
        epsilon_decay = (self.config.epsilon - self.config.final_epsilon) / self.config.n_episodes
        self.epsilon = max(self.config.final_epsilon, self.config.epsilon - epsilon_decay * episode)

#################################################################

    def run_episode(self):
        state, _ = self.env.reset()
        done = False
        total_reward = 0

        while not done:
            action = self.get_action(state)
            new_state, reward, terminated, truncated, _ = self.env.step(action)
            done = terminated or truncated


            self.update_q_table(state, new_state, reward, action, done)
            state = new_state
            total_reward += reward

        return total_reward

####################################################################

    def train(self):
        rewards = []
        win_count = 0

        for episode in tqdm(range(self.config.n_episodes)):
            reward = self.run_episode()
            rewards.append(reward)

            if reward > 0:
                win_count += 1

            if (episode + 1) % self.config.print_every == 0:
                win_rate = win_count / self.config.print_every
                print(f"–≠–ø–∏–∑–æ–¥ {episode + 1}: Win rate = {win_rate:.2f}")
                win_count = 0


            self.decay_epsilon(episode)

        return rewards

########################################################################

    def evaluate(self, n_games=10_000):
        wins = 0
        for _ in range(n_games):
            state, _ = self.env.reset()
            done = False

            while not done:
                action = np.argmax(self.q_table[state])
                new_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                state = new_state

            if reward > 0:
                wins += 1

        return wins / n_games
              
env = gym.make("Blackjack-v1")
config = Config()
agent = Agent(env, config)


print("–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞...")
rewards = agent.train()
print("–û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–≥–µ–Ω—Ç–∞...")
win_rate = agent.evaluate()
print(f"–î–æ–ª—è –≤—ã–∏–≥—Ä–∞–Ω–Ω—ã—Ö –∏–≥—Ä: {win_rate:.2f}")
''')
    elif st=='3':
        print('''
@dataclass
class Config:
    discount: float = 0.95
    lr: float = 0.005
    n_episodes: int = 100_000
    epsilon: float = 1.0
    final_epsilon: float = 0.3
    print_every: int = 5000
              

class DoubleQLearningAgent:
    def __init__(self, env: gym.Env, config: Config):
        self.env = env
        self.config = config
        self._create_q_tables()
        self.epsilon = config.epsilon

############################################################

    def _create_q_tables(self):
        obs_space = (32, 11, 2)
        action_space = self.env.action_space.n
        self.q_table_a = np.zeros(obs_space + (action_space,))
        self.q_table_b = np.zeros(obs_space + (action_space,))

############################################################

    def get_action(self, state: tuple) -> int:
        if np.random.rand() < self.epsilon:
            return self.env.action_space.sample()
        else:
            q_sum = self.q_table_a[state] + self.q_table_b[state]
            return np.argmax(q_sum)

    def update_q_tables(self, state, new_state, reward, action, done):
        if np.random.rand() < 0.5:
            current_q = self.q_table_a[state][action]
            max_action = np.argmax(self.q_table_a[new_state])
            target_q = reward + self.config.discount * self.q_table_b[new_state][max_action] * (not done)
            self.q_table_a[state][action] += self.config.lr * (target_q - current_q)

        else:
            current_q = self.q_table_b[state][action]
            max_action = np.argmax(self.q_table_b[new_state])
            target_q = reward + self.config.discount * self.q_table_a[new_state][max_action] * (not done)
            self.q_table_b[state][action] += self.config.lr * (target_q - current_q)

#################################################

    def decay_epsilon(self, episode: int):
        epsilon_decay = (self.config.epsilon - self.config.final_epsilon) / self.config.n_episodes
        self.epsilon = max(self.config.final_epsilon, self.config.epsilon - epsilon_decay * episode)

###################################################

    def run_episode(self):
        state, _ = self.env.reset()
        done = False
        total_reward = 0

        while not done:
            action = self.get_action(state)
            new_state, reward, terminated, truncated, _ = self.env.step(action)
            done = terminated or truncated

            self.update_q_tables(state, new_state, reward, action, done)
            state = new_state
            total_reward += reward

        return total_reward

#########################################################

    def train(self):
        rewards = []
        win_count = 0

        for episode in tqdm(range(self.config.n_episodes)):
            reward = self.run_episode()
            rewards.append(reward)

            if reward > 0:
                win_count += 1

            if (episode + 1) % self.config.print_every == 0:
                win_rate = win_count / self.config.print_every
                print(f"–≠–ø–∏–∑–æ–¥ {episode + 1}: Win rate = {win_rate:.2f}")
                win_count = 0

            # –£–º–µ–Ω—å—à–µ–Ω–∏–µ epsilon
            self.decay_epsilon(episode)

        return rewards

###############################################################

    def evaluate(self, n_games=10_000):
        wins = 0
        for _ in range(n_games):
            state, _ = self.env.reset()
            done = False

            while not done:
                q_sum = self.q_table_a[state] + self.q_table_b[state]
                action = np.argmax(q_sum)
                new_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                state = new_state

            if reward > 0:
                wins += 1

        return wins / n_games
              

env = gym.make("Blackjack-v1")
config = Config()
agent = DoubleQLearningAgent(env, config)


print("–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞...")
rewards = agent.train()
print("–û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–≥–µ–Ω—Ç–∞...")
win_rate = agent.evaluate()
print(f"–î–æ–ª—è –≤—ã–∏–≥—Ä–∞–Ω–Ω—ã—Ö –∏–≥—Ä: {win_rate:.2f}")
''')
    elif st=='4':
        print('''
class Discretizer:
    def __init__(self, low, high, bins):
        self.bins = bins
        self.bin_width = (high - low) / bins
        self.low = low

    def discretize(self, value): #–∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∏–Ω–¥–µ–∫—Å
        return np.floor((value - self.low) / self.bin_width).astype(int)

    def transform(self, state):
        return tuple(self.discretize(state))

##############################################################

class QLearningAgent:
    """
    –ö–ª–∞—Å—Å –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–∞—à–∏–Ω–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Q-learning.
    """
    def __init__(self, env, n_bins, learning_rate=0.1, discount=0.99, epsilon=1.0, final_epsilon=0.1, n_episodes=10_000):
        self.env = env
        self.n_bins = n_bins
        self.learning_rate = learning_rate
        self.discount = discount
        self.epsilon = epsilon
        self.final_epsilon = final_epsilon
        self.n_episodes = n_episodes

        # –î–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π
        obs_space = env.observation_space
        self.discretizer = Discretizer(low=obs_space.low, high=obs_space.high, bins=n_bins)


        self.q_table = np.zeros((n_bins, n_bins, env.action_space.n))

######################################################################

    def get_action(self, state):
        if np.random.rand() < self.epsilon:
            return self.env.action_space.sample()
        else:
            return np.argmax(self.q_table[state])

#######################################################

    def update_q_table(self, state, action, reward, new_state, done):
        max_future_q = 0 if done else np.max(self.q_table[new_state])
        current_q = self.q_table[state][action]
        new_q = current_q + self.learning_rate * (reward + self.discount * max_future_q - current_q)
        self.q_table[state][action] = new_q

#####################################################

    def decay_epsilon(self, episode):
        epsilon_decay = (self.epsilon - self.final_epsilon) / self.n_episodes
        self.epsilon = max(self.final_epsilon, self.epsilon - epsilon_decay * episode)

#######################################################

    def train(self):
        rewards = []
        success_count = 0

        for episode in tqdm(range(self.n_episodes)):
            state, _ = self.env.reset()
            state = self.discretizer.transform(state) #–Ω–∞—á —Å–æ—Å—Ç –≤ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ
            total_reward = 0
            done = False

            while not done:
                action = self.get_action(state)
                new_state, reward, terminated, truncated, _ = self.env.step(action)
                new_state = self.discretizer.transform(new_state)
                done = terminated or truncated

                self.update_q_table(state, action, reward, new_state, done)
                state = new_state
                total_reward += reward

            rewards.append(total_reward)
            if total_reward >= -199:
                success_count += 1

            if (episode + 1) % 1000 == 0:
                avg_reward = np.mean(rewards[-1000:])
                print(f"–≠–ø–∏–∑–æ–¥ {episode + 1}: –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ = {avg_reward:.2f}, –£—Å–ø–µ—Ö–∏ = {success_count}/1000")
                success_count = 0

            self.decay_epsilon(episode)

        return rewards


#########################################################################

    def evaluate(self, n_games=100):
        rewards = []
        for _ in range(n_games):
            state, _ = self.env.reset()
            state = self.discretizer.transform(state)
            done = False
            total_reward = 0

            while not done:
                action = np.argmax(self.q_table[state])
                new_state, reward, terminated, truncated, _ = self.env.step(action)
                new_state = self.discretizer.transform(new_state)
                done = terminated or truncated
                state = new_state
                total_reward += reward

            rewards.append(total_reward)

        avg_reward = np.mean(rewards)
        print(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ {n_games} –∏–≥—Ä: {avg_reward:.2f}")
        return avg_reward

# –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
env = gym.make("MountainCar-v0", render_mode="rgb_array")

# –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∏–¥–µ–æ –≤ Colab
video_folder = "/content/videos"
video_env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda x: x % 5000 == 0)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–≥–µ–Ω—Ç–∞
agent = QLearningAgent(env, n_bins=20, learning_rate=0.1, discount=0.99, epsilon=1.0, final_epsilon=0.01, n_episodes=20_000)

# –û–±—É—á–µ–Ω–∏–µ
print("–û–±—É—á–µ–Ω–∏–µ...")
agent.train()

# –û—Ü–µ–Ω–∫–∞
print("–û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏...")
agent.evaluate()

# –£–ø–∞–∫–æ–≤–∫–∞ –≤–∏–¥–µ–æ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
shutil.make_archive("/content/mountaincar_videos", 'zip', video_folder)

# –ó–∞–∫—Ä—ã—Ç–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏–π
env.close()
video_env.close()

print("–í–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∏ —É–ø–∞–∫–æ–≤–∞–Ω—ã –≤ –∞—Ä—Ö–∏–≤: /content/mountaincar_videos.zip. –í—ã –º–æ–∂–µ—Ç–µ —Å–∫–∞—á–∞—Ç—å –µ–≥–æ.")
              
import gym
import numpy as np
from gym.wrappers import RecordVideo
from tqdm import tqdm

class Discretizer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π.
    """
    def __init__(self, low, high, bins):
        self.bins = bins
        self.bin_width = (high - low) / bins
        self.low = low

    def discretize(self, value):
        """
        –î–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏—è.
        """
        return np.floor((value - self.low) / self.bin_width).astype(int)

    def transform(self, state):
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è (–ø–æ–∑–∏—Ü–∏–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏).
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∏–Ω–¥–µ–∫—Å—ã
        discretized_state = tuple(self.discretize(state))

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã, —á—Ç–æ–±—ã –æ–Ω–∏ –Ω–µ –≤—ã—Ö–æ–¥–∏–ª–∏ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã
        discretized_state = tuple(np.clip(discretized_state, 0, self.bins - 1))

        return discretized_state

class QLearningAgent:
    """
    –ö–ª–∞—Å—Å –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–∞—à–∏–Ω–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Q-learning.
    """
    def __init__(self, env, n_bins, learning_rate=0.1, discount=0.99, epsilon=1.0, final_epsilon=0.1, n_episodes=10_000):
        self.env = env
        self.n_bins = n_bins
        self.learning_rate = learning_rate
        self.discount = discount
        self.epsilon = epsilon
        self.final_epsilon = final_epsilon
        self.n_episodes = n_episodes

        # –î–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π
        obs_space = env.observation_space
        self.discretizer = Discretizer(low=obs_space.low, high=obs_space.high, bins=n_bins)

        # –°–æ–∑–¥–∞–Ω–∏–µ Q-—Ç–∞–±–ª–∏—Ü—ã
        self.q_table = np.zeros((n_bins, n_bins, env.action_space.n))

    def get_action(self, state):
        """
        –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è: epsilon-–∂–∞–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è.
        """
        if np.random.rand() < self.epsilon:
            return self.env.action_space.sample()
        else:
            return np.argmax(self.q_table[state])

    def update_q_table(self, state, action, reward, new_state, done):
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q-—Ç–∞–±–ª–∏—Ü—ã –ø–æ –ø—Ä–∞–≤–∏–ª—É Q-learning.
        """
        max_future_q = 0 if done else np.max(self.q_table[new_state])
        current_q = self.q_table[state][action]
        new_q = current_q + self.learning_rate * (reward + self.discount * max_future_q - current_q)
        self.q_table[state][action] = new_q

    def decay_epsilon(self, episode):
        """
        –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ epsilon.
        """
        epsilon_decay = (self.epsilon - self.final_epsilon) / self.n_episodes
        self.epsilon = max(self.final_epsilon, self.epsilon - epsilon_decay * episode)

    def train(self):
        """
        –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞.
        """
        rewards = []
        success_count = 0

        for episode in tqdm(range(self.n_episodes)):
            state, _ = self.env.reset()
            state = self.discretizer.transform(state)
            total_reward = 0
            done = False

            while not done:
                action = self.get_action(state)
                new_state, reward, done, info = self.env.step(action)  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: 4 –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è
                new_state = self.discretizer.transform(new_state)
                total_reward += reward

                self.update_q_table(state, action, reward, new_state, done)
                state = new_state

            rewards.append(total_reward)
            if total_reward >= -199:  # –£—Å–ø–µ—Ö: –º–∞—à–∏–Ω–∞ –¥–æ—Å—Ç–∏–≥–ª–∞ –≤–µ—Ä—à–∏–Ω—ã
                success_count += 1

            # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–∂–¥—ã–µ 1000 —ç–ø–∏–∑–æ–¥–æ–≤
            if (episode + 1) % 1000 == 0:
                avg_reward = np.mean(rewards[-1000:])
                print(f"–≠–ø–∏–∑–æ–¥ {episode + 1}: –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ = {avg_reward:.2f}, –£—Å–ø–µ—Ö–∏ = {success_count}/1000")
                success_count = 0

            self.decay_epsilon(episode)

        return rewards

    def evaluate(self, n_games=100):
        """
        –û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞.
        """
        rewards = []
        for _ in range(n_games):
            state, _ = self.env.reset()
            state = self.discretizer.transform(state)
            done = False
            total_reward = 0

            while not done:
                action = np.argmax(self.q_table[state])  # –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
                new_state, reward, done, info = self.env.step(action)  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: 4 –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è
                new_state = self.discretizer.transform(new_state)
                total_reward += reward
                state = new_state

            rewards.append(total_reward)

        avg_reward = np.mean(rewards)
        print(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ {n_games} –∏–≥—Ä: {avg_reward:.2f}")
        return avg_reward

# –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è —Å —Ä–µ–∂–∏–º–æ–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤–∏–¥–µ–æ
env = gym.make("MountainCar-v0", render_mode="rgb_array")

# –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤–∏–¥–µ–æ
video_env = RecordVideo(env, video_folder="./videos", episode_trigger=lambda x: x % 5000 == 0)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–≥–µ–Ω—Ç–∞
agent = QLearningAgent(env, n_bins=20, learning_rate=0.1, discount=0.99, epsilon=1.0, final_epsilon=0.01, n_episodes=20_000)

# –û–±—É—á–µ–Ω–∏–µ
print("–û–±—É—á–µ–Ω–∏–µ...")
agent.train()

# –û—Ü–µ–Ω–∫–∞
print("–û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏...")
agent.evaluate()

# –ó–∞–∫—Ä—ã—Ç–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
env.close()
video_env.close()
''')
    else:
        print('no such index')

def QDN_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.QDN_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")
          
1. –î–æ–ø–∏—à–∏—Ç–µ –∫–ª–∞—Å—Å ReplayMemory –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –º–µ–∂–¥—É —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏.
          
2. –î–æ–ø–∏—à–∏—Ç–µ –∫–ª–∞—Å—Å DQN –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è Q-—Ñ—É–Ω–∫—Ü–∏–∏.
          
3. –î–æ–ø–∏—à–∏—Ç–µ –∫–ª–∞—Å—Å—ã PolicyConfig –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ–ª–∏—Ç–∏–∫–∏ –∞–≥–µ–Ω—Ç–∞ –∏ Policy –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏.
          
4. –ù–∞–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é plot_metrics, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è: —Å—É–º–º–∞—Ä–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã –∑–∞ –∫–∞–∂–¥—ã–π —ç–ø–∏–∑–æ–¥ –∏ 
–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ x-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –º–∞—à–∏–Ω—ã –∑–∞ —ç–ø–∏–∑–æ–¥. –î–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–∂–µ—Ç–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è wandb –∏–ª–∏ –ª—é–±—ã–º –¥—Ä—É–≥–∏–º —É–¥–æ–±–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º.
          
5. –î–æ–ø–∏—à–∏—Ç–µ –∫–ª–∞—Å—Å—ã TrainConfig –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –æ–±—É—á–µ–Ω–∏—è –∏ Trainer –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è.
          
6. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–∞—à–∏–Ω–æ–π –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏ MountainCar-v0. –î–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–æ—Å—Ç–æ—è–Ω–∏–π –≤ —Ç–µ–Ω–∑–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–±–µ—Ä—Ç–∫—É 
TransformObservation. –í—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –≥—Ä–∞—Ñ–∏–∫ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤—Å—Ç–∞–≤—å—Ç–µ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã —ç—Ç–∏—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤.
''')
    
def QDN_code(st):
    if st == '1':
        print('''
import torch as th
import torch.nn as nn
from collections import namedtuple, deque
import random
import math
import gymnasium as gym
from dataclasses import dataclass
import wandb
from tqdm import tqdm
from gymnasium.wrappers import RecordVideo
import numpy as np
              
wandb.login()
              
device = th.device("cpu")
device
              
#—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–∂–¥–æ–º –ø–µ—Ä–µ—Ö–æ–¥–µ –æ–±—É—á–µ–Ω–∏—è
Transition = namedtuple(
    'Transition',
    ('state', 'action', 'next_state', 'reward', 'done')
)

class ReplayMemory(object):
    def __init__(self, capacity):
        """capacity - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        self.memory = deque([], maxlen=capacity) #–¥–æ–±–∞–≤–ª—è–µ–º –∏ —É–¥–∞–ª—è–µ–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å –æ–±–µ–∏—Ö —Å—Ç–æ—Ä–æ–Ω


    #–¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –≤ –ø–∞–º—è—Ç—å
    def push(self, *args):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥. –ü—Ä–∏ –Ω–µ—Ö–≤–∞—Ç–∫–µ –º–µ—Å—Ç–∞ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ —É–¥–∞–ª—è—é—Ç—Å—è."""
        #–¥–æ–±–∞–≤–ª—è–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –≤ –ø–∞–º—è—Ç—å
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç batch_size —Å–ª—É—á–∞–π–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤"""
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)
''')
    elif st == '2':
        print('''
class DQN(nn.Module):
    """–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è Q-—Ñ—É–Ω–∫—Ü–∏–∏."""
    def __init__(self, n_observations, n_actions):
      #n_observations: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π (—Ñ—É–Ω–∫—Ü–∏–π —Å–æ—Å—Ç–æ—è–Ω–∏—è)
        super().__init__()
        self.layer_1 = nn.Linear(n_observations, 32)
        self.layer_2 = nn.Linear(32, 32) #–≥–ª—É–±–∏–Ω–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.layer_3 = nn.Linear(32, n_actions) #–æ–∂–∏–¥–∞–µ–º–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è
        self.relu = nn.ReLU()

    def forward(self, x):
        """–î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–æ–ª–∂–Ω—ã –ø–æ–ª—É—á–∞—Ç—å n_actions —á–∏—Å–µ–ª."""
        x = self.relu(self.layer_1(x))
        x = self.relu(self.layer_2(x))
        out = self.layer_3(x)
        return out
''')
    elif st == '3':
        print('''
@dataclass
class PolicyConfig:
    """–°–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Policy: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, –∫–æ–ª-–≤–æ –¥–µ–π—Å—Ç–≤–∏–π,
    —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥–µ—Ç —Ä–∞—Å–ø–æ–ª–∞–≥–∞—Ç—å—Å—è –º–æ–¥–µ–ª–∏; Œµ –∏ —Ç.–¥."""
    n_observations: int = 2
    n_actions: int = 3
    epsilon: float = 1.0 #–Ω–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –µ–ø—Å–∏–ª–æ–Ω –¥–ª—è –µ-–∂–∞–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    final_epsilon: float = 0.01
    epsilon_decay: float = 0.001 #—Å–∫–æ—Ä–æ—Å—Ç—å —É–º–µ–Ω—å—à–µ–Ω–∏—è –µ
    device: th.device = th.device("cpu")
              

class Policy:
    def __init__(self, env: gym.Env, policy_cfg: PolicyConfig) -> None:
        self.cfg = policy_cfg
        self.env = env
        self.action_space = env.action_space
        self.policy_network = DQN(
            policy_cfg.n_observations, policy_cfg.n_actions
        ).to(policy_cfg.device) # —ç—Ç—É –≤–µ—Ä—Å–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ

        self.target_network = DQN(
            policy_cfg.n_observations, policy_cfg.n_actions
        ).to(policy_cfg.device) # —ç—Ç—É –≤–µ—Ä—Å–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ


        self.sync_models()
        self.steps_done = 0

    def sync_models(self) -> None:
        # –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –∏–∑ –º–æ–¥–µ–ª–∏ self.policy_network –≤ self.target_network , —á—Ç–æ–±—ã –æ–±–µ —Å–µ—Ç–∏ –Ω–∞—á–∏–Ω–∞–ª–∏ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        self.target_network.load_state_dict(self.policy_network.state_dict())

    def get_best_action(self, state: th.Tensor) -> int:
        # Œµ-–∂–∞–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –≤—ã–±–æ—Ä–∞
        # –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è state. –°–Ω–∞—á–∞–ª–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ Œµ,
        # –∫–æ—Ç–æ—Ä–æ–µ —É–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –ø–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞
        epsilon_th = self.cfg.final_epsilon + (self.cfg.epsilon - self.cfg.final_epsilon) * \
            math.exp(-1 * self.steps_done * self.cfg.epsilon_decay)
        self.steps_done += 1


        #–ï—Å–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–µ —á–∏—Å–ª–æ –±–æ–ª—å—à–µ Œµ, –∞–≥–µ–Ω—Ç –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ, –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—é—â–µ–µ –æ—Ü–µ–Ω–∫—É –æ—Ç policy_network.
        # –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –æ–Ω –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        if random.random() > epsilon_th:
            with th.no_grad():
                return th.argmax(self.policy_network(state)).item()
        else:
            return self.action_space.sample()

    def save(self) -> None:
        # –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–∏—Å–∫
        th.save(self.policy_network.state_dict(), "policy_weights.pth")
        th.save(self.target_network.state_dict(), "target_weights.pth")

    def load(self) -> None:
        # –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –¥–∏—Å–∫–∞
        self.policy_network.load_state_dict(th.load("policy_weights.pth", weights_only=True))
        self.target_network.load_state_dict(th.load("target_weights.pth", weights_only=True))

''')
    elif st == '4':
        print('''
def plot_metrics(reward, max_x_coord, loss):
    wandb.log({
        "Reward Sum": reward,
        "Max X-coord": max_x_coord,
        "SmoothL1Loss": loss,
    })
              

#   loss(x) = 0.5 * (x^2)   if |x| < 1
            # |x| - 0.5
#–≥–ª–∞–¥–∫–∞—è —Ñ-–∏—è , –º–µ–Ω–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞—è , —á–µ–º L1
''')
    elif st=='5':
        print('''
@dataclass
class TrainConfig:
    """–°–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è: –∫-—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è,
    –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –∏ —Ç.–¥."""
    lr: float = 0.001
    batch_size: int = 64
    num_episodes: int = 100
    capacity: int = 10000 #–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
    gamma: float = 0.99
    sync_frequency: int = 20
              
class Trainer:
    def __init__(self, env: gym.Env, train_config: TrainConfig, policy: Policy):
        self.cfg = train_config
        self.env = env
        self.memory = ReplayMemory(self.cfg.capacity)
        self.policy = policy
        self.optimizer = th.optim.Adam(policy.policy_network.parameters(), lr=self.cfg.lr)
        self.loss_func = nn.SmoothL1Loss()

    def train(self):
        max_x_coord = -100

        wandb.init(project="08-03-dqn", name="train-5")

        # - –∏—Ç–µ—Ä–∞—Ü–∏—è –ø–æ —ç–ø–∏–∑–æ–¥–∞–º (run_episode)
        with tqdm(total=self.cfg.num_episodes, desc="Episode") as pbar:
            for episode in range(self.cfg.num_episodes):
                ep_max_x_coord, ep_reward, ep_loss = self.run_episode(env.reset()[0])

                # - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –æ—Å–∏ x, –∫–æ—Ç–æ—Ä–æ–≥–æ —É–¥–∞–ª–æ—Å—å –¥–æ—Å—Ç–∏—á—å)
                if ep_max_x_coord.item() > max_x_coord:
                    max_x_coord = ep_max_x_coord.item()
                    self.policy.save()
                    print(f"Current Max X: {ep_max_x_coord}, Episode: {episode + 1}")

                # - —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π (policy –∏ target) - –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
                if episode % self.cfg.sync_frequency == 0:
                    self.policy.sync_models()

                pbar.set_postfix(
                    {"Ep_Max_X": f"{ep_max_x_coord.item():.4f}",
                      "Reward": f"{ep_reward.item():.4f}",
                      "Loss": f"{ep_loss.item():.4f}",
                      "Abs_Max_X": f"{max_x_coord:.4f}",
                     })
                pbar.set_description(f"Episode {episode + 1}")
                pbar.update(1)

                # - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                # - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∑–∞ —ç–ø–∏–∑–æ–¥
                plot_metrics(ep_reward, ep_max_x_coord, ep_loss)

        wandb.finish()

    def run_episode(self, start_state: th.Tensor):
        # –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–æ–≥–æ–Ω–∞ —ç–ø–∏–∑–æ–¥–∞
        # - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –∏ –∏—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        done = False
        ep_reward = 0
        ep_x_coords = []
        ep_losses = []
        state = start_state

        while not done:
            action = self.policy.get_best_action(state=state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            ep_x_coords.append(next_state[0])

            update_reward = reward + abs(state[0] - next_state[0]) * 10
            if next_state[0] > 0.5:
                update_reward += 20
            ep_reward += update_reward

            self.memory.push(state, action, next_state, update_reward, terminated or truncated)

            # - –∑–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–∞—Ç—á–∞ –∏ –æ–±—É—á–µ–Ω–∏—è (generate_batch_and_fit)
            if len(self.memory) >= self.cfg.batch_size:
                loss = self.generate_batch_and_fit()
                ep_losses.append(loss.item())

            state = next_state
            done = terminated or truncated

        return max(ep_x_coords), ep_reward, np.mean(ep_losses)

    def generate_batch_and_fit(self):
        # –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∞—Ç—á –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        transitions = self.memory.sample(self.cfg.batch_size)

        # –ø–æ–ª—É—á–∞–µ–º –Ω–∞–±–æ—Ä —Ç–µ–∫—É—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
        states = th.stack([t.state for t in transitions])
        next_states = th.stack([t.next_state for t in transitions])

        # –ø–æ–ª—É—á–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è —Ç–µ–∫—É—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
        targets = self.policy.policy_network(states)
        next_state_targets = self.policy.target_network(next_states)

        # –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º targets: –µ—Å–ª–∏ —Å–µ—Å—Å–∏—è –∑–∞–∫–æ–Ω—á–µ–Ω–∞, —Ç–æ targets[t.action] = t.reward
        # –µ—Å–ª–∏ –Ω–µ—Ç, —Ç–æ targets[t.action] = t.reward + gamma * max(next_state_targets[i])
        # t - –ø–µ—Ä–µ—Ö–æ–¥ –∏–∑ –±–∞—Ç—á–∞ —Å –Ω–æ–º–µ—Ä–æ–º i
        for i, t in enumerate(transitions):
            if transitions[i].done:
                targets[transitions[i].action] = transitions[i].reward
            else:
                targets[transitions[i].action] = transitions[i].reward + self.cfg.gamma * max(next_state_targets[i])

        return self.fit_policy_network(states, targets)

    def fit_policy_network(self, X, y):
        # X - –±–∞—Ç—á —Å–æ—Å—Ç–æ—è–Ω–∏–π (batch_size x 2)
        # y - –Ω–∞–±–æ—Ä Q-–∑–Ω–∞—á–µ–Ω–∏–π (batch_size x 3)
        self.optimizer.zero_grad()
        y_pred = self.policy.policy_network(X)
        loss = self.loss_func(y_pred, y)
        loss.backward()
        self.optimizer.step()
        return loss
''')
    elif st == '6':
        print('''
class TransformObservation(gym.ObservationWrapper):
    def __init__(self, env, device):
        self.device = device
        super(TransformObservation, self).__init__(env)

    def observation(self, observation):
        return th.tensor(observation, device=device, dtype=th.float32)

env = gym.make("MountainCar-v0", render_mode="rgb_array")
env = RecordVideo(env,
                  video_folder="videos_MountainCar_DQN",
                  episode_trigger=lambda x: True,
                  name_prefix="rl-MountainCar")
env = TransformObservation(env, device)
              

tr = Trainer(env, TrainConfig(num_episodes=500), Policy(env, PolicyConfig()))
tr.train()
              

# –¢–µ—Å—Ç

test_env = gym.make("MountainCar-v0", render_mode="rgb_array")
test_env = RecordVideo(
    test_env,
    video_folder="test_videos_MountainCar_DQN",
    episode_trigger=lambda x: True,
    name_prefix="test-run-MountainCar"
)

policy = Policy(test_env, PolicyConfig())
policy.load()

num_episodes = 50

for episode in range(num_episodes):
    state = test_env.reset()[0]
    done = False
    total_reward = 0

    while not done:
        action = th.argmax(policy.policy_network(th.tensor(state, device=device))).item()
        state, reward, terminated, truncated, _ = test_env.step(action)
        total_reward += reward
        done = terminated or truncated

    print(f"Episode {episode + 1}: Total reward = {total_reward}")

test_env.close()
''')
def polGr_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.polGr_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")
              
 1. –î–æ–ø–∏—à–∏—Ç–µ –∫–ª–∞—Å—Å—ã Policy –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ Trainer –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ REINFORCE. 
–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∏–≥—Ä—ã –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏ CartPole-v1. –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –¥–∏–Ω–∞–º–∏–∫—É –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ —ç–ø–∏–∑–æ–¥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è. 
–°—ã–≥—Ä–∞–π—Ç–µ —ç–ø–∏–∑–æ–¥, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –∏ —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∞–≥–µ–Ω—Ç –≤—ã—É—á–∏–≤–∞–µ—Ç—Å—è, –∫–∞–∫ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —à–µ—Å—Ç.
ùêøùëÉùê∫=‚àí‚àëùëñùëÖùëñlogùëù(ùëéùëñ|ùë†ùëñ) 
ùëÖùë°=‚àëùëò=0‚àûùõæùëòùëüùë°+ùëò 
       
2. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ 1, –¥–µ–ª–∞—è —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –Ω–µ –ø–æ—Å–ª–µ –æ–¥–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞, –∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –ø—Ä–æ–≥–æ–Ω–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤.
–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ –≤—Å–µ —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–¥–∞–ª–µ–Ω—ã.
          
3. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ 1, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–≤ –∞–ª–≥–æ—Ä–∏—Ç–º REINFONCE —Å baseline.
ùêøùëÉùê∫=‚àí‚àëùëñùê¥ùëñlogùëù(ùëéùëñ|ùë†ùëñ) 
ùê¥ùëñ=ùëÖùëñ‚àíùëâ(ùë†ùëñ) 
ùëÖùë°=‚àëùëò=0‚àûùõæùëòùëüùë°+ùëò 
–≥–¥–µ  ùëüùë°  - –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —à–∞–≥  ùë° .
ùëù(ùëéùëñ|ùë†ùëñ)  –∏  ùëâ(ùë†ùëñ)  –º–æ–¥–µ–ª–∏—Ä—É—é—Ç—Å—è –ø—Ä–∏ –ø–æ–º–æ—â–∏ –¥–≤—É—Ö –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Å–µ—Ç–µ–π. 
–°–µ—Ç—å –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –∑–∞–¥–∞—á–µ 1 –∏ 2 –ø—Ä–∏ –ø–æ–º–æ—â–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å  ùêøùëÉùê∫ . 
–°–µ—Ç—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–∞–∑—ã –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:  ùêøùëâ=‚àëùëñ(ùëâ(ùë†ùëñ)‚àíùëÖùëñ)2 . 
–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ—Å–æ–≤ –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞.
          
4. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ 1, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–≤ –∞–ª–≥–æ—Ä–∏—Ç–º Actor-Critic
ùêøùëÉùê∫=‚àí‚àëùëñùê¥ùëñlogùëù(ùëéùëñ|ùë†ùëñ) 
ùê¥ùëñ=ùëÖùëñ‚àíùëâ(ùë†ùëñ) 
ùëÖùë°=‚àëùëò=0‚àûùõæùëòùëüùë°+ùëò 
–≥–¥–µ  ùëüùë°  - –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —à–∞–≥  ùë° .
ùëù(ùëéùëñ|ùë†ùëñ)  –∏  ùëâ(ùë†ùëñ)  –º–æ–¥–µ–ª–∏—Ä—É—é—Ç—Å—è –ø—Ä–∏ –ø–æ–º–æ—â–∏ –æ–¥–Ω–æ–π —Å–µ—Ç–∏ –≤ –¥–≤—É–º—è –≥–æ–ª–æ–≤–∞–º–∏. 
–ì–æ–ª–æ–≤–∞ –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –∑–∞–¥–∞—á–µ 1 –∏ 2 –ø—Ä–∏ –ø–æ–º–æ—â–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å  ùêøùëÉùê∫ . 
–ì–æ–ª–æ–≤–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–∞–∑—ã –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏: ùêøùëâ=‚àëùëñ(ùëâ(ùë†ùëñ)‚àíùëÖùëñ)2 . 
–ò—Ç–æ–≥–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–∑ —Å–µ–±—è —Å—É–º–º—É —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å –¥–ª—è –≥–æ–ª–æ–≤: ùêø=ùêøùëÉùê∫+ùêøùëâ .
          
''')
def polGr_code(st):
    if st == '1':
        print('''
!pip install gymnasium
              
import torch
import gymnasium as gym
import torch.nn as nn
from torch.distributions import Categorical
import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass
import torch as th
import torch.nn as nn
import torch.optim as optim
from dataclasses import dataclass
import numpy as np
import gymnasium as gym
import matplotlib.pyplot as plt
              

@dataclass
class PolicyConfig:
    n_state: int #4 —Å–æ—Å—Ç–æ—è–Ω–∏—è
    n_action: int #2 (–ª–µ–≤–æ , –ø—Ä–∞–≤–æ )
    n_hidden: int #128
              

@dataclass
class TrainConfig:
    gamma: float = 0.99
    learning_rate: float = 0.001
    episode_num: int = 400
              

#–±–µ—Ä–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–µ–ª–µ–∂–∫–∏ (–≤–µ–∫—Ç–æ—Ä –∏–∑ 4) –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π

class PolicyNetwork(nn.Module):
    def __init__(self, policy_config: PolicyConfig):
        super().__init__()
        self.cfg = policy_config
        self.model = nn.Sequential(
            nn.Linear(self.cfg.n_state, self.cfg.n_hidden),
            nn.ReLU(),
            nn.Linear(self.cfg.n_hidden, self.cfg.n_action),
            nn.Softmax(dim=-1)
        )

    def forward(self, s: th.Tensor) -> th.Tensor:
        return self.model(s) #—Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏

    def get_action(self, s: th.Tensor) -> tuple[int, float]:
        probs = self.forward(s)
        action_dist = th.distributions.Categorical(probs)
        action = action_dist.sample()
        log_prob = action_dist.log_prob(action) #log –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ –æ—à–∏–±–∫–∏
        return action.item(), log_prob
              

class Trainer:
    def __init__(self, env_name: str, policy_config: PolicyConfig, train_config: TrainConfig):
        self.env = gym.make(env_name)
        self.policy = PolicyNetwork(policy_config)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=train_config.learning_rate)
        self.gamma = train_config.gamma
        self.episode_num = train_config.episode_num
        self.rewards_history = []  # –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏

###############################################################

    def train(self):
        for episode in range(self.episode_num):
            log_probs = []
            rewards = []

            state, _ = self.env.reset()
            state = th.tensor(state, dtype=th.float32)

            done = False
            while not done:
                action, log_prob = self.policy.get_action(state)
                log_probs.append(log_prob)


                next_state, reward, done, _, _ = self.env.step(action)
                rewards.append(reward)


                state = th.tensor(next_state, dtype=th.float32)


            discounted_rewards = self.compute_discounted_rewards(rewards)

            loss = self.update_policy(log_probs, discounted_rewards)

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            total_reward = sum(rewards)
            self.rewards_history.append(total_reward)
            print(f"–≠–ø–∏–∑–æ–¥ {episode + 1}/{self.episode_num}, –ù–∞–≥—Ä–∞–¥–∞: {total_reward:.2f}, –ü–æ—Ç–µ—Ä—è: {loss:.4f}")

#######################################################################

    def compute_discounted_rewards(self, rewards: list) -> list:
        discounted_rewards = []
        R = 0
        for reward in reversed(rewards):
            R = reward + self.gamma * R
            discounted_rewards.insert(0, R)
        return discounted_rewards

 #######################################################################

    def update_policy(self, log_probs: list, rewards: list) -> float:
        rewards = th.tensor(rewards, dtype=th.float32) #–Ω–∞–≥—Ä–∞–¥—ã –∫ —Ç–µ–Ω–∑–æ—Ä—É
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)


        #gradient policy
        loss = -th.stack([log_prob * reward for log_prob, reward in zip(log_probs, rewards)]).sum()


        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        return loss.item()

###############################################################################

    def plot_rewards(self):
        plt.plot(self.rewards_history)
        plt.xlabel('–≠–ø–∏–∑–æ–¥')
        plt.ylabel('–°—É–º–º–∞—Ä–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞')
        plt.title('–î–∏–Ω–∞–º–∏–∫–∞ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ —ç–ø–∏–∑–æ–¥')
        plt.show()

 ################################################################

    def evaluate(self):
        state, _ = self.env.reset()
        state = th.tensor(state, dtype=th.float32) #–ø—Ä–∏–≤–æ–¥–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫ —Ç–µ–Ω–∑–æ—Ä—É
        done = False
        total_reward = 0

        while not done:
            self.env.render() #–≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ (–Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –∞–≥–µ–Ω—Ç –æ–±—É—á–∏–ª—Å—è)
            action, _ = self.policy.get_action(state)
            next_state, reward, done, _, _ = self.env.step(action)
            state = th.tensor(next_state, dtype=th.float32)
            total_reward += reward

        print(f"–°—É–º–º–∞—Ä–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –≤ —ç–ø–∏–∑–æ–¥–µ: {total_reward:.2f}")
        self.env.close()
              

env1 = gym.make("CartPole-v1")
              

env1.reset()
              

env1.action_space.n
              

env1.step(1)
              

env1.close()
              

env = gym.make("CartPole-v1")
policy_config = PolicyConfig(n_state=env.observation_space.shape[0], n_action=env.action_space.n, n_hidden=128)
policy_network = PolicyNetwork(policy_config)
train_config = TrainConfig()
              

trainer = Trainer(env, policy_network, train_config)
rewards = trainer.train()
              

plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Training Progress');
              

env = gym.make("CartPole-v1", render_mode='rgb_array')
env = gym.wrappers.RecordVideo(env, 'result1', episode_trigger=lambda x: True)
              

state, _ = env.reset()
state = torch.tensor(state, dtype=torch.float32)
done = False
total_reward = 0
while not done:
    action, _ = policy_network.get_action(state)
    next_state, reward, done, _, _ = env.step(action)
    total_reward += reward
    state = torch.tensor(next_state, dtype=torch.float32)
env.close()
              

total_reward
''')
    elif st == '2':
        print('''
class TrainConfig2:
    gamma: float = 0.99
    learning_rate: float = 0.005
    episode_num: int = 500
    episode_step: int = 10
              

class Trainer:
    def __init__(self, env_name: str, policy_config: PolicyConfig, train_config: TrainConfig, batch_size: int = 10):
        self.env = gym.make(env_name)
        self.policy = PolicyNetwork(policy_config)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=train_config.learning_rate)
        self.gamma = train_config.gamma
        self.episode_num = train_config.episode_num
        self.batch_size = batch_size  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –≤ –æ–¥–Ω–æ–π –æ–±—É—á–∞—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
        self.rewards_history = []

################################################

    def train(self):
        batch_log_probs = []
        batch_rewards = []

        for episode in range(self.episode_num):
            log_probs = []
            rewards = []


            state, _ = self.env.reset()
            state = th.tensor(state, dtype=th.float32)

            done = False
            while not done:
                action, log_prob = self.policy.get_action(state)
                log_probs.append(log_prob)

                next_state, reward, done, _, _ = self.env.step(action)
                rewards.append(reward)


                state = th.tensor(next_state, dtype=th.float32)

           #—Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            batch_log_probs.extend(log_probs)
            batch_rewards.append(rewards)

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã
            total_reward = sum(rewards)
            self.rewards_history.append(total_reward)

            print(f"–≠–ø–∏–∑–æ–¥ {episode + 1}/{self.episode_num}, –ù–∞–≥—Ä–∞–¥–∞: {total_reward:.2f}")


            if (episode + 1) % self.batch_size == 0:
                self.update_policy(batch_log_probs, batch_rewards)

                #–æ—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                batch_log_probs = []
                batch_rewards = []

##############################################################

    def compute_discounted_rewards(self, rewards: list) -> list:
        discounted_rewards = []
        R = 0
        for reward in reversed(rewards):
            R = reward + self.gamma * R
            discounted_rewards.insert(0, R)
        return discounted_rewards

###################################################################

    def update_policy(self, batch_log_probs: list, batch_rewards: list) -> None:
        all_discounted_rewards = []
        for rewards in batch_rewards:
            discounted_rewards = self.compute_discounted_rewards(rewards)
            all_discounted_rewards.extend(discounted_rewards)


        rewards_tensor = th.tensor(all_discounted_rewards, dtype=th.float32)
        rewards_tensor = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)


        loss = -th.stack([log_prob * reward for log_prob, reward in zip(batch_log_probs, rewards_tensor)]).sum()

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        print(f"–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏. –ü–æ—Ç–µ—Ä—è: {loss.item():.4f}")

###########################################################################

    def plot_rewards(self):
        plt.plot(self.rewards_history)
        plt.xlabel('–≠–ø–∏–∑–æ–¥')
        plt.ylabel('–°—É–º–º–∞—Ä–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞')
        plt.title('–î–∏–Ω–∞–º–∏–∫–∞ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ —ç–ø–∏–∑–æ–¥')
        plt.show()

############################################################

    def evaluate(self):
        state, _ = self.env.reset()
        state = th.tensor(state, dtype=th.float32)
        done = False
        total_reward = 0

        while not done:
            self.env.render()
            action, _ = self.policy.get_action(state)
            next_state, reward, done, _, _ = self.env.step(action)
            state = th.tensor(next_state, dtype=th.float32)
            total_reward += reward

        print(f"–°—É–º–º–∞—Ä–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –≤ —ç–ø–∏–∑–æ–¥–µ: {total_reward:.2f}")
        self.env.close()

##################################################################

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
policy_cfg = PolicyConfig(n_state=4, n_action=2, n_hidden=128)  # CartPole-v1
train_cfg = TrainConfig(gamma=0.99, learning_rate=0.001, episode_num=400)

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
trainer = Trainer(env_name="CartPole-v1", policy_config=policy_cfg, train_config=train_cfg, batch_size=10)
trainer.train()

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
trainer.plot_rewards()

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
trainer.evaluate()
              

env2 = gym.make("CartPole-v1")
policy_config2 = PolicyConfig(n_state=env.observation_space.shape[0], n_action=env.action_space.n, n_hidden=128)
policy_network2 = PolicyNetwork(policy_config2)
train_config2 = TrainConfig2()
              

trainer2 = Trainer(env2, policy_network2, train_config2)
rewards2 = trainer2.train()
              

plt.plot(rewards2)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Training Progress');
              

env2test = gym.make("CartPole-v1", render_mode='rgb_array')
env2test = gym.wrappers.RecordVideo(env2test, 'result2', episode_trigger=lambda x: True)
              

state, _ = env2test.reset()
state = torch.tensor(state, dtype=torch.float32)
done = False
total_reward = 0
while not done:
    action, _ = policy_network2.get_action(state)
    next_state, reward, done, _, _ = env2test.step(action)
    total_reward += reward
    state = torch.tensor(next_state, dtype=torch.float32)
env2test.close()
              

total_reward
''')
    elif st == '3':
        print('''
@dataclass
class PolicyConfig:
    n_state: int
    n_action: int
    n_hidden: int

#################################################

class PolicyNetwork(nn.Module):
    def __init__(self, policy_config: PolicyConfig):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(policy_config.n_state, policy_config.n_hidden),
            nn.ReLU(),
            nn.Linear(policy_config.n_hidden, policy_config.n_action),
            nn.Softmax(dim=-1)
        )

    def forward(self, s: th.Tensor) -> th.Tensor:
        return self.model(s)

    def get_action(self, s: th.Tensor) -> tuple[int, float]:
        probs = self.forward(s)
        action_dist = th.distributions.Categorical(probs)
        action = action_dist.sample()
        log_prob = action_dist.log_prob(action)
        return action.item(), log_prob

#########################################################
class ValueNetwork(nn.Module):
    def __init__(self, n_state: int, n_hidden: int):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(n_state, n_hidden),
            nn.ReLU(),
            nn.Linear(n_hidden, 1) #–æ—Ü–µ–Ω–∫–∞ v(s)
        )

    def forward(self, s: th.Tensor) -> th.Tensor:
        return self.model(s).squeeze(-1) #(batch_size ,)

######################################################
@dataclass
class TrainConfig:
    gamma: float = 0.99
    learning_rate: float = 0.001
    episode_num: int = 400

#####################################################
class Trainer:
    def __init__(self, env_name: str, policy_config: PolicyConfig, train_config: TrainConfig):
        self.env = gym.make(env_name)
        self.policy = PolicyNetwork(policy_config)
        self.value = ValueNetwork(policy_config.n_state, policy_config.n_hidden)
        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=train_config.learning_rate)
        self.value_optimizer = optim.Adam(self.value.parameters(), lr=train_config.learning_rate)
        self.gamma = train_config.gamma
        self.episode_num = train_config.episode_num
        self.rewards_history = []
##############################################

    def train(self):
        for episode in range(self.episode_num):
            state, _ = self.env.reset()
            log_probs = []
            rewards = []
            values = []

            done = False
            while not done:
                state_tensor = th.tensor(state, dtype=th.float32)
                action, log_prob = self.policy.get_action(state_tensor)
                value = self.value(state_tensor)

                next_state, reward, done, _, _ = self.env.step(action)
                log_probs.append(log_prob)
                rewards.append(reward)
                values.append(value)

                state = next_state

            returns = self.compute_returns(rewards) # –≤—ã—á–∏—Å–ª—è–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã —Å —É—á–µ—Ç–æ–º –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            advantages = [r - v.item() for r, v in zip(returns, values)] #–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ

            policy_loss = -th.stack([log_prob * adv for log_prob, adv in zip(log_probs, advantages)]).mean() #PG
            self.policy_optimizer.zero_grad()
            policy_loss.backward()
            self.policy_optimizer.step()

            # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –±–∞–∑—ã
            returns_tensor = th.tensor(returns, dtype=th.float32)
            values_tensor = th.stack(values)
            value_loss = nn.functional.mse_loss(values_tensor, returns_tensor) #L_v
            self.value_optimizer.zero_grad()
            value_loss.backward()
            self.value_optimizer.step()

            # –õ–æ–≥–∏—Ä—É–µ–º –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º
            total_reward = sum(rewards)
            self.rewards_history.append(total_reward)
            print(f"Episode {episode + 1}/{self.episode_num}, Reward: {total_reward:.2f}")

        self.plot_rewards()
####################################################################
    def compute_returns(self, rewards: list[float]) -> list[float]:
        returns = []
        g = 0
        for reward in reversed(rewards):
            g = reward + self.gamma * g
            returns.insert(0, g)
        return returns
########################################################

    def plot_rewards(self):
        plt.plot(self.rewards_history)
        plt.xlabel("Episode")
        plt.ylabel("Total Reward")
        plt.title("Training Progress")
        plt.show()


              
env3 = gym.make("CartPole-v1")
policy_network3 = PolicyNetwork(policy_config)
value_network = ValueNetwork(policy_config)
              

trainer3 = Trainer(env3, policy_network3, value_network, train_config)
rewards3 = trainer3.train()
              

plt.plot(rewards3)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Training Progress');
              

env3test = gym.make("CartPole-v1", render_mode='rgb_array')
env3test = gym.wrappers.RecordVideo(env3test, 'result3', episode_trigger=lambda x: True)
              

state, _ = env3test.reset()
state = torch.tensor(state, dtype=torch.float32)
done = False
total_reward = 0
while not done:
    action, _ = policy_network3.get_action(state)
    next_state, reward, done, _, _ = env3test.step(action)
    total_reward += reward
    state = torch.tensor(next_state, dtype=torch.float32)
env3test.close()
              

total_reward
''')
    elif st == '4':
        print('''
class ActorCriticNetwork(nn.Module):
    def __init__(self, policy_config: PolicyConfig):
        super(ActorCriticNetwork, self).__init__()
        self.cfg = policy_config
        self.shared = nn.Sequential(
            nn.Linear(self.cfg.n_state, self.cfg.n_hidden),
            nn.ReLU()
        )

        self.policy_head = nn.Linear(self.cfg.n_hidden, self.cfg.n_action)

        self.value_head = nn.Linear(self.cfg.n_hidden, 1)

    def forward(self, s: torch.Tensor):
        shared_out = self.shared(s)

        policy_probs = torch.softmax(self.policy_head(shared_out), dim=-1)

        value = self.value_head(shared_out)

        return policy_probs, value

    def get_action(self, s: torch.Tensor) -> tuple[int, float]:
        policy_probs, _ = self.forward(s)
        dist = Categorical(policy_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob
              

import os
@dataclass
class ActorCriticConfig:
    n_state: int
    n_action: int
    n_hidden: int
    gamma: float
    learning_rate: float
    episode_num: int
###################################################################

class ActorCriticNetwork(nn.Module):
    def __init__(self, config: ActorCriticConfig):
        super().__init__()
        self.shared_layers = nn.Sequential( #–∏–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            nn.Linear(config.n_state, config.n_hidden),
            nn.ReLU()
        )
        self.actor_head = nn.Sequential( #p –¥–µ–π—Å—Ç–≤–∏–π
            nn.Linear(config.n_hidden, config.n_action),
            nn.Softmax(dim=-1)
        )
        self.critic_head = nn.Linear(config.n_hidden, 1)  #V(s)



    def forward(self, state: th.Tensor):
        shared_output = self.shared_layers(state)
        policy_probs = self.actor_head(shared_output)
        state_value = self.critic_head(shared_output)
        return policy_probs, state_value

#####################################################
class ActorCriticTrainer:
    def __init__(self, env_name: str, config: ActorCriticConfig):
        self.env = gym.make(env_name)
        self.config = config
        self.model = ActorCriticNetwork(config)
        self.optimizer = optim.Adam(self.model.parameters(), lr=config.learning_rate)
        self.gamma = config.gamma
        self.episode_num = config.episode_num
        self.rewards_history = []


##################################################

    def train(self):
        for episode in range(self.episode_num):
            state, _ = self.env.reset()
            state = th.tensor(state, dtype=th.float32)

            log_probs = []
            values = []
            rewards = []

            done = False
            while not done:

                policy_probs, state_value = self.model(state)
                action_dist = th.distributions.Categorical(policy_probs)
                action = action_dist.sample()

                log_prob = action_dist.log_prob(action)
                log_probs.append(log_prob)
                values.append(state_value)


                next_state, reward, done, _, _ = self.env.step(action.item())
                rewards.append(reward)

                state = th.tensor(next_state, dtype=th.float32)

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            self.update_policy_and_value(log_probs, values, rewards)

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            total_reward = sum(rewards)
            self.rewards_history.append(total_reward)
            print(f"–≠–ø–∏–∑–æ–¥ {episode + 1}/{self.episode_num}, –ù–∞–≥—Ä–∞–¥–∞: {total_reward:.2f}")
####################################################################################

    def update_policy_and_value(self, log_probs, values, rewards):
        rewards = self.compute_discounted_rewards(rewards)
        rewards = th.tensor(rewards, dtype=th.float32)

        log_probs = th.stack(log_probs)
        values = th.cat(values).squeeze()

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å
        advantages = rewards - values.detach()
        policy_loss = -(log_probs * advantages).mean()  # L_PG
        value_loss = nn.functional.mse_loss(values, rewards)  # L_V

        loss = policy_loss + value_loss


        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

#########################################################################

    def compute_discounted_rewards(self, rewards):
        discounted_rewards = []
        R = 0
        for reward in reversed(rewards):
            R = reward + self.gamma * R
            discounted_rewards.insert(0, R)
        return discounted_rewards

#############################################################################
    def plot_rewards(self):
        plt.plot(self.rewards_history)
        plt.xlabel('–≠–ø–∏–∑–æ–¥')
        plt.ylabel('–°—É–º–º–∞—Ä–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞')
        plt.title('–î–∏–Ω–∞–º–∏–∫–∞ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ —ç–ø–∏–∑–æ–¥')
        plt.show()


env4 = gym.make("CartPole-v1")
actor_critic_network = ActorCriticNetwork(policy_config)
              

trainer4 = Trainer(env4, actor_critic_network, train_config)
rewards4 = trainer4.train()
              

plt.plot(rewards4)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Training Progress');
              

env4test = gym.make("CartPole-v1", render_mode='rgb_array')
env4test = gym.wrappers.RecordVideo(env4test, 'result4', episode_trigger=lambda x: True)
              

state, _ = env4test.reset()
state = torch.tensor(state, dtype=torch.float32)
done = False
total_reward = 0
while not done:
    action, _ = actor_critic_network.get_action(state)
    next_state, reward, done, _, _ = env4test.step(action)
    total_reward += reward
    state = torch.tensor(next_state, dtype=torch.float32)
env4test.close()
              


total_reward
''')
        
def obDet_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.obDet_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")          

1. –ù–∞–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é parse_xml, –∫–æ—Ç–æ—Ä–∞—è —á–∏—Ç–∞–µ—Ç xml-—Ñ–∞–π–ª —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∞—Ä—Ö–∏–≤–∞ animals.zip –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ç—Ä–∏ –∫–ª—é—á–∞:
{
        "raw": # —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ xmin, ymin, xmax, ymax
        "scaled": # —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ xmin, ymin, xmax, ymax
        "obj_name": # —Å—Ç—Ä–æ–∫–∞
}
–í —ç—Ç–æ–º —Å–ª–æ–≤–∞—Ä–µ row - –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤–µ—Ä—à–∏–Ω bounding box, –∞ scaled - –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ (–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —à–∏—Ä–∏–Ω—É –∏ –≤—ã—Å–æ—Ç—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è). 
–ü—Ä–∏–º–µ–Ω–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –∫ —Ñ–∞–π–ª—É cat.0.xml –∏ –≤—ã–≤–µ–¥–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ —ç–∫—Ä–∞–Ω.
          
2. –û–ø–∏—à–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç AnimalDetectionDataset –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏–≤–∞ animals.zip. –†–µ–∞–ª–∏–∑—É–π—Ç–µ __getitem__ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –æ–Ω –≤–æ–∑–≤—Ä–∞—â–∞–ª —Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–∞: 
—Ç–µ–Ω–∑–æ—Ä —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º, —Å–ª–æ–≤–∞—Ä—å —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ bounding box –∏ –º–µ—Ç–∫—É –æ–±—ä–µ–∫—Ç–∞. 
–ü—Ä–µ–¥—É—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∏–∑–≤–Ω–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞–±–æ—Ä –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –º–µ—Ç–∫–∏ –æ–±—ä–µ–∫—Ç–∞ 
(–¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è) –∏ —Ñ–ª–∞–≥, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π, –Ω—É–∂–Ω–æ –ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ –∏–ª–∏ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bounding box.
          
3. –°–æ–∑–¥–∞–π—Ç–µ –æ–±—ä–µ–∫—Ç –∫–ª–∞—Å—Å–∞ AnimalDetectionDataset –±–µ–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –∏ —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º return_scaled=False. 
–ù–∞–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é show_image_with_bounding_box –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –Ω–∞ –Ω–µ–≥–æ bounding box –∏ –ø–æ–¥–ø–∏—Å–∏ –æ–±—ä–µ–∫—Ç–∞. 
–ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–π—Ç–µ —Ä–∞–±–æ—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å–æ–±–∞–∫–∏ –∏ –∫–æ—à–∫–∏.
          
4. –ù–∞–ø–∏—à–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –≤—ã–¥–µ–ª–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤. 
–†–µ–∞–ª–∏–∑—É–π—Ç–µ –¥–≤—É—Ö–≥–æ–ª–æ–≤—É—é —Å–µ—Ç—å, –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –º–µ—Ç–∫—É –æ–±—ä–µ–∫—Ç–∞ (–∑–∞–¥–∞—á–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏), –∞ –≤—Ç–æ—Ä–∞—è –≥–æ–ª–æ–≤–∞ 
–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç 4 –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤–µ—Ä—à–∏–Ω bounding box (–∑–∞–¥–∞—á–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏). –í –∫–∞—á–µ—Å—Ç–≤–µ backbone –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å resnet50 –∏–∑ –ø–∞–∫–µ—Ç–∞ torchvision.
          
5. –†–∞–∑–±–µ–π—Ç–µ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–∞—é—â–µ–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ. –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å, –æ–ø–∏—Å–∞–Ω–Ω—É—é –≤ –∑–∞–¥–∞—á–µ 4. 
–ü—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –∑–∞–±—É–¥—å—Ç–µ —É–∫–∞–∑–∞—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ ResNet.
–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—É–º–º—É MSELoss (–¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –æ—à–∏–±–∫–∏ –Ω–∞ –∑–∞–¥–∞—á–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏) –∏ CrossEntropyLoss (–¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –æ—à–∏–±–∫–∏ –Ω–∞ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏) –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏.
–î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è —Å–ª–æ–∏ backbone –º–æ–∂–Ω–æ –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å. –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤—ã–≤–æ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ. 
–ò—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –ø–æ–ª—É—á–∏—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–æ—à–∫–∏ –∏ —Å–æ–±–∞–∫–∏ –∏ –æ—Ç—Ä–∏—Å—É–π—Ç–µ –∏—Ö. 
–í—ã–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—É, –æ–±—Ä–∞—Ç–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, —á—Ç–æ–±—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏.
          
6. –ù–∞–π–¥–∏—Ç–µ –≤ —Å–µ—Ç–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–æ—Ç–æ–≤ –∏ —Å–æ–±–∞–∫. 
–ò—Å–ø–æ–ª—å–∑—É—è –ª—é–±–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, CVAT), –≤—ã–¥–µ–ª–∏—Ç–µ –∫–æ—Ç–æ–≤ –∏ —Å–æ–±–∞–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. 
–í—Å—Ç–∞–≤—å—Ç–µ —Å–∫—Ä–∏–Ω—à–æ—Ç —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π. 
–ò—Å–ø–æ–ª—å–∑—É—è –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ä–∞–∑–º–µ—Ç–∫—É –∏ bounding boxes, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –º–æ–¥–µ–ª–∏.
          
7*. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏, –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å fasterrcnn_resnet50_fpn. 
–ó–∞–º–µ–Ω–∏—Ç–µ —Å–ª–æ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è bounding box –Ω–∞ FastRCNNPredictor —Å –Ω—É–∂–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–ª–∞—Å—Å–æ–≤.
''')
    
def obDet_code(st):
    if st == '1':
        print('''
import os
import xml.etree.ElementTree as ET #–º–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ö–º–ª
import torch
import torch.nn as nn
from torch.utils.data import Dataset #–∫–ª–∞—Å—Å –ø–∞–π—Ç–æ—Ä—á –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
from torchvision.models import resnet50
from torchvision import transforms as T
from PIL import Image
import matplotlib.pyplot as plt
import torchvision.transforms as T #–∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º
from torch.utils.data import DataLoader #–∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞—Ç—á–∏
import matplotlib.patches as patches #–º–æ–¥—É–ª—å –ø–∞—Ç—á–µ—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–±
from torchvision.transforms import ToPILImage #—Ç–µ–Ω–∑–æ—Ä –≤ –ø–∏–ª –∫–∞—Ä—Ç–∏–Ω–∫—É
import torchvision
from torchvision import models
from torchvision.transforms import ToTensor, Normalize, Compose
from torch.utils.data import DataLoader, random_split
from torchvision.transforms import Resize
import torch
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torch.utils.data import DataLoader, Dataset
from torchvision.transforms.functional import hflip, rotate
import zipfile
              

zip_file_path = '/content/Asirra- cat vs dogs 2.zip'
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall()
              

def parse_xml(file_path):

    #—á–∏—Ç–∞–µ–º —Ö–º–ª —Ñ–∞–π–ª —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—É—Ç–∏ —Ñ–∞–π–ª–∞
    #—Ä–µ–∑—É–ª—å—Ç–∞—Ç - ElementTree —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π —Ö–º–ª –¥–æ–∫–∞
    tree = ET.parse(file_path)

    root = tree.getroot() #–ø–æ–ª—É—á–∞–µ–º –∫–æ—Ä–Ω–µ–≤–æ–π —ç–ª–µ–º–µ–Ω—Ç



 #############################################
    #–†–ê–ó–ú–ï–†–´ –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø
    #size –≤–Ω—É—Ç—Ä–∏ –∫–æ—Ä–Ω–µ–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –∏—â–µ–º —Å–∞–π–∑
    size = root.find("size")

    #–Ω–∞—Ö–æ–¥–∏–º –≤–Ω—É—Ç—Ä–∏ —Å–∞–π–∑ –≤–∏–¥–∑ –∏ —Ö–µ–π—Ç –∏–∑–≤–ª–µ–∫–∞–µ–º –∏ –¥–µ–ª–∞–µ–º –∏–Ω—Ç–µ–¥–∂–µ—Ä–æ–º
    img_width = int(size.find("width").text)
    img_height = int(size.find("height").text)



############################################
    #BOUNDING BOX
    #–∏—â–µ–º –æ–±–¥–∂–µ–∫—Ç –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ—É –∫—Ç–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ (–∫–æ—Ç, —Å–æ–±–∞–∫–∞)
    obj = root.find("object")

    #–∏—â–µ–º –±–∞—É–Ω–¥–∏–Ω–≥ –±–æ–∫—Å –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ—É –æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö
    bndbox = obj.find("bndbox")

    #–ª–µ–≤—ã–π –≤–µ—Ä—Ö–Ω–∏–π –∏ –ø—Ä–∞–≤—ã–π –Ω–∏–∂–Ω–∏–π —É–≥–ª—ã –∏ –¥–µ–ª–∞–µ–º —Ñ–ª–æ–∞—Ç–æ–º
    xmin = float(bndbox.find("xmin").text)
    ymin = float(bndbox.find("ymin").text)
    xmax = float(bndbox.find("xmax").text)
    ymax = float(bndbox.find("ymax").text)



########################################
    #–ö–û–û–†–î–ò–ù–ê–¢–´
    #–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (–∏–Ω—Ñ–∞ –æ –±–∞—É–Ω–¥–∏–Ω–≥ –±–æ–∫—Å –≤–∑—è—Ç–∞—è –∏–∑ —Ö–º–ª)
    #–ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –±–± –Ω–∞ —Å–∞–º–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
    raw = {"xmin": xmin, "ymin": ymin, "xmax": xmax, "ymax": ymax}

    #–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1 [–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ/width or height])
    #–≤–∞–∂–Ω—ã, –ø–æ—Ç–æ–º—É —á—Ç–æ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö –∏ —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–º–µ–Ω—è–µ—Ç—Å—è
    #–º–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–∂–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ –æ–±—É—á–∏—Ç—å—Å—è (–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –æ–±—ä–µ–∫—Ç—ã) –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∞—Ö –¥—Ä—É–≥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    scaled = {
        "xmin": xmin / img_width,
        "ymin": ymin / img_height,
        "xmax": xmax / img_width,
        "ymax": ymax / img_height,
    }


######################################
    #–∏–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –æ–±—ä–µ–∫—Ç–∞
    obj_name = obj.find("name").text

    return {
        "raw": raw,
        "scaled": scaled,
        "obj_name": obj_name,
    }


######################################
xml_file_path = '/content/Asirra- cat vs dogs/cat.0.xml'
result = parse_xml(xml_file_path)
result
''')
    elif st == '2':
        print('''
class AnimalDetectionDataset(Dataset):
    def __init__(self, root_dir, transform=None, label_transform=None, return_scaled=False):

       #–ö–û–ù–°–¢–†–£–ö–¢–û–† –ö–õ–ê–°–°–ê
        self.root_dir = root_dir

        #–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –∫–ª–∞—Å—Å–∞ (transfrom, label_transform –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None)
        self.transform = transform
        self.label_transform = label_transform

        #return_scaled (True - –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, False - –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ [–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ñ–æ–ª—Å])
        self.return_scaled = return_scaled

        #–¥–ª—è –∫–∞–∂–¥–æ–≥ jpg –µ—Å—Ç—å —Å–≤–æ–π xml
        self.image_files = sorted(
            [f for f in os.listdir(root_dir) if f.endswith(".jpg")]
        )
        self.annotation_files = sorted(
            [f for f in os.listdir(root_dir) if f.endswith(".xml")]
        )
######################################################

    #–†–ê–ó–ú–ï–† –î–ê–¢–ê–°–ï–¢–ê
    def __len__(self):
        return len(self.image_files)

####################################################

    def __getitem__(self, idx):
      #–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–∏–Ω –æ–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –±–±, –ª—ç–π–±–ª) –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∏–Ω–¥–µ–∫—Å—É
        #–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img_path = os.path.join(self.root_dir, self.image_files[idx])
        image = Image.open(img_path).convert("RGB") #–∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–æ–±—Ä –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ –†–ì–ë

        #–∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
        xml_path = os.path.join(self.root_dir, self.annotation_files[idx])
        annotation = parse_xml(xml_path) #–≤—ã–¥–∞–µ—Ç –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –∫—Ç–æ –Ω–∞ –∫—Ä–∞—Ç–∏–Ω–∫–µ

        #–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–Ω–∞—á–µ–Ω–∏—è self.returned_scaled
        bbox = annotation["scaled"] if self.return_scaled else annotation["raw"]
        label = annotation["obj_name"]

        #–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è (transform)
        if self.transform:
            image = self.transform(image)
        if self.label_transform:
            label = self.label_transform(label)

        return image, bbox, label
              


transform = T.Compose([ #–∫–æ–º–ø–æ—É–∑ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –∏ –ø—Ä–∏–µ–Ω—è–µ—Ç –∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
    T.Resize((224, 224)),
    T.ToTensor() #–∏–∑ –ü–ò–õ –≤ —Ç–æ—Ä—á —Ç–µ–Ω–∑–æ—Ä
])

#—Ñ—É–Ω–∫—Ü–∏—è - –º–µ—Ç–∫–∞ –≤ —á–∏—Å–ª–æ (–∏–Ω–¥–µ–∫—Å)
def label_transform(label):
    label_map = {"cat": 0, "dog": 1}
    return label_map[label]
              

root_dir = "/content/Asirra: cat vs dogs"


dataset = AnimalDetectionDataset(
    root_dir=root_dir,
    transform=transform,
    label_transform=label_transform,
    return_scaled=True
)

len(dataset)
              

data_loader = DataLoader(dataset, batch_size=4, shuffle=True)

for images, bboxes, labels in data_loader:
    print("Batch of images shape:", images.shape)
    print("Bounding boxes:", bboxes)
    print("Labels:", labels)
    break
''')
    elif st == '3':
        print('''
def show_image_with_bounding_box(image, bbox, label):

    #–ü–†–û–í–ï–†–Ø–ï–ú –ö–õ–ê–°–°
    #isinstance –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–±—ä–µ–∫—Ç –æ—Ç–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∏–ª–∏ –Ω–µ—Ç
    if isinstance(image, torch.Tensor):
        image = ToPILImage()(image)

    fig, ax = plt.subplots(1)
    ax.imshow(image)

######################################
    #–î–û–ë–ê–í–õ–Ø–ï–ú –ë–ë
    rect = patches.Rectangle(
        (bbox['xmin'], bbox['ymin']), #–≤–µ—Ä—Ö–Ω–∏–π –ª–µ–≤—ã–π —É–≥–æ–ª , –±–µ—Ä–µ–º –∏–∑ —Ñ–∞–π–ª–∞ —Ö–º–ª
        bbox['xmax'] - bbox['xmin'],
        bbox['ymax'] - bbox['ymin'],
        linewidth=2, edgecolor='r', facecolor='none'
    )
    ax.add_patch(rect)

    #–ø–æ–¥–ø–∏—Å—å
    ax.text(
        bbox['xmin'], bbox['ymin'] - 10, label,
        color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.5)
    )

    plt.axis('off')
    plt.show()
              


dataset = AnimalDetectionDataset(
    root_dir='/content/Asirra: cat vs dogs',
    transform=None,
    label_transform=None,
    return_scaled=False #–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–µ –Ω—É–∂–Ω–æ –æ–±—É—á–∞—Ç—å
)

################################
cat_image, cat_bbox, cat_label = dataset[0]
show_image_with_bounding_box(cat_image, cat_bbox, cat_label)

dog_image, dog_bbox, dog_label = dataset[666]
show_image_with_bounding_box(dog_image, dog_bbox, dog_label)
''')
    elif st == '4':
        print('''
class TwoHeadedObjectDetectionModel(nn.Module):
    def __init__(self, num_classes=2):  # num_classes: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, cat –∏ dog)
        super(TwoHeadedObjectDetectionModel, self).__init__()

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ResNet50 –≤ –∫–∞—á–µ—Å—Ç–≤–µ backbone
        resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä 'weights'

        # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π fully connected —Å–ª–æ–π (fc)
        self.backbone = nn.Sequential(*list(resnet50.children())[:-2])

        # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–µ—Ç–∫–∏ –æ–±—ä–µ–∫—Ç–∞)
        self.classification_head = nn.Linear(2048 * 7 * 7, num_classes)

        # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç bounding box)
        self.bbox_head = nn.Linear(2048 * 7 * 7, 4)  # 4 –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bounding box (xmin, ymin, xmax, ymax)

        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–∞ –ø–µ—Ä–µ–¥ –ø–æ–¥–∞—á–µ–π –≤ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        self.flatten = nn.Flatten(start_dim=1)

    def forward(self, x):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Ö–æ–¥ —á–µ—Ä–µ–∑ backbone
        x = self.backbone(x)

        # –ü–µ—á–∞—Ç—å —Ä–∞–∑–º–µ—Ä–æ–≤ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ backbone
        print("Shape after backbone:", x.shape)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ (flatten) –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞
        x = self.flatten(x)

        # –ü–µ—á–∞—Ç—å —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–æ—Å–ª–µ flatten
        print("Shape after flatten:", x.shape)

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        class_preds = self.classification_head(x)

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è bounding box
        bbox_preds = self.bbox_head(x)

        return class_preds, bbox_preds

# –ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
model = TwoHeadedObjectDetectionModel(num_classes=2)  # –ù–∞–ø—Ä–∏–º–µ—Ä, 2 –∫–ª–∞—Å—Å–∞ (cat –∏ dog)

# –ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–∞ (–±–∞—Ç—á –∏–∑ 4 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä–∞–∑–º–µ—Ä–æ–º 3x224x224)
example_input = torch.randn(4, 3, 224, 224)

# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
class_preds, bbox_preds = model(example_input)

# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
print("Class predictions shape:", class_preds.shape)  # [batch_size, num_classes]
print("Bounding box predictions shape:", bbox_preds.shape)  # [batch_size, 4]
              


class ImprovedObjectDetectionModel(nn.Module):
    def __init__(self, num_classes=2): #–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä, –¥–≤–∞ –∫–ª–∞—Å—Å–∞ –∫–æ—à–∫–∞ —Å–æ–±–∞–∫–∞
        super(ImprovedObjectDetectionModel, self).__init__()

        #BACKBONE
        resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
        self.backbone = nn.Sequential(*list(resnet50.children())[:-2]) #–ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –∏—Å–ø –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏


##################################################
        #–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –°–í–ï–†–¢–û–ß–ù–´–ï –°–õ–û–ò
        #—É–º–µ–Ω—å—à–∞—é—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ + –∏–∑–≤–ª–µ–∫–∞—é—Ç –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        #kernel –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ * –Ω–∞ –ø–æ–¥–º–∞—Ç—Ä–∏—Ü—É –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
        #in_channels = 2048 - –∫–æ–ª-–≤–æ –∫–∞–Ω–∞–ª–æ–≤ –Ω–∞ –≤—ã—Ö–æ–¥–µ –±—ç–∫–±–æ–Ω
        self.extra_conv1 = nn.Conv2d(2048, 1024, kernel_size=3, stride=1, padding=1)

        self.extra_bn1 = nn.BatchNorm2d(1024)
        #–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞—Ç—á–µ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–∞–Ω–∞–ª—É (mean –∏ std)
        #—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π (–∑–Ω–∞—á–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Ü–µ–Ω—Ç—Ä–∏—Ä—É—é—Ç—Å—è –≤–æ–∫—Ä—É–≥ –Ω—É–ª—è[activation - mean_channel])
        #–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (activation - mean_channel)/std_channel , –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç act => var = 0
        #–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–¥–≤–∏–≥ (–ø–æ—Å–ª–µ normalization act –º–∞—Å—à—Ç–∞–± —Å gamma –∏ —Å–¥–≤–∏–≥ —Å beta), –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –Ω–∞ –æ–ø—Ç –º–∞—Å—à—Ç –∏ —Å–¥–≤–∏–≥—É –Ω–∞ –∫–∞–∂–¥–æ–º channel

        self.extra_conv2 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)

        self.extra_bn2 = nn.BatchNorm2d(512)

        self.extra_conv3 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)

        self.extra_bn3 = nn.BatchNorm2d(256)


###################################################
        #GLOBAL AVERAGE POOLING
        #–∫–∞—Ä—Ç–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ç–µ–Ω–∑–æ—Ä –¥–ª—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è
        #AdaptiveAvgPool2d –≤—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–Ω–µ–µ –ø–æ width –∏ height
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.flatten = nn.Flatten()


###################################################
        #CLASSIFICATION HEAD
        self.classification_head = nn.Sequential(
            #y = xw + b (w = 512 * 256, b = 512 * 1)
            nn.Linear(256, 512),

            #inplace = True - –Ω–µ —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä –¥–ª—è –∑–∞–ø–∏—Å–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(512),
            #–∑–∞–Ω—É–ª—è–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )


##################################################
        #REGRESSION HEAD
        self.bbox_head = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(512),
            nn.Dropout(0.5),

            #4 —Ç–∫ xmin, ymin, xmax, ymax
            nn.Linear(512, 4)
        )



################################################
    #FORWARD PASS
    def forward(self, x):

        x = self.backbone(x)

        #–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏
        x = nn.ReLU(inplace=True)(self.extra_bn1(self.extra_conv1(x)))
        x = nn.ReLU(inplace=True)(self.extra_bn2(self.extra_conv2(x)))
        x = nn.ReLU(inplace=True)(self.extra_bn3(self.extra_conv3(x)))

        #GA + flatten
        x = self.global_pool(x)
        x = self.flatten(x)

        #result
        class_preds = self.classification_head(x)
        bbox_preds = self.bbox_head(x)
        return class_preds, bbox_preds

###############################################
    #–§–£–ù–ö–¶–ò–Ø –ü–û–¢–ï–†–¨
    def compute_loss(self, class_preds, bbox_preds, labels, bboxes):
        #–∫—Ä–æ—Å—Å —ç–Ω—Ç—Ä–æ–ø–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        criterion_class = nn.CrossEntropyLoss()
        loss_class = criterion_class(class_preds, labels)

        #—Å–º—É—Å1 –ª–æ—Å –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        criterion_bbox = nn.SmoothL1Loss()
        loss_bbox = criterion_bbox(bbox_preds, bboxes)

        #–æ–±—â–∞—è –ø–æ—Ç–µ—Ä—è
        return loss_class + loss_bbox


#–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –±–∞—Ç—á–µ, CrossEntropyLoss –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫: -sum(yi) * logpi
#smoothloss
# —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –¥–ª—è –∑–∞–¥–∞—á —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
#(–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è bounding box –≤ –∑–∞–¥–∞—á–∞—Ö –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤). –û–Ω–∞ —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ –ª—É—á—à–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ L1 –∏ L2 –ø–æ—Ç–µ—Ä—å, —á—Ç–æ –¥–µ–ª–∞–µ—Ç
#–µ—ë –º–µ–Ω–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–π –∫ –≤—ã–±—Ä–æ—Å–∞–º, —á–µ–º L2 Loss (—Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞).
#loss = 0.5 * x^2    if |x| < 1
#loss = |x| - 0.5    if |x| >= 1
''')
    elif st == '5':
        print('''
transform = Compose([
    Resize((224, 224)),
    ToTensor(),
    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
])


##############################################
root_dir = "/content/Asirra- cat vs dogs"
dataset = AnimalDetectionDataset(root_dir=root_dir, transform=transform, return_scaled=True)


##############################################
#–æ–±—É—á–∞—é—â–∞—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])


##############################################
#DataLoader
#collate_fn=lambda x: tuple(zip(*x) –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))
              


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ImprovedObjectDetectionModel(num_classes=2).to(device)


###################################################
#–∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —Å–ª–æ–∏ backbone, —Ç–∞–∫ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞
for param in model.backbone.parameters():
    param.requires_grad = False


###################################################
#–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


###################################################
#—Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
criterion_class = nn.CrossEntropyLoss()
criterion_bbox = nn.MSELoss()
              


model
              


for name, param in model.state_dict().items():
    print(f"{name}: {param.shape}")
              

#–≤–µ—Å–∞ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –º–æ–¥–µ–ª–∏
print(model.state_dict()['backbone.0.weight'])
              

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏
model = ImprovedObjectDetectionModel()

# –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è
def get_num_classes(model):
    # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –≤ classification_head –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å Linear
    last_layer = list(model.classification_head.children())[-1]
    if isinstance(last_layer, nn.Linear):
        return last_layer.out_features
    else:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –Ω–µ —è–≤–ª—è–µ—Ç—Å—è nn.Linear")

num_classes = get_num_classes(model)
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {num_classes}")
              

label_map = {"cat": 0, "dog": 1}  # –ü—Ä–∏–º–µ—Ä –º–∞–ø–ø–∏–Ω–≥–∞: cat -> 0, dog -> 1
              

def train_epoch(model, loader, optimizer, device):
    #–ú–û–î–ï–õ–¨ –í –†–ï–ñ–ò–ú –û–ë–£–ß–ï–ù–ò–Ø
    #drop out —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –æ—Ç–∫–ª—é—á–∞–µ—Ç –Ω–µ–π—Ä–æ–Ω—ã
    #batchnorm –≤—ã—á–∏—Å–ª—è–µ—Ç mean –∏ std –∞–∫—Ç–∏–≤–∞—Ü–∏–π –ø–æ –±–∞—Ç—á—É + –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ—Ç mean –∏ var –ø–æ –≤—Å–µ–º –±–∞—Ç—á–∞–º
    model.train()
    epoch_loss = 0
    for images, bboxes, labels in loader:
        #–æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–Ω–∑–æ—Ä—ã) –≤ –æ–¥–∏–Ω —Ç–µ–Ω–∑–æ—Ä -> –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        images = torch.stack(images).to(device)

        #—Å–ª–æ–≤–∞—Ä–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤ —Ç–µ–Ω–∑–æ—Ä -> –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        bboxes = torch.tensor([list(b.values()) for b in bboxes]).to(device)

        #—Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–µ—Ç–æ–∫ –≤ —Ç–µ–Ω–∑–æ—Ä —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        labels = torch.tensor([label_map[label] for label in labels]).to(device)

        #–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –º–æ–¥–µ–ª—å => preds
        class_preds, bbox_preds = model(images)

        #–ø–æ—Ç–µ—Ä—è
        loss_class = criterion_class(class_preds, labels)
        loss_bbox = criterion_bbox(bbox_preds, bboxes)
        loss = loss_class + loss_bbox

        #—à–∞–≥
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    return epoch_loss / len(loader) #—Å—Ä–µ–¥–Ω—è—è –ø–æ—Ç–µ—Ä—è –∑–∞ —ç–ø–æ—Ö—É


############################################################
#–ú–û–î–ï–õ–¨ –í –†–ï–ñ–ò–ú –í–ê–õ–ò–î–ê–¶–ò–ò
#drop out –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è, –≤—Å–µ –Ω–µ–π—Ä–æ–Ω—ã —É—á–∞—Å—Ç–≤—É—é—Ç
#batchnorm –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ std –∏ var –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –Ω–æ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ (—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–º–µ—à—â–µ–Ω–∏–µ)
def validate_epoch(model, loader, device):
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for images, bboxes, labels in loader:

            images = torch.stack(images).to(device)
            bboxes = torch.tensor([list(b.values()) for b in bboxes]).to(device)

            labels = torch.tensor([label_map[label] for label in labels]).to(device)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫ –≤ —á–∏—Å–ª–∞

            class_preds, bbox_preds = model(images)

            loss_class = criterion_class(class_preds, labels)
            loss_bbox = criterion_bbox(bbox_preds, bboxes)
            loss = loss_class + loss_bbox
            epoch_loss += loss.item()

    return epoch_loss / len(loader)


############################################################
num_epochs = 50
for epoch in range(num_epochs):
    train_loss = train_epoch(model, train_loader, optimizer, device)
    val_loss = validate_epoch(model, val_loader, device)
    print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
              

#–î–ï–ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô
#–æ–±—Ä–∞—Ç–Ω–æ–µ —Ñ–æ—Ä–º—É–ª–µ normalized_image = (image - mean) / std –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞
def denormalize_image(image, mean, std):
    for c in range(3):
        image[c] = image[c] * std[c] + mean[c]
    return image


############################################################
#–û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô
def show_image(image):
    image = denormalize_image(image, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
    #–¥–µ–ª–∞–µ–º —Ñ–æ—Ä–º–∞—Ç h w c –¥–ª—è matplotlib , pytorch c h w
    plt.imshow(image.permute(1, 2, 0).numpy())
    plt.axis('off')
    plt.show()
              

import matplotlib.pyplot as plt
import matplotlib.patches as patches
import torch

# –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è bounding box (–æ–±—Ä–∞—Ç–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏)
def denormalize_bbox(bbox, width, height):
    """–î–µ–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç bounding box –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∏—Ä–∏–Ω—ã –∏ –≤—ã—Å–æ—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    x_min, y_min, x_max, y_max = bbox
    x_min = x_min * width
    y_min = y_min * height
    x_max = x_max * width
    y_max = y_max * height
    return [x_min, y_min, x_max, y_max]

# –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
def denormalize_image(image, mean, std):
    for c in range(3):
        image[c] = image[c] * std[c] + mean[c]
    return image

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
def show_image(image):
    image = denormalize_image(image, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
    # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç (c, h, w) –≤ (h, w, c) –¥–ª—è matplotlib
    plt.imshow(image.permute(1, 2, 0).numpy())
    plt.axis('off')
    plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏
def visualize_predictions(model, loader, device, num_examples=10):
    model.eval()
    class_map = {0: 'cat', 1: 'dog'}

    fig, axes = plt.subplots(5, 2, figsize=(16, 20))
    axes = axes.flatten()
    images_shown = 0

    with torch.no_grad():
        for images, bboxes, labels in loader:
            images = torch.stack(images).to(device)
            bboxes = torch.tensor([list(b.values()) for b in bboxes]).to(device)
            labels = torch.tensor([label_map[label] for label in labels]).to(device)

            class_preds, bbox_preds = model(images)

            images = images.cpu()
            bboxes = bboxes.cpu()
            labels = labels.cpu()
            class_preds = class_preds.cpu()
            bbox_preds = bbox_preds.cpu()

            for i in range(images.size(0)):
                if images_shown >= num_examples:
                    break

                ax = axes[images_shown]

                image = images[i].permute(1, 2, 0).numpy()
                true_bbox = bboxes[i].numpy()
                true_label = labels[i].item()
                pred_bbox = bbox_preds[i].numpy()
                pred_label_idx = torch.argmax(class_preds[i]).item()
                pred_label = class_map[pred_label_idx]
                true_label_str = class_map[true_label]

                # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                height, width = image.shape[:2]

                # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º bounding boxes
                true_bbox = denormalize_bbox(true_bbox, width, height)
                pred_bbox = denormalize_bbox(pred_bbox, width, height)

                ax.imshow(image)

                # –†–∏—Å—É–µ–º bounding box –¥–ª—è –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                rect_true = patches.Rectangle(
                    (true_bbox[0], true_bbox[1]), true_bbox[2] - true_bbox[0],
                    true_bbox[3] - true_bbox[1], linewidth=2, edgecolor='green', facecolor='none'
                )
                ax.add_patch(rect_true)

                # –†–∏—Å—É–µ–º bounding box –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                rect_pred = patches.Rectangle(
                    (pred_bbox[0], pred_bbox[1]), pred_bbox[2] - pred_bbox[0],
                    pred_bbox[3] - pred_bbox[1], linewidth=2, edgecolor='red', facecolor='none'
                )
                ax.add_patch(rect_pred)

                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏
                ax.text(
                    true_bbox[0], true_bbox[1] - 10,
                    f"True: {true_label_str}", color='green', fontsize=10,
                    bbox=dict(facecolor='white', alpha=0.5, edgecolor='none')
                )
                ax.text(
                    pred_bbox[0], pred_bbox[1] - 10,
                    f"Pred: {pred_label}", color='red', fontsize=10,
                    bbox=dict(facecolor='white', alpha=0.5, edgecolor='none')
                )

                ax.axis('off')
                images_shown += 1

            if images_shown >= num_examples:
                break

        plt.tight_layout()
        plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
visualize_predictions(model, val_loader, device, num_examples=10)
''')
    elif st == '6':
        print('''
zip_file_path = '/content/labels_dl 2.zip'
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall()
              
xml_file_path = '/content/labels_dl 2/cat2.xml'
result = parse_xml(xml_file_path)
result
              

transform = T.Compose([
    T.Resize((224, 224)),
    T.ToTensor()
])


############################################
root_dir = "/content/labels_dl 2"


############################################
dataset = AnimalDetectionDataset(
    root_dir=root_dir,
    transform=transform,
    label_transform=label_transform,
    return_scaled=False
)

len(dataset)
              


data_loader = DataLoader(dataset, batch_size=4, shuffle=True)


######################################################
for images, bboxes, labels in data_loader:
    print("Batch of images shape:", images.shape)
    print("Bounding boxes:", bboxes)
    print("Labels:", labels)
    break
              


dataset = AnimalDetectionDataset(
    root_dir="/content/labels_dl 2",
    transform=None,
    label_transform=None,
    return_scaled=False
)


##########################################################
cat_image, cat_bbox, cat_label = dataset[0]
show_image_with_bounding_box(cat_image, cat_bbox, cat_label)


dog_image, dog_bbox, dog_label = dataset[-1]
show_image_with_bounding_box(dog_image, dog_bbox, dog_label)
''')
    elif st == '7':
        print('''
# –ü–∞—Ä—Å–∏–Ω–≥ XML-–∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
def parse_xml(file_path):
    tree = ET.parse(file_path)
    root = tree.getroot()
    bboxes = []
    labels = []
    for obj in root.findall("object"):
        name = obj.find("name").text.strip()
        bbox = obj.find("bndbox")
        xmin = float(bbox.find("xmin").text)
        ymin = float(bbox.find("ymin").text)
        xmax = float(bbox.find("xmax").text)
        ymax = float(bbox.find("ymax").text)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ bounding box
        if xmax > xmin and ymax > ymin:
            bboxes.append([xmin, ymin, xmax, ymax])
            labels.append(name)
    return {"bboxes": bboxes, "labels": labels}

# –ö–ª–∞—Å—Å –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è Faster R-CNN
class AnimalDetectionDataset(Dataset):
    def __init__(self, root_dir, transform=None, augment=False):
        self.root_dir = root_dir
        self.image_files = sorted([f for f in os.listdir(root_dir) if f.endswith(".png")])
        self.annotation_files = sorted([f for f in os.listdir(root_dir) if f.endswith(".xml")])
        self.transform = transform
        self.augment = augment
        self.label_map = {"cat": 1, "dog": 2}  # 1: –∫–æ—à–∫–∞, 2: —Å–æ–±–∞–∫–∞

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = os.path.join(self.root_dir, self.image_files[idx])
        xml_path = os.path.join(self.root_dir, self.annotation_files[idx])

        image = Image.open(img_path).convert("RGB")
        annotation = parse_xml(xml_path)
        bboxes = torch.tensor(annotation["bboxes"], dtype=torch.float32)
        labels = torch.tensor([self.label_map[label] for label in annotation["labels"]], dtype=torch.int64)

        if self.augment:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–æ—Å—Ç—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            if torch.rand(1).item() > 0.5:
                image = hflip(image)
                bboxes[:, [0, 2]] = image.width - bboxes[:, [2, 0]]  # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –±–æ–∫—Å–æ–≤
            if torch.rand(1).item() > 0.5:
                angle = torch.randint(-30, 30, (1,)).item()
                image = rotate(image, angle)

        if self.transform:
            image = self.transform(image)

        target = {"boxes": bboxes, "labels": labels}
        return image, target

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
transform = T.Compose([
    T.ToTensor(),
])

# –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
root_dir = "/content/labels_dl 2"
dataset = AnimalDetectionDataset(root_dir=root_dir, transform=transform, augment=True)
data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ Faster R-CNN
num_classes = 3  # –ö–æ—à–∫–∞, —Å–æ–±–∞–∫–∞ –∏ —Ñ–æ–Ω
model = fasterrcnn_resnet50_fpn(pretrained=True)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

epoch_losses = []
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0
    for images, targets in data_loader:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        optimizer.zero_grad()
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())
        losses.backward()
        optimizer.step()

        epoch_loss += losses.item()

    epoch_losses.append(epoch_loss)
    print(f"Epoch {epoch + 1}/{num_epochs}, Total Loss: {epoch_loss:.4f}, Components: {loss_dict}")

# –ì—Ä–∞—Ñ–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
def visualize_predictions(model, dataset, idx):
    model.eval()
    image, target = dataset[idx]
    with torch.no_grad():
        prediction = model([image.to(device)])[0]

    image = image.permute(1, 2, 0).cpu().numpy()
    plt.imshow(image)
    ax = plt.gca()

    for bbox, label in zip(target["boxes"], target["labels"]):
        xmin, ymin, xmax, ymax = bbox.tolist()
        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
                                 linewidth=2, edgecolor="r", facecolor="none")
        ax.add_patch(rect)
        ax.text(xmin, ymin - 10, f"True: {label.item()}", color="red", fontsize=12)

    for bbox, label, score in zip(prediction["boxes"], prediction["labels"], prediction["scores"]):
        if score > 0.5:
            xmin, ymin, xmax, ymax = bbox.tolist()
            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
                                     linewidth=2, edgecolor="blue", facecolor="none", linestyle="--")
            ax.add_patch(rect)
            ax.text(xmin, ymax + 10, f"Pred: {label.item()} ({score:.2f})", color="blue", fontsize=12)

    plt.axis("off")
    plt.show()

# –ü—Ä–∏–º–µ—Ä –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
visualize_predictions(model, dataset, idx=0)
''')
    else:
        print('no such index')
def imSeg_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.imSeg_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")    

1. –û–ø–∏—à–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç ClothesSegmentationDataset. –†–µ–∞–ª–∏–∑—É–π—Ç–µ __getitem__ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –æ–Ω –≤–æ–∑–≤—Ä–∞—â–∞–ª –¥–≤–∞ —ç–ª–µ–º–µ–Ω—Ç–∞: —Ç–µ–Ω–∑–æ—Ä —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∏ —Ç–µ–Ω–∑–æ—Ä —Å –º–∞—Å–∫–æ–π. 
–ú–∞—Å–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã–º —Ç–µ–Ω–∑–æ—Ä–æ–º —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª. 
–ü—Ä–µ–¥—É—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∏–∑–≤–Ω–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞–±–æ—Ä –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–∞—Å–æ–∫. 
–°–æ–∑–¥–∞–π—Ç–µ –æ–±—ä–µ–∫—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –≤—ã–≤–µ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω —Ñ–æ—Ä–º—É –∏ —Ç–∏–ø—ã –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –µ–≥–æ –º–∞—Å–∫–∏.   
          
2. –ù–∞–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é show_image_with_mask, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–≤–æ–¥–∏—Ç —Ä—è–¥–æ–º –¥–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é –∏ –º–∞—Å–∫—É. 
–ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–π—Ç–µ —Ä–∞–±–æ—Ç—É —Ñ—É–Ω–∫—Ü–∏–∏, –≤–∑—è–≤ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∏–∑ —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞.
          
3. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É U-Net. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –Ω–∞ –≤—ã—Ö–æ–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–ª—É—á–∞–ª—Å—è 
—Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ n_classes x h x w, –≥–¥–µ n_classes - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –º–∞—Å–∫–∞—Ö, –∞ h –∏ w - —Ä–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. 
–í–æ–∑—å–º–∏—Ç–µ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∏–∑ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ –µ–≥–æ —á–µ—Ä–µ–∑ —Å–µ—Ç—å. –í—ã–≤–µ–¥–∏—Ç–µ —Ñ–æ—Ä–º—É –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –Ω–∞ —ç–∫—Ä–∞–Ω.
          
4. –†–∞–∑–±–µ–π—Ç–µ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–∞—é—â–µ–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ. –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å U-Net –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. 
–í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤—ã–≤–æ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤—ã–±–æ—Ä–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π. –ü—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –ª—é–±—É—é –∏–∑–≤–µ—Å—Ç–Ω—É—é –≤–∞–º —Ç–µ—Ö–Ω–∏–∫—É –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤—ã–±–æ—Ä–∫–∞–º–∏.
–ü—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, —É–º–µ–Ω—å—à–∞—é—â–∏–µ —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è).
–ò—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –ø–æ–ª—É—á–∏—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –æ—Ç—Ä–∏—Å—É–π—Ç–µ –∏—Ö.
          
5. –û–±—É—á–∏—Ç–µ –º–æ–¥—É–ª—å SegformerForSemanticSegmentation –∏–∑ –ø–∞–∫–µ—Ç–∞ transformers –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. 
–í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤—ã–≤–æ–¥–∏—Ç–µ –Ω–∞ —ç–∫—Ä–∞–Ω –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ.
–î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –∫–æ—Ç–æ—Ä–æ–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–∞–º –º–æ–¥–µ–ª—å.
–ò—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –ø–æ–ª—É—á–∏—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –æ—Ç—Ä–∏—Å—É–π—Ç–µ –∏—Ö.
''')
def imSeg_code(st):
    if st == '1':
        print('''
import os
from PIL import Image
import torch
from torch.utils.data import Dataset
from torchvision import transforms
import kagglehub
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
from torch.utils.data import Dataset
import numpy as np
import torch
from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor
from torch.utils.data import DataLoader, Dataset
from torchvision.transforms import Compose, ToTensor, Normalize
import numpy as np
from torch.optim import AdamW
from tqdm import tqdm
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Subset
import torchvision
              

import kagglehub

# Download latest version
path = kagglehub.dataset_download("rajkumarl/people-clothing-segmentation")

print("Path to dataset files:", path)
              

print("–§–∞–π–ª—ã –∏ –ø–∞–ø–∫–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏:", os.listdir(path))
              

print("–§–∞–π–ª—ã –≤ png_images:", os.listdir(os.path.join(path, "png_images"))[:5])
print("–§–∞–π–ª—ã –≤ png_masks:", os.listdir(os.path.join(path, "png_masks"))[:5])
              

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–æ–∫ IMAGES –∏ MASKS
images_subdir = os.path.join(path, "png_images", "IMAGES")
masks_subdir = os.path.join(path, "png_masks", "MASKS")

print("–§–∞–π–ª—ã –≤ IMAGES:", os.listdir(images_subdir)[:5])
print("–§–∞–π–ª—ã –≤ MASKS:", os.listdir(masks_subdir)[:5])
              

path = kagglehub.dataset_download("rajkumarl/people-clothing-segmentation")
print("Path to dataset files:", path)

# –ü—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –∏ –º–∞—Å–∫–∞–º
image_dir = os.path.join(path, "png_images/IMAGES")
mask_dir = os.path.join(path, "png_masks/MASKS")
              


class ClothesSegmentationDataset(Dataset):
  #–ö–û–ù–°–¢–†–£–ö–¢–û–† –ö–õ–ê–°–°–ê
    def __init__(self, image_dir, mask_dir, image_transform=None, mask_transform=None):

        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.image_transform = image_transform
        self.mask_transform = mask_transform

        #–°–û–û–¢–ù–û–®–ï–ù–ò–ï –ú–ï–ñ–î–£ –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø–ú–ò –ò –ú–ê–°–ö–ê–ú–ò –ü–û –ò–ù–î–ï–ö–°–ê–ú
        self.images = sorted(os.listdir(image_dir))
        self.masks = sorted(os.listdir(mask_dir))

        #–ø—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–∞—Å–æ–∫ —Å–æ–≤–ø–∞–¥–∞–µ—Ç
        assert len(self.images) == len(self.masks), "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–∞—Å–æ–∫ –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å"


##############################################################
#–†–ê–ó–ú–ï–† –î–ê–¢–ê–°–ï–¢–ê
    def __len__(self):
        return len(self.images)


################################################################
#–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É –∏ –º–∞—Å–∫—É
    def __getitem__(self, idx):
        #–Ω–∞—Ö–æ–¥–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –º–∞—Å–∫—É –ø–æ –ø—É—Ç–∏ –∏ –∏–Ω–¥–µ–∫—Å—É
        image_path = os.path.join(self.image_dir, self.images[idx])
        mask_path = os.path.join(self.mask_dir, self.masks[idx])


        #–æ—Ç–∫—Ä—ã–≤–∞–µ–º + —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ –†–ì–ë(—Ç–æ–ª—å–∫–æ –∫–∞—Ä—Ç–∏–Ω–∫—É)
        image = Image.open(image_path).convert("RGB")
        mask = Image.open(mask_path)


        #–ø—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        if self.image_transform:
            image = self.image_transform(image)
        if self.mask_transform:
            mask = self.mask_transform(mask)

        mask = torch.from_numpy(np.array(mask)).long() #–º–∞—Å–∫–∞ –≤ —Ç–µ–Ω–∑–æ—Ä

        return image, mask
              

#–¢–†–ê–ù–°–§–û–†–ú –î–õ–Ø –ò–ó–û–ë–†
image_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    #–ø–∏–ª –∏–∑–æ–±—Ä –≤ —Ç–µ–Ω–∑–æ—Ä
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.2, 0.2, 0.2]),
])

mask_transform = transforms.Compose([
    #interpolation –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞,
    #–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –ø–∏–∫—Å–µ–ª–∏ , –ø—Ä–∏–Ω–∏–º–∞—é –∑–Ω–∞—á–µ–Ω–∏—è 0 –∏–ª–∏ 1
    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST),
])


###################################################################

dataset = ClothesSegmentationDataset(image_dir, mask_dir, image_transform, mask_transform)


image, mask = dataset[0]
print(f"–§–æ—Ä–º–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image.shape}, —Ç–∏–ø: {image.dtype}")
print(f"–§–æ—Ä–º–∞ –º–∞—Å–∫–∏: {mask.shape}, —Ç–∏–ø: {mask.dtype}")
''')
    elif st == '2':
        print('''
def show_image_with_mask(image, mask):

    #–∏–∑–º–µ–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ —Å —Å h w –¥–ª—è torch –Ω–∞ h w c –¥–ª—è matplotlib
    image = image.permute(1, 2, 0).cpu().numpy()
    mask = mask.cpu().numpy()


    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    axes[0].imshow(image)
    axes[0].set_title('Image')
    axes[0].axis('off')

    axes[1].imshow(mask, cmap='jet')
    axes[1].set_title('Mask')
    axes[1].axis('off')

    plt.show()


################################################
image, mask = dataset[7]
show_image_with_mask(image, mask)
''')
    elif st =='3':
        print('''
def crop_tensor(tensor, target_tensor):
    _, _, h, w = target_tensor.shape
    tensor = torchvision.transforms.functional.center_crop(tensor, (h, w))
    return tensor
              

class UNet(nn.Module):
    def __init__(self, in_channels, n_classes):
        super(UNet, self).__init__()

        #ENCODER
        #in_channels = 3 , –¥–ª—è rgb
        self.enc1 = self.conv_block(in_channels, 64)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.enc2 = self.conv_block(64, 128)
        self.pool2 = nn.MaxPool2d(2, 2)

        self.enc3 = self.conv_block(128, 256)
        self.pool3 = nn.MaxPool2d(2, 2)



##################################################################
        #DECODER
        #nn.ConvTranspose2d —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç—Å—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        #kernel_size = 2, stride=2 —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–¥–≤–æ–µ
        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec3 = self.conv_block(256, 128)

        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec2 = self.conv_block(128, 64)

        self.final_conv = nn.Conv2d(64, n_classes, kernel_size=1)


#######################################################################
    #–°–í–ï–†–¢–û–ß–ù–´–ï –°–õ–û–ò
    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
        )


######################################################################
    #FORWARD PASS
    def forward(self, x):
      # encoder
      #—Ö —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤ —Å —Ä–µ–ª—É
      x1 = self.enc1(x)
      x1_pooled = self.pool1(x1)

      x2 = self.enc2(x1_pooled)
      x2_pooled = self.pool2(x2)

      #–≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ, –Ω–æ –Ω–∏–∑–∫–æ—Ä–∞–∑–º–µ—Ä–Ω–æ–µ —Ä–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
      x3 = self.enc3(x2_pooled)
      x3_pooled = self.pool3(x3)

      #decoder —Å–æ skip connections
      #upconv —É–¥–≤–∞–∏–≤–∞–µ—Ç h –∏ w –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
      #decoder —Å–æ skip connections
      x3_up = self.upconv3(x3_pooled)
      x3_up_cropped = crop_tensor(x3_up, x2)  # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
      x3_cat = torch.cat((x3_up_cropped, x2), dim=1)
      x3_decoded = self.dec3(x3_cat)

      x2_up = self.upconv2(x3_decoded)
      x2_up_cropped = crop_tensor(x2_up, x1)  # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
      x2_cat = torch.cat((x2_up_cropped, x1), dim=1)
      x2_decoded = self.dec2(x2_cat)



      #–ø—Ä–∏–º–µ–Ω—è–µ–º —è–¥—Ä–æ 1*1 —á—Ç–æ–± –ø–æ–ª—É—á–∏—Ç—å –∫–∞—Ä—Ç—É —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
      output = self.final_conv(x2_decoded)
      return output


##############################################################################
in_channels = 3
n_classes = 59  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
model = UNet(in_channels=in_channels, n_classes=n_classes)


############################################################################
#unsqueeze(0) –¥–æ–±–∞–≤–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
image = dataset[0][0].unsqueeze(0)
output = model(image)
print("–§–æ—Ä–º–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞:", output.shape)  # –û–∂–∏–¥–∞–µ–º—ã–π: [1, n_classes, h, w]
''')
    elif st == '4':
        print('''
#–¥–µ–ª–∏–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤—É—é –∏ –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫–∏
indices = list(range(len(dataset)))
#test_size=0.2 20% –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)

#subset –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω–¥–µ–∫—Å—ã
train_subset = Subset(dataset, train_indices)
val_subset = Subset(dataset, val_indices)
              

train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)
              

import torch

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ, –∏–Ω–∞—á–µ CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
              

#–§–£–ù–ö–¶–ò–Ø –ü–û–¢–ï–†–¨ –ò –û–ü–¢–ò–ú–ò–ó–ê–¢–û–†
#–ø–µ—Ä–≤—ã–π –≤–µ—Å –∑–∞–¥–∞–Ω 0.1 (–Ω–∞–ø—Ä–∏–º–µ—Ä —Ñ–æ–Ω), –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è —Å–ª—É—á–∞–π–Ω–æ [2,3]
class_weights = torch.tensor([0.1] + [torch.rand(1).item() * (3 - 1) + 2 for _ in range(58)]).to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

#–ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤–µ—Å–æ–≤ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω: 0.1. –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ –∫–ª–∞—Å—Å —Å –∏–Ω–¥–µ–∫—Å–æ–º 0
#—Å–∏–ª—å–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ñ–æ–Ω), –∏ –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–µ —É–º–µ–Ω—å—à–∞—é—Ç, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –Ω–µ —É–¥–µ–ª—è–ª–∞ –µ–º—É —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è.
#–î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö 58 –∫–ª–∞—Å—Å–æ–≤ –≤–µ—Å–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è —Å–ª—É—á–∞–π–Ω–æ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 2 –¥–æ 3

#–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (–ª–æ–≥–∏—Ç—ã –º–æ–¥–µ–ª–∏) –∏ –∏—Å—Ç–∏–Ω–Ω—ã–º –∫–ª–∞—Å—Å–æ–º.
#–í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤, —á—Ç–æ–±—ã —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å –¥–∞–Ω–Ω—ã—Ö.
              

class_weights
              

model
              

# –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –º–æ–¥–µ–ª–∏
num_classes = model.final_conv.out_channels
print("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –≤ –º–æ–¥–µ–ª–∏:", num_classes)
              

#INTERSE–°TION OVER UNION
#–Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ segmentation mask pred ture mask –≤ –∑–∞–¥–∞—á–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏


#–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ/–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
#–ø–∏–∫—Å–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∞ –∫–∞–∫ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—â–∏–µ –∫–ª–∞—Å—Å—É i /
#–≤—Å–µ –ø–∏–∫—Å–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç –ª–∏–±–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–º—É –∫–ª–∞—Å—Å—É i, –ª–∏–±–æ —Ä–µ–∞–ª—å–Ω–æ–º—É –∫–ª–∞—Å—Å—É i
#–ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∏–¥–µ–∞–ª—å–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ–±—ä–µ–∫—Ç, —Ç–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç, –∏ IoU=1. –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–ª–æ—Ö–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç, IoU –±—É–¥–µ—Ç –º–µ–Ω—å—à–µ

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ IoU
def iou(pred, target, n_classes):
    iou_list = []
    pred = pred.view(-1)
    target = target.view(-1)
    for i in range(n_classes):
        intersection = ((pred == i) & (target == i)).sum().item()
        union = ((pred == i) | (target == i)).sum().item()
        iou_list.append(intersection / union if union != 0 else float('nan'))
    return iou_list
              


# –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ü–∏–∫–ª

#–æ–±—É—á–∞—é—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (–≤–µ—Å–∞ –∏ —Å–º–µ—â–µ–Ω–∏—è –≤ —Å–ª–æ—è—Ö). –û–Ω–∏ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –ø—Ä–∏ –ø–æ–º–æ—â–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ (—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —á–µ—Ä–µ–∑ optimizer.step())
#–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞—é—Ç—Å—è –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ –º–æ–¥–µ–ª–∏ UNet (–Ω–∞–ø—Ä–∏–º–µ—Ä, nn.Conv2d, nn.ReLU, nn.MaxPool2d)
num_epochs = 200
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    train_iou = []
    #—Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–∏–∫—Å–µ–ª–µ–π –∏ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∏–∫—Å–µ–ª–µ–π –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    correct_train = 0
    total_train = 0
    for images, masks in train_loader:
        images, masks = images.to(device), masks.to(device)


#–ü–µ—Ä–µ–¥ –æ–±—Ä–∞—Ç–Ω—ã–º –ø—Ä–æ—Ö–æ–¥–æ–º (backpropagation) –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ–±–Ω—É–ª—è—é—Ç—Å—è, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∏—Ö –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è
        optimizer.zero_grad()


#–í—Ö–æ–¥: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ train_loader. –í—ã—Ö–æ–¥: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (outputs)
        outputs = model(images)

        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

        # IoU

#–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è –±–µ—Ä–µ–º –∏–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Å–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é. –≠—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
#–°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫—É IoU –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ —Å –ø–æ–º–æ—â—å—é –∑–∞—Ä–∞–Ω–µ–µ –Ω–∞–ø–∏—Å–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
        pred = torch.argmax(outputs, dim=1)
        iou_values = iou(pred, masks, n_classes)
        train_iou.append(iou_values)

        correct_train += (pred == masks).sum().item()
        total_train += masks.numel()

    avg_train_loss = train_loss / len(train_loader)
    avg_train_iou = [sum(iou_val) / len(iou_val) for iou_val in zip(*train_iou)]
    train_accuracy = correct_train / total_train

    print(f"Epoch [{epoch+1}/{num_epochs}]")
    print(f"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}")
    print(f"Train IoU: {avg_train_iou}")
              

model = UNet(in_channels=in_channels, n_classes=n_classes).to(device)
              

images, masks = images.to(device), masks.to(device)
              

import torch
import matplotlib.pyplot as plt

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –º–∞—Å–æ–∫ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
def show_images_with_predictions(images, masks, predictions, num_classes):
    num_images = len(images)
    fig, axes = plt.subplots(num_images, 3, figsize=(18, 6 * num_images))

    for i in range(num_images):
        image = images[i].permute(1, 2, 0).cpu().numpy()  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy
        mask = masks[i].cpu().numpy()
        prediction = predictions[i].cpu().numpy()

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        axes[i, 0].imshow(image)
        axes[i, 0].set_title(f'Image {i+1}')
        axes[i, 0].axis('off')

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–∞—Å–∫–∏
        axes[i, 1].imshow(mask, cmap='jet')
        axes[i, 1].set_title(f'Ground Truth Mask {i+1}')
        axes[i, 1].axis('off')

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        axes[i, 2].imshow(prediction, cmap='jet')
        axes[i, 2].set_title(f'Predicted Mask {i+1}')
        axes[i, 2].axis('off')

    plt.show()

# –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–∞—Å–æ–∫ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ train_loader –∏–ª–∏ –¥—Ä—É–≥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞)
# –î–ª—è –ø—Ä–∏–º–µ—Ä–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –º–∞—Å–∫–∏:
images, masks = next(iter(train_loader))  # –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å train_loader
images = images[:3].to(device)  # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
masks = masks[:3].to(device)    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–∞—Å–∫–∏

# –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ (eval)
model.eval()

# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º –º–∞—Å–∫–∏ –¥–ª—è —ç—Ç–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
with torch.no_grad():
    outputs = model(images)  # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º –¥–ª—è –≤—Å–µ—Ö 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    preds = torch.argmax(outputs, dim=1)  # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

# –í—ã–≤–æ–¥–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –º–∞—Å–∫–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
show_images_with_predictions(images, masks, preds, n_classes)
''')
    elif st == '5':
        print('''
!pip install datasets

              
from torchvision.transforms import InterpolationMode

#–†–∞–∑–º–µ—Ä 224x224 —á–∞—â–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ResNet, VGG),
#–≥–¥–µ –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –æ–±—ä–µ–∫—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏, –Ω–æ –Ω–µ –≤–∞–∂–Ω–æ –µ–≥–æ —Ç–æ—á–Ω–æ–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ.
#–î–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–∞–∫–∏–µ –º–∞–ª–µ–Ω—å–∫–∏–µ —Ä–∞–∑–º–µ—Ä—ã –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –ø–æ—Ç–µ—Ä–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π.

#Segformer —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏

image_transform = transforms.Compose([
    transforms.Resize((512, 512)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.2, 0.2, 0.2]),
])
mask_transform = transforms.Compose([
    #–ò–∑–º–µ–Ω—è–µ—Ç —Ä–∞–∑–º–µ—Ä –º–∞—Å–∫–∏ –¥–æ 512x512 –ø–∏–∫—Å–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏.
    transforms.Resize((512, 512), interpolation=InterpolationMode.NEAREST),
])

# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
dataset_HF = ClothesSegmentationDataset(
    image_dir=image_dir,
    mask_dir=mask_dir,
    image_transform=image_transform,
    mask_transform=mask_transform
)
              


indices = list(range(len(dataset_HF)))

# –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏ (80% –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, 20% –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)

# –°–æ–∑–¥–∞–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è train –∏ val
train_subset = torch.utils.data.Subset(dataset_HF, train_indices)
val_subset = torch.utils.data.Subset(dataset_HF, val_indices)

# –°–æ–∑–¥–∞–µ–º DataLoader'—ã
train_loader_HF = torch.utils.data.DataLoader(train_subset, batch_size=2, shuffle=True)
val_loader_HF = torch.utils.data.DataLoader(val_subset, batch_size=2, shuffle=False)
              

model_name = "nvidia/segformer-b0-finetuned-ade-512-512"
model = SegformerForSemanticSegmentation.from_pretrained(
    model_name,
    ignore_mismatched_sizes=True,  # –î–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è —á–∏—Å–ª–∞ –∫–ª–∞—Å—Å–æ–≤
#–≠—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ —Ä–∞–∑–º–µ—Ä–∞—Ö —Å–ª–æ–µ–≤ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏,
#–µ—Å–ª–∏ –≤ –≤–∞—à–µ–º —Å–ª—É—á–∞–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Ç–æ–π, —Å –∫–æ—Ç–æ—Ä–æ–π –æ–Ω–∞ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞
    num_labels=59,
).to(device)
processor = SegformerImageProcessor.from_pretrained(model_name)
              

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class_weights = torch.tensor([0.1] + [torch.rand(1).item() * (3 - 1) + 2 for _ in range(58)]).to(device)

criterion = nn.CrossEntropyLoss(weight=class_weights)

optimizer = optim.Adam(model.parameters(), lr=1e-3)
              

import torch.nn.functional as F
import torch.optim as optim

def train_epoch(model, loader, optimizer):
    model.train()
    epoch_loss = 0
    correct = 0
    total = 0

    for images, masks in tqdm(loader):
        images = images.to(device)  # [B, 3, H, W]
        masks = masks.to(device)    # [B, H, W]

        optimizer.zero_grad()
        outputs = model(pixel_values=images).logits  # [B, num_labels, H_out, W_out]

        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –º–∞—Å–æ–∫ –∫ —Ä–∞–∑–º–µ—Ä—É –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
        masks_resized = F.interpolate(
            masks.unsqueeze(1).float(), size=outputs.shape[2:], mode="nearest"
        ).squeeze(1).long()

        loss = criterion(outputs, masks_resized)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

        preds = torch.argmax(outputs, dim=1)  # [B, H_out, W_out]
        correct += (preds == masks_resized).sum().item()
        total += masks_resized.numel()

    accuracy = correct / total
    return epoch_loss / len(loader), accuracy


def validate_epoch(model, loader):
    model.eval()
    epoch_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, masks in tqdm(loader):
            images = images.to(device)
            masks = masks.to(device)

            outputs = model(pixel_values=images).logits
            masks_resized = F.interpolate(
                masks.unsqueeze(1).float(), size=outputs.shape[2:], mode="nearest"
            ).squeeze(1).long()

            loss = criterion(outputs, masks_resized)
            epoch_loss += loss.item()

            preds = torch.argmax(outputs, dim=1)
            correct += (preds == masks_resized).sum().item()
            total += masks_resized.numel()

    accuracy = correct / total
    return epoch_loss / len(loader), accuracy

num_epochs = 75
for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader_HF, optimizer)
    val_loss, val_acc = validate_epoch(model, val_loader_HF)

    print(f"Epoch {epoch + 1}/{num_epochs}")
    print(f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}")
    print(f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}")
              

import torch
import torch.nn.functional as F
import torch.optim as optim
from tqdm import tqdm

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
class EarlyStopping:
    def __init__(self, patience=10, delta=0, verbose=False):
      #verbose=False: –ï—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤ True, —Ç–æ –ø—Ä–∏ –∫–∞–∂–¥–æ–º —É–ª—É—á—à–µ–Ω–∏–∏ –±—É–¥–µ—Ç –≤—ã–≤–æ–¥–∏—Ç—å—Å—è —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Ç–æ–º, —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        """
        patience: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö, –≤ —Ç–µ—á–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä—ã—Ö –¥–æ–ª–∂–Ω—ã –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ,
                  –ø—Ä–µ–∂–¥–µ —á–µ–º –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.
        delta: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ, —á—Ç–æ–±—ã —Å—á–∏—Ç–∞–ª–æ—Å—å —É–ª—É—á—à–µ–Ω–∏–µ–º.
        verbose: –µ—Å–ª–∏ True, —Ç–æ –±—É–¥–µ—Ç –≤—ã–≤–æ–¥–∏—Ç—å—Å—è —Å–æ–æ–±—â–µ–Ω–∏–µ –ø—Ä–∏ –∫–∞–∂–¥–æ–º —É–ª—É—á—à–µ–Ω–∏–∏.
        """
        self.patience = patience
        self.delta = delta
        self.verbose = verbose
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss + self.delta:
            self.counter += 1
            if self.verbose:
                print(f"Validation loss did not improve for {self.counter} epochs.")
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print("Early stopping triggered!")
        else:
            self.best_loss = val_loss
            self.counter = 0

# –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
def train_epoch(model, loader, optimizer, criterion, device):
    model.train()
    epoch_loss = 0
    correct = 0
    total = 0

#–¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Ç–≤–æ
    for images, masks in tqdm(loader):
        images = images.to(device)  # [B, 3, H, W]
        masks = masks.to(device)    # [B, H, W]

        optimizer.zero_grad()
        outputs = model(pixel_values=images).logits  # [B, num_labels, H_out, W_out]
        #–ü—Ä–æ–≥–æ–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å. –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —è–≤–ª—è—é—Ç—Å—è –ª–æ–≥–∏—Ç—ã (–Ω–µ–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è),
        #–∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–∂–µ –±—É–¥—É—Ç –ø–µ—Ä–µ–¥–∞–Ω—ã –≤ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å.

        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –º–∞—Å–æ–∫ –∫ —Ä–∞–∑–º–µ—Ä—É –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ outputs
        masks_resized = F.interpolate(
            masks.unsqueeze(1).float(), size=outputs.shape[2:], mode="nearest"
        ).squeeze(1).long()

        loss = criterion(outputs, masks_resized)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

        preds = torch.argmax(outputs, dim=1)  # [B, H_out, W_out]
        correct += (preds == masks_resized).sum().item()
        total += masks_resized.numel()

    accuracy = correct / total
    return epoch_loss / len(loader), accuracy


# –§—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ
def validate_epoch(model, loader, criterion, device):
    model.eval()
    epoch_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, masks in tqdm(loader):
            images = images.to(device)
            masks = masks.to(device)

            outputs = model(pixel_values=images).logits
            masks_resized = F.interpolate(
                masks.unsqueeze(1).float(), size=outputs.shape[2:], mode="nearest"
            ).squeeze(1).long()

            loss = criterion(outputs, masks_resized)
            epoch_loss += loss.item()


            #argmax –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞—Ö–æ–¥–∏—Ç –∏–Ω–¥–µ–∫—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤–¥–æ–ª—å —É–∫–∞–∑–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (dim).
            #–í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ, dim=1 –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ argmax –∏—â–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤–¥–æ–ª—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤ (C).
            #–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è (h, w) –≤ –∫–∞–∂–¥–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ b argmax –≤—ã–±–∏—Ä–∞–µ—Ç –∫–ª–∞—Å—Å c —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é.

            preds = torch.argmax(outputs, dim=1)
            correct += (preds == masks_resized).sum().item()
            total += masks_resized.numel()

    accuracy = correct / total
    return epoch_loss / len(loader), accuracy


# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
num_epochs = 75
patience = 10  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
early_stopping = EarlyStopping(patience=patience, delta=0.001, verbose=True)


for epoch in range(num_epochs):
    # –û–±—É—á–µ–Ω–∏–µ
    train_loss, train_acc = train_epoch(model, train_loader_HF, optimizer, criterion, device)

    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    val_loss, val_acc = validate_epoch(model, val_loader_HF, criterion, device)

    # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print(f"Epoch {epoch + 1}/{num_epochs}")
    print(f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}")
    print(f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}")

    # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
    early_stopping(val_loss)
    if early_stopping.early_stop:
        print(f"Training stopped early at epoch {epoch + 1}")
        break
              

import matplotlib.pyplot as plt
import torch

def plot_predictions(model, loader, device, num_samples=5):
    model.eval()
    with torch.no_grad():
        sample_count = 0
        for images, masks in loader:
            images = images.to(device)
            masks = masks.to(device)

            outputs = model(pixel_values=images).logits  # [B, num_labels, H_out, W_out]
            preds = torch.argmax(outputs, dim=1)  # [B, H_out, W_out]

            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            for i in range(images.size(0)):  # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –±–∞—Ç—á—É
                if sample_count >= num_samples:
                    return
                fig, axs = plt.subplots(1, 3, figsize=(12, 4))
                axs[0].imshow(images[i].cpu().numpy().transpose(1, 2, 0))  # –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                axs[0].set_title('Input Image')
                axs[0].axis('off')

                axs[1].imshow(masks[i].cpu().numpy(), cmap='gray')  # –ò—Å—Ç–∏–Ω–Ω–∞—è –º–∞—Å–∫–∞
                axs[1].set_title('True Mask')
                axs[1].axis('off')

                axs[2].imshow(preds[i].cpu().numpy(), cmap='gray')  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–∞—Å–∫–∞
                axs[2].set_title('Predicted Mask')
                axs[2].axis('off')

                plt.show()

                sample_count += 1

              
# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≤—ã–≤–æ–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
plot_predictions(model, val_loader_HF, device, num_samples=5)  # num_samples - —Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å
''')
def ligDep_info():
    print('''
–ß–¢–û–ë–´ –í–´–ó–í–ê–¢–¨ –ö–û–î: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DL.ligDep_code("–ù–û–ú–ï–† –ó–ê–î–ê–ù–ò–Ø")    
          
1. –û–ø–∏—à–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç AnimalDetectionDataset –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏–≤–∞ animals.zip. –†–µ–∞–ª–∏–∑—É–π—Ç–µ __getitem__ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –æ–Ω –≤–æ–∑–≤—Ä–∞—â–∞–ª —Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–∞: —Ç–µ–Ω–∑–æ—Ä —Å 
–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º, —Å–ª–æ–≤–∞—Ä—å —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ bounding box –∏ –º–µ—Ç–∫—É –æ–±—ä–µ–∫—Ç–∞. –ü—Ä–µ–¥—É—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∏–∑–≤–Ω–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞–±–æ—Ä –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π 
–¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –º–µ—Ç–∫–∏ –æ–±—ä–µ–∫—Ç–∞ (–¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è) –∏ —Ñ–ª–∞–≥, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π, –Ω—É–∂–Ω–æ –ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ –∏–ª–∏ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bounding box. 
–†–∞–∑–±–µ–π—Ç–µ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–∞—é—â–µ–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ. –ü—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –∑–∞–±—É–¥—å—Ç–µ —É–∫–∞–∑–∞—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ ResNet.
          
2. –ù–∞–ø–∏—à–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –≤—ã–¥–µ–ª–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ –æ–±—ä–µ–∫—Ç–∞ lightning.LightningModule. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –¥–≤—É—Ö–≥–æ–ª–æ–≤—É—é —Å–µ—Ç—å, 
–æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –º–µ—Ç–∫—É –æ–±—ä–µ–∫—Ç–∞ (–∑–∞–¥–∞—á–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏), –∞ –≤—Ç–æ—Ä–∞—è –≥–æ–ª–æ–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç 4 –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤–µ—Ä—à–∏–Ω bounding box (–∑–∞–¥–∞—á–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏). 
–í –∫–∞—á–µ—Å—Ç–≤–µ backbone –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å resnet50 –∏–∑ –ø–∞–∫–µ—Ç–∞ torchvision. –í –∫–∞—á–µ—Å—Ç–≤–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—É–º–º—É MSELoss (–¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –æ—à–∏–±–∫–∏ –Ω–∞ –∑–∞–¥–∞—á–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏) –∏ CrossEntropyLoss (–¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –æ—à–∏–±–∫–∏ –Ω–∞ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏).
–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –ø—Ä–∏ –ø–æ–º–æ—â–∏ lightning –∏ torchmetrics:
–¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤, –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é —ç–ø–æ—Ö–∏ –º–µ—Ç—Ä–∏–∫–∏ —É—Å—Ä–µ–¥–Ω—è—é—Ç—Å—è;
–¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –≤–æ –≤—Ä–µ–º—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤, –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é —ç–ø–æ—Ö–∏ –º–µ—Ç—Ä–∏–∫–∏ —É—Å—Ä–µ–¥–Ω—è—é—Ç—Å—è;
–µ—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–µ —É–ª—É—á—à–∞–ª–æ—Å—å –≤ —Ç–µ—á–µ–Ω–∏–∏ 5 —ç–ø–æ—Ö, –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ä–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞;
–ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–æ–¥–µ–ª—å –Ω–∞ —ç–∫—Ä–∞–Ω –≤—ã–≤–æ–¥–∏—Ç—Å—è —Å–≤–æ–¥–∫–∞ –ø–æ –º–æ–¥–µ–ª–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –≤—ã—Ö–æ–¥–æ–≤ —Å–ª–æ–µ–≤;
–¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è tensorboard.
–ò—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –ø–æ–ª—É—á–∏—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–æ—à–∫–∏ –∏ —Å–æ–±–∞–∫–∏ –∏ –æ—Ç—Ä–∏—Å—É–π—Ç–µ –∏—Ö. 
–í—ã–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—É, –æ–±—Ä–∞—Ç–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, —á—Ç–æ–±—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏.
          
3. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —á–µ–∫–ø–æ–∏–Ω—Ç –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –ø–µ—Ä–µ–≤–µ–¥–∏—Ç–µ –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏. –î–æ–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é transform_image –∏ route predict. 
–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä flask –∏ —Å–¥–µ–ª–∞–π—Ç–µ POST-–∑–∞–ø—Ä–æ—Å –∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º—É —ç–Ω–¥–ø–æ–∏–Ω—Ç—É.
–ü—Ä–∏ —Ä–∞–±–æ—Ç–µ –≤ Google Colab –≤—ã –º–æ–∂–µ—Ç–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º ngrok –¥–ª—è –ø—Ä–æ–±—Ä–æ—Å–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∞–¥—Ä–µ—Å–∞ –∏–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä Flask –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ.
''')
    
def ligDep_code(st):
    if st == '1':
        print('''
!pip install pytorch_lightning
!pip install torchinfo
!pip install pyngrok

import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from torchvision.transforms.functional import to_pil_image
import torchvision
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import torch.nn.functional as F
import torchvision.models as models
from tqdm.auto import tqdm
from sklearn import metrics
import seaborn as sns
import pandas as pd
import zipfile
from tqdm import tqdm
from PIL import Image
from sklearn.metrics import confusion_matrix
import random
import matplotlib.patches as patches
import kagglehub
import os
import numpy as np
from collections import Counter
import pytorch_lightning as pl
from torchmetrics.classification import Accuracy
from torchvision import models
from torch.utils.data import DataLoader
import torchmetrics
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)
              

from google.colab import drive
drive.mount('/content/drive')
              
zip_path = '/content/drive/MyDrive/–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ/animals.zip'

extracted_dir = './animals'
with zipfile.ZipFile(zip_path, 'r') as zf:
    for file in tqdm(zf.infolist()):
        zf.extract(file, extracted_dir)
              

import xml.etree.ElementTree as ET
import os

def parse_xml(xml_file):
    tree = ET.parse(xml_file) #–ø—É—Ç—å —Ñ–∞–π–ª–∞ —Ö–º–ª
    root = tree.getroot()


    size = root.find("size") #—Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    width = float(size.find("width").text)
    height = float(size.find("height").text)

    #–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –±–∞—É–Ω–¥–∏–Ω–≥ –±–æ–∫—Å
    bndbox = root.find(".//bndbox")
    xmin = float(bndbox.find("xmin").text)
    ymin = float(bndbox.find("ymin").text)
    xmax = float(bndbox.find("xmax").text)
    ymax = float(bndbox.find("ymax").text)


    #–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
    raw = {"xmin": xmin, "ymin": ymin, "xmax": xmax, "ymax": ymax}
    scaled = {
        "xmin": xmin / width,
        "ymin": ymin / height,
        "xmax": xmax / width,
        "ymax": ymax / height,
    }


    obj_name = root.find(".//object/name").text


    return {"raw": raw, "scaled": scaled, "obj_name": obj_name}

##############
file_name = 'cat.0.xml'
extracted_dir = './animals/Asirra: cat vs dogs'

#–ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ XML —Ñ–∞–π–ª—É –≤–Ω—É—Ç—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π –ø–∞–ø–∫–∏
xml_file_path = os.path.join(extracted_dir, file_name)

try:
    #–æ—Ç–∫—Ä—ã–≤–∞–µ–º –∏ –ø–∞—Ä—Å–∏–º —Ñ–∞–π–ª
    result = parse_xml(xml_file_path)
    print("–†–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(result)
except FileNotFoundError:
    print(f"–§–∞–π–ª '{file_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–∞–ø–∫–µ '{extracted_dir}'.")
              

class AnimalDetectionDataset(Dataset):
    def __init__(self, root, transforms=None, target_transform=None, return_scaled=True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤.

        :param root: –ü—É—Ç—å –∫ –∞—Ä—Ö–∏–≤—É —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ XML —Ñ–∞–π–ª–∞–º–∏
        :param transforms: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        :param target_transform: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –º–µ—Ç–∫–∏ –æ–±—ä–µ–∫—Ç–∞
        :param return_scaled: –§–ª–∞–≥, –Ω—É–∂–Ω–æ –ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bounding box
        """
        self.root = root
        self.transforms = transforms
        self.target_transform = target_transform
        self.return_scaled = return_scaled

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∞—Ä—Ö–∏–≤–∞
        self.image_dir = './animals/Asirra: cat vs dogs'
        with zipfile.ZipFile(self.root, 'r') as zf:
            zf.extractall(self.image_dir)

        # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è XML —Ñ–∞–π–ª–æ–≤
        self.xml_files = set()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
        seen_files = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –ø–æ –∏—Ö –∏–º–µ–Ω–∞–º

        for root_dir, dirs, files in os.walk(self.image_dir):
            for file in files:
                if file.endswith('.xml'):
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å –¥–ª—è —Ñ–∞–π–ª–∞
                    normalized_path = os.path.normpath(os.path.join(root_dir, file))

                    if file not in seen_files:
                        seen_files.add(file)
                        self.xml_files.add(normalized_path)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ø–∏—Å–æ–∫ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        self.xml_files = list(self.xml_files)

        print(f"–ù–∞–π–¥–µ–Ω–æ XML —Ñ–∞–π–ª–æ–≤: {len(self.xml_files)}")

        if len(self.xml_files) == 0:
            print("–û—à–∏–±–∫–∞: XML —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

    def __len__(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        return len(self.xml_files)

    def __getitem__(self, idx):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º XML —Ñ–∞–π–ª
        xml_file = self.xml_files[idx]
        image_file_name = xml_file.replace('.xml', '.jpg')  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ç–∞–∫–∏–º –∂–µ –∏–º–µ–Ω–µ–º
        image_file_path = image_file_name

        # –ß—Ç–µ–Ω–∏–µ XML —Ñ–∞–π–ª–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è bounding box –∏ –º–µ—Ç–∫–∏ –æ–±—ä–µ–∫—Ç–∞ —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ parse_xml
        result = parse_xml(xml_file)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º bbox –∏ label –∏–∑ —Å–ª–æ–≤–∞—Ä—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–≥–æ parse_xml
        bbox = result['scaled'] if self.return_scaled else result['raw']
        label = result['obj_name']

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º bbox –≤ —Ç–µ–Ω–∑–æ—Ä
        bbox = torch.tensor([bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']], dtype=torch.float32)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫—É –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
        label_map = {'cat': 0, 'dog': 1}  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫ –≤ —á–∏—Å–ª–∞
        label = torch.tensor(label_map[label], dtype=torch.long)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = Image.open(image_file_path).convert('RGB')

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if self.transforms:
            image = self.transforms(image)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫—É, –µ—Å–ª–∏ –∑–∞–¥–∞–Ω–æ target_transform
        if self.target_transform:
            label = self.target_transform(label)

        return image, bbox, label

#################################################################################
zip_path = '/content/drive/MyDrive/–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ/animals.zip'


transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])


dataset = AnimalDetectionDataset(root=zip_path, transforms=transform, return_scaled=True)

#
if len(dataset) > 0:
    image, bbox, label = dataset[0]
    print(f"Image shape: {image.shape}, BBox: {bbox}, Label: {label}")
else:
    print("–î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç.")
              

def split_dataset(dataset, val_split=0.2):
    """
    –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é —á–∞—Å—Ç–∏.
    """
    dataset_size = len(dataset)
    val_size = int(val_split * dataset_size)
    train_size = dataset_size - val_size
    return random_split(dataset, [train_size, val_size])

train_dataset, val_dataset = split_dataset(dataset)
len(train_dataset), len(val_dataset)
''')
    elif st == '2':
        print('''
class DualHeadModel(pl.LightningModule):
    def __init__(self, train_dataset, val_dataset, num_classes=2):
        super(DualHeadModel, self).__init__()


        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)

        # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ —Å–ª–æ—è avgpool
        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ —Å–ª–æ—è AdaptiveAvgPool2d
        self.in_features = 2048  # –î–ª—è ResNet-50 —ç—Ç–æ –≤—Å–µ–≥–¥–∞ 2048

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞
        self.classification_head = nn.Linear(self.in_features, num_classes)

        # –ì–æ–ª–æ–≤–∞ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç BBox)
        self.regression_head = nn.Linear(self.in_features, 4)

        # –ú–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self.classification_accuracy = Accuracy(task='binary', num_classes=num_classes)

        # –ú–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (MSE)
        self.regression_mse = torchmetrics.MeanSquaredError()

        # –î–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset

    def forward(self, x):
        features = self.resnet(x).view(x.size(0), -1)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –≤–µ–∫—Ç–æ—Ä
        classification_output = self.classification_head(features)
        regression_output = self.regression_head(features)
        return classification_output, regression_output

    def training_step(self, batch, batch_idx):
        images, bboxes, labels = batch
        class_preds, bbox_preds = self(images)

        # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–µ—Ä–µ–º –∏–Ω–¥–µ–∫—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é (–ª–æ–≥–∏—Ç–∞)
        class_preds_max = torch.argmax(class_preds, dim=1)

        # –ü–æ—Ç–µ—Ä–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        classification_loss = nn.CrossEntropyLoss()(class_preds, labels)

        # –ü–æ—Ç–µ—Ä–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        regression_loss = nn.MSELoss()(bbox_preds, bboxes)

        # –°—É–º–º–∞—Ä–Ω–∞—è –ø–æ—Ç–µ—Ä—è
        total_loss = classification_loss + regression_loss

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        self.log('train_loss', total_loss)
        self.log('train_classification_accuracy', self.classification_accuracy(class_preds_max, labels))
        self.log('train_regression_mse', self.regression_mse(bbox_preds, bboxes))

        return total_loss

    def validation_step(self, batch, batch_idx):
        images, bboxes, labels = batch
        class_preds, bbox_preds = self(images)

        # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–µ—Ä–µ–º –∏–Ω–¥–µ–∫—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é (–ª–æ–≥–∏—Ç–∞)
        class_preds_max = torch.argmax(class_preds, dim=1)

        # –ü–æ—Ç–µ—Ä–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        classification_loss = nn.CrossEntropyLoss()(class_preds, labels)

        # –ü–æ—Ç–µ—Ä–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        regression_loss = nn.MSELoss()(bbox_preds, bboxes)

        # –°—É–º–º–∞—Ä–Ω–∞—è –ø–æ—Ç–µ—Ä—è
        total_loss = classification_loss + regression_loss

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        self.log('val_loss', total_loss)
        self.log('val_classification_accuracy', self.classification_accuracy(class_preds_max, labels))
        self.log('val_regression_mse', self.regression_mse(bbox_preds, bboxes))

        return total_loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)
        return optimizer

    def on_epoch_end(self):
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é —ç–ø–æ—Ö–∏
        self.log('train_classification_accuracy_epoch', self.classification_accuracy.compute())
        self.log('train_regression_mse_epoch', self.regression_mse.compute())

        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π —ç–ø–æ—Ö–∏
        self.classification_accuracy.reset()
        self.regression_mse.reset()

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=32, shuffle=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=32, shuffle=False)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

model = DualHeadModel(train_dataset=train_dataset, val_dataset=val_dataset)

# –°–≤–æ–¥–∫–∞ –ø–æ –º–æ–¥–µ–ª–∏
summary(model, input_size=(1, 3, 256, 256))  # –ü–µ—á–∞—Ç–∞–µ—Ç —Å–≤–æ–¥–∫—É –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–º 256x256 —Å 3 –∫–∞–Ω–∞–ª–∞–º–∏
              
logger = TensorBoardLogger("tb_logs", name="dual_head_model")
early_stop_callback = EarlyStopping(monitor="val_loss", patience=5, verbose=True)

checkpoint_callback = ModelCheckpoint(
    monitor="val_loss",   # –ú—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞, –∫–æ–≥–¥–∞ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É—é—Ç—Å—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏
    filename="best_model",  # –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    save_top_k=1,           # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
    mode="min",             # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –ø–æ—Ç–µ—Ä–∏
    save_weights_only=True, # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –≤–µ—Å–∞
    verbose=True
)

trainer = pl.Trainer(
    max_epochs=50,
    logger=logger,
    callbacks=[early_stop_callback, checkpoint_callback],
    devices=1,  # –ï—Å–ª–∏ —É –≤–∞—Å –æ–¥–∏–Ω GPU
    accelerator="gpu",  # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU
)

trainer.fit(model)
              

model.eval()

# –°–º–æ—Ç—Ä–∏–º –Ω–∞ –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ val_dataset
img, bbox, label = val_dataset[15]

# –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ (–æ–±—Ä–∞—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
mean = torch.tensor([0.485, 0.456, 0.406])
std = torch.tensor([0.229, 0.224, 0.225])

# –û–±—Ä–∞—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
img = img * std[:, None, None] + mean[:, None, None]

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç, –ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
img = img.permute(1, 2, 0)  # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –∫–∞–Ω–∞–ª—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è

# –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
height, width, _ = img.shape

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(img)
ax.axis('off')  # –£–±–∏—Ä–∞–µ–º –æ—Å–∏

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º bounding box –≤ —Ä–µ–∞–ª—å–Ω—ã–µ –ø–∏–∫—Å–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
bbox = torch.tensor(bbox)
scaling_factors = torch.tensor([width, height, width, height])

# –†–∞—Å—á–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç bbox –≤ –ø–∏–∫—Å–µ–ª—è—Ö
true_xmin, true_ymin, true_xmax, true_ymax = bbox * scaling_factors

# –†–∏—Å—É–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–π bounding box (ground truth) –∫—Ä–∞—Å–Ω—ã–º —Ü–≤–µ—Ç–æ–º
ax.add_patch(plt.Rectangle((true_xmin, true_ymin), true_xmax - true_xmin, true_ymax - true_ymin,
                           linewidth=2, edgecolor='r', facecolor='none', label='Ground Truth'))

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Å –ª–µ–π–±–ª–æ–º –¥–ª—è –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ bounding box
class_label = 'Cat' if label == 0 else 'Dog'
ax.text(true_xmin, true_ymin - 10, f'True: {class_label}', color='r', fontsize=12, weight='bold')

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç [channels, height, width]
img = img.permute(2, 0, 1)  # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–∏

# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
with torch.no_grad():
    class_preds, bbox_preds = model(img.unsqueeze(0))  # –î–æ–±–∞–≤–ª—è–µ–º batch dimension
    bbox_preds = bbox_preds.squeeze(0)  # –£–±–∏—Ä–∞–µ–º batch dimension

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π bbox –≤ —Ä–µ–∞–ª—å–Ω—ã–µ –ø–∏–∫—Å–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
pred_xmin, pred_ymin, pred_xmax, pred_ymax = bbox_preds * scaling_factors

# –†–∏—Å—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π bounding box –∑–µ–ª–µ–Ω—ã–º —Ü–≤–µ—Ç–æ–º
ax.add_patch(plt.Rectangle((pred_xmin, pred_ymin), pred_xmax - pred_xmin, pred_ymax - pred_ymin,
                           linewidth=2, edgecolor='g', facecolor='none', label='Prediction'))

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Å –ª–µ–π–±–ª–æ–º –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ bounding box
pred_class_label = 'Cat' if class_preds.argmax() == 0 else 'Dog'
ax.text(pred_xmin, pred_ymin - 10, f'Pred: {pred_class_label}', color='g', fontsize=12, weight='bold')

# –í—ã–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ bounding boxes
ax.text(0.5, -0.05, 'Red box: Ground Truth, Green box: Prediction', ha='center', va='center',
        transform=ax.transAxes, fontsize=12, color='black', bbox=dict(facecolor='white', alpha=0.7))

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
plt.show()
              

model.eval()

# –°–º–æ—Ç—Ä–∏–º –Ω–∞ –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ val_dataset
img, bbox, label = val_dataset[14]

# –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ (–æ–±—Ä–∞—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
mean = torch.tensor([0.485, 0.456, 0.406])
std = torch.tensor([0.229, 0.224, 0.225])

# –û–±—Ä–∞—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
img = img * std[:, None, None] + mean[:, None, None]

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç, –ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
img = img.permute(1, 2, 0)  # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –∫–∞–Ω–∞–ª—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è

# –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
height, width, _ = img.shape

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(img)
ax.axis('off')  # –£–±–∏—Ä–∞–µ–º –æ—Å–∏

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º bounding box –≤ —Ä–µ–∞–ª—å–Ω—ã–µ –ø–∏–∫—Å–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
bbox = torch.tensor(bbox)
scaling_factors = torch.tensor([width, height, width, height])

# –†–∞—Å—á–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç bbox –≤ –ø–∏–∫—Å–µ–ª—è—Ö
true_xmin, true_ymin, true_xmax, true_ymax = bbox * scaling_factors

# –†–∏—Å—É–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–π bounding box (ground truth) –∫—Ä–∞—Å–Ω—ã–º —Ü–≤–µ—Ç–æ–º
ax.add_patch(plt.Rectangle((true_xmin, true_ymin), true_xmax - true_xmin, true_ymax - true_ymin,
                           linewidth=2, edgecolor='r', facecolor='none', label='Ground Truth'))

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Å –ª–µ–π–±–ª–æ–º –¥–ª—è –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ bounding box
class_label = 'Cat' if label == 0 else 'Dog'
ax.text(true_xmin, true_ymin - 10, f'True: {class_label}', color='r', fontsize=12, weight='bold')

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç [channels, height, width]
img = img.permute(2, 0, 1)  # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–∏

# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
with torch.no_grad():
    class_preds, bbox_preds = model(img.unsqueeze(0))  # –î–æ–±–∞–≤–ª—è–µ–º batch dimension
    bbox_preds = bbox_preds.squeeze(0)  # –£–±–∏—Ä–∞–µ–º batch dimension

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π bbox –≤ —Ä–µ–∞–ª—å–Ω—ã–µ –ø–∏–∫—Å–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
pred_xmin, pred_ymin, pred_xmax, pred_ymax = bbox_preds * scaling_factors

# –†–∏—Å—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π bounding box –∑–µ–ª–µ–Ω—ã–º —Ü–≤–µ—Ç–æ–º
ax.add_patch(plt.Rectangle((pred_xmin, pred_ymin), pred_xmax - pred_xmin, pred_ymax - pred_ymin,
                           linewidth=2, edgecolor='g', facecolor='none', label='Prediction'))

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Å –ª–µ–π–±–ª–æ–º –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ bounding box
pred_class_label = 'Cat' if class_preds.argmax() == 0 else 'Dog'
ax.text(pred_xmin, pred_ymin - 10, f'Pred: {pred_class_label}', color='g', fontsize=12, weight='bold')

# –í—ã–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ bounding boxes
ax.text(0.5, -0.05, 'Red box: Ground Truth, Green box: Prediction', ha='center', va='center',
        transform=ax.transAxes, fontsize=12, color='black', bbox=dict(facecolor='white', alpha=0.7))

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
plt.show()
''')
    elif st == '3':
        print('''
extracted_dir = './animals/Asirra: cat vs dogs'

# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ
files = os.listdir(extracted_dir)

image_files = [f for f in files if f.endswith(('.jpg', '.jpeg', '.png'))]
print(image_files)
              

from PIL import Image
import io
from torchvision.transforms import v2 as T
import torch
from flask import Flask
# from flask_ngrok import run_with_ngrok
from flask import Flask, request, jsonify
import threading
              

checkpoint_path = 'tb_logs/dual_head_model/version_0/checkpoints/best_model.ckpt'

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
model2 = DualHeadModel.load_from_checkpoint(checkpoint_path, train_dataset=train_dataset, val_dataset=val_dataset)

summary(model2, input_size=(1, 3, 256, 256))
              


model2.eval()

# Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = Flask(__name__)
              


def bytes_to_pil(image_bytes: bytes) -> Image:
    return Image.open(io.BytesIO(image_bytes))

def transform_image(image: Image) -> torch.Tensor:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç PIL.Image –≤ —Ç–µ–Ω–∑–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –ø–æ–¥–∞—Ç—å –≤ –º–æ–¥–µ–ª—å"""
    transform = transforms.Compose([
        transforms.Resize((256, 256)),  # –¢–æ—Ç –∂–µ —Ä–∞–∑–º–µ—Ä, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # –û–±—Ä–∞—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    ])
    image_tensor = transform(image)
    return image_tensor
              

ALLOWED_EXTENSIONS = {"png", "jpg", "jpeg"}


def allowed_file(filename):
    return "." in filename and filename.rsplit(".", 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route("/predict", methods=["POST"])
def predict():
    if request.method == "POST":
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        file = request.files.get("file")
        if file is None or file.filename == "":
            return jsonify({"error": "no file"})

        if not allowed_file(file.filename):
            return jsonify({"error": "format not supported"})

        try:
            # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –±–∞–π—Ç–∞—Ö
            img_bytes = file.read()
            image = bytes_to_pil(img_bytes)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä
            tensor = transform_image(image)

            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –º–æ–¥–µ–ª–∏
            with torch.no_grad():
                class_preds, bbox_preds = model2(tensor.unsqueeze(0))  # –î–æ–±–∞–≤–ª—è–µ–º batch dimension
                class_preds = class_preds.squeeze(0)
                bbox_preds = bbox_preds.squeeze(0)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞
            class_preds = torch.argmax(class_preds, dim=0).item()  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–µ—Ç–∫—É –∫–ª–∞—Å—Å–∞
            bbox_preds = bbox_preds.tolist()  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            data = {
                "label": class_preds,
                "bbox": bbox_preds
            }
            return jsonify(data)

        except Exception as e:
            return jsonify({"error": f"Error during prediction: {e}"})

def run_app():
    app.run(debug=True, use_reloader=False)

if __name__ == "__main__":
    threading.Thread(target=lambda: app.run()).start()
    # app.run()
              

from pyngrok import ngrok
              

!ngrok authtoken 2pUu4KzSHKbV3WJgfd5Nosy88V9_3eyhVN2DGKskYdozP5sv5
              

public_url = ngrok.connect(5000)
print(f" * Flask app is running at: {public_url}")
              

import requests

filename = 'cat.3715.jpg'
file_path = os.path.join(extracted_dir, filename)

resp = requests.post(
    "http://127.0.0.1:5000/predict",  # –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π –ø—É–±–ª–∏—á–Ω—ã–π URL –æ—Ç ngrok
    files={"file": open(file_path, "rb")}
)

response_json = resp.json()

response_json
              

image = Image.open(file_path)
bbox = response_json.get('bbox', [])
label = response_json.get('label', None)
bbox
              

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox –≤ –ø–∏–∫—Å–µ–ª—å–Ω—ã–µ
width, height = image.size
xmin, ymin, xmax, ymax = [
    int(bbox[0] * width),
    int(bbox[1] * height),
    int(bbox[2] * width),
    int(bbox[3] * height)
]

# –û—Ç–∫—Ä—ã–≤–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É –∏ —Ä–∏—Å—É–µ–º –Ω–∞ –Ω–µ–π bounding box
fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(image)
ax.axis('off')  # –£–±–∏—Ä–∞–µ–º –æ—Å–∏

# –†–∏—Å—É–µ–º bounding box (–∫—Ä–∞—Å–Ω—ã–π –¥–ª—è –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è)
ax.add_patch(plt.Rectangle(
    (xmin, ymin), xmax - xmin, ymax - ymin,
    linewidth=2, edgecolor='r', facecolor='none', label='Prediction'
))

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –º–µ—Ç–∫–∏ (label)
labels_map = {0: 'Cat', 1: 'Dog'}  # –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫
label_name = labels_map.get(label, 'Unknown')
ax.text(xmin, ymin - 10, label_name, color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
plt.show()
''')