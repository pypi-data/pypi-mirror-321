Metadata-Version: 2.4
Name: arctic_training
Version: 0.0.1
Summary: Snowflake LLM training library
Author-email: michael.wyatt@snowflake.com, jeff.rasley@snowflake.com, zhewei.yao@snowflake.com, samyam.rajbhandari@snowflake.com, aurick.qiao@snowflake.com, canwen.xu@snowflake.com
License-Expression: Apache-2.0
License-File: LICENSE
Keywords: llm,training
Requires-Dist: datasets
Requires-Dist: deepspeed
Requires-Dist: devtools
Requires-Dist: jsonlines
Requires-Dist: loguru
Requires-Dist: openai>=1.48.0
Requires-Dist: peft
Requires-Dist: pydantic>=2.0
Requires-Dist: tabulate
Requires-Dist: torch
Requires-Dist: tqdm
Requires-Dist: transformers==4.47.0
Provides-Extra: cortex
Requires-Dist: snowflake-connector-python==3.12.3; extra == 'cortex'
Provides-Extra: dev
Requires-Dist: autodoc-pydantic>=2.0.0; extra == 'dev'
Requires-Dist: black>=23.1; extra == 'dev'
Requires-Dist: flake8>=5.0; extra == 'dev'
Requires-Dist: mypy>=1.1; extra == 'dev'
Requires-Dist: pre-commit; extra == 'dev'
Requires-Dist: pytest>=7.0; extra == 'dev'
Requires-Dist: sphinx-autodoc-typehints; extra == 'dev'
Requires-Dist: sphinx-copybutton; extra == 'dev'
Requires-Dist: sphinx-rtd-theme; extra == 'dev'
Requires-Dist: sphinx>=8.1.3; extra == 'dev'
Provides-Extra: vllm
Requires-Dist: vllm==0.6.2; extra == 'vllm'
Description-Content-Type: text/markdown

[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/snowflakedb/ArcticTraining/blob/main/LICENSE)

<h3 align="center">
  <img src="docs/images/arctic_training_logo.svg" width=500px><br>
  | <a href="https://arctictraining.readthedocs.io/en/latest/"><b>Documentation</b></a> | <a href="https://www.snowflake.com/en/engineering-blog/arctictraining-llm-post-training-framework/"><b>Blog</b></a> |
</h3>

<!--| <a href="#"><b>Discourse</b></a> | -->

# ArcticTraining: Simplifying and Accelerating Post-Training for LLMs

ArcticTraining is a framework designed to simplify and accelerate the post-training process for large language models (LLMs). It addresses challenges in current frameworks, such as limited support for rapid prototyping and the lack of native data generation tools, by offering modular trainer designs, simplified code structures, and integrated pipelines for creating and cleaning synthetic data. These features enable users to enhance LLM capabilities, like code generation and complex reasoning, with greater efficiency and flexibility. Read more about ArcticTraining [in our blog](https://www.snowflake.com/en/engineering-blog/arctictraining-llm-post-training-framework/).

# Quickstart

To get started training a model with ArcticTraining, follow the steps below:

1. Clone the ArcticTraining repository and navigate to the root directory:

```bash
git clone https://github.com/snowflakedb/ArcticTraining.git
cd ArcticTraining
```

2. Install the ArcticTraining package and its dependencies:

```bash
pip install -e .
```

3. Create a training recipe YAML that uses the built-in Supervised Fine-Tuning (SFT) trainer:

```yaml
type: sft
micro_batch_size: 2
model:
  name_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct
data:
  sources:
    - HuggingFaceH4/ultrachat_200k
checkpoint:
  - type: huggingface
    save_end_of_training: true
    output_dir: ./fine-tuned-model
```

4. Run the training recipe with the ArcticTraining CLI (see below). This will use the `DeepSpeed` launcher behind the scenes, you can pass any compatible DeepSpeed launcher arguments to the ArcticTraining CLI (e.g., --num_nodes, --num_gpus).

```bash
arctic_training path/to/sft-recipe.yaml
```

## Customize Training

To customize the training workflow, you can modify the training recipe YAML we
created in step 3 above. For example, you can change the model, dataset,
checkpoint, or other settings to meet your specific requirements. A full list of
configuration options can be found on the [configuration documentation
page](https://arctictraining.readthedocs.io/en/latest/config.html).

## Creating a New Trainer

If you want to create a new trainer, you can do so by subclassing the
``Trainer`` or ``SFTTrainer`` classes and implementing the necessary
modifications. For example, you could create a new trainer from ``SFTTrainer``
that uses a different loss function:

```python
from arctic_training import register
from arctic_training import SFTTrainer

@register
class CustomTrainer(SFTTrainer):
   name = "my_custom_trainer"

   def loss(self, batch):
       # Custom loss function implementation
       return loss
```

Remember to register this new trainer using the ``@register`` decorator so that
it can be used in training recipes. By default, ArcticTraining looks for a
``train.py`` in the current working directory to find custom trainers. You can
also specify a custom path to the trainers with the ``code`` field in your
training recipe:

```yaml
type: my_custom_trainer
code: path/to/custom_trainers.py
model:
 name_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct
data:
 sources:
   - HuggingFaceH4/ultrachat_200k
```
