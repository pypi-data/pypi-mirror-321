# coding: utf-8

"""
    Antimatter Security Lakehouse Public API

    Interact with the Antimatter ASL API

    The version of the OpenAPI document: 0.0.2
    Contact: support@antimatter.io
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from asl_api.models.core_v1alpha1_data_source_spec_input_cloud_files import CoreV1alpha1DataSourceSpecInputCloudFiles
from asl_api.models.core_v1alpha1_data_source_spec_input_clustering import CoreV1alpha1DataSourceSpecInputClustering
from typing import Optional, Set
from typing_extensions import Self

class CoreV1alpha1DataSourceSpecInput(BaseModel):
    """
    Describes what and how to import data into bronze tables
    """ # noqa: E501
    clustering: Optional[CoreV1alpha1DataSourceSpecInputClustering] = None
    bronze_table: Optional[StrictStr] = Field(default=None, description="the name of the bronze table to create and hold the imported data", alias="bronzeTable")
    skip_bronze_loading: Optional[StrictBool] = Field(default=None, description="Whether to skip the bronze loading step. This could be useful if data is already in a Unity Catalog table, etc.", alias="skipBronzeLoading")
    source: Optional[StrictStr] = Field(default=None, description="The name of originator of the data we will import (GCP, AWS, Okta, etc)")
    max_bytes_per_trigger: Optional[StrictInt] = Field(default=None, description="Approximately how much data gets processed in each micro-batch", alias="maxBytesPerTrigger")
    max_files_per_trigger: Optional[StrictInt] = Field(default=None, description="How many new files to be considered in every micro-batch", alias="maxFilesPerTrigger")
    source_type: Optional[StrictStr] = Field(default=None, description="The type of data we are importing from the source (e.g. S3, LB, etc)", alias="sourceType")
    format: Optional[StrictStr] = Field(default=None, description="json | parquet | csv | kafka | txt | cloudFiles")
    location: Optional[StrictStr] = Field(default=None, description="External location for a volume in Unity Catalog (anything you can pass to autoloader is here)")
    schema_file: Optional[StrictStr] = Field(default=None, description="An optional file containing the schema of the data source", alias="schemaFile")
    cloud_files: Optional[CoreV1alpha1DataSourceSpecInputCloudFiles] = Field(default=None, alias="cloudFiles")
    __properties: ClassVar[List[str]] = ["clustering", "bronzeTable", "skipBronzeLoading", "source", "maxBytesPerTrigger", "maxFilesPerTrigger", "sourceType", "format", "location", "schemaFile", "cloudFiles"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of CoreV1alpha1DataSourceSpecInput from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of clustering
        if self.clustering:
            _dict['clustering'] = self.clustering.to_dict()
        # override the default output from pydantic by calling `to_dict()` of cloud_files
        if self.cloud_files:
            _dict['cloudFiles'] = self.cloud_files.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of CoreV1alpha1DataSourceSpecInput from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "clustering": CoreV1alpha1DataSourceSpecInputClustering.from_dict(obj["clustering"]) if obj.get("clustering") is not None else None,
            "bronzeTable": obj.get("bronzeTable"),
            "skipBronzeLoading": obj.get("skipBronzeLoading"),
            "source": obj.get("source"),
            "maxBytesPerTrigger": obj.get("maxBytesPerTrigger"),
            "maxFilesPerTrigger": obj.get("maxFilesPerTrigger"),
            "sourceType": obj.get("sourceType"),
            "format": obj.get("format"),
            "location": obj.get("location"),
            "schemaFile": obj.get("schemaFile"),
            "cloudFiles": CoreV1alpha1DataSourceSpecInputCloudFiles.from_dict(obj["cloudFiles"]) if obj.get("cloudFiles") is not None else None
        })
        return _obj


